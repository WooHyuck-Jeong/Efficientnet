{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7.0\n",
      "WARNING:tensorflow:From C:\\Users\\jwhyu\\AppData\\Local\\Temp/ipykernel_8876/1550206244.py:3: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.list_physical_devices('GPU')` instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "tf.test.is_gpu_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 7401966202322916523\n",
      "xla_global_id: -1\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 6300696576\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 15770876028577201797\n",
      "physical_device_desc: \"device: 0, name: NVIDIA GeForce RTX 2060 SUPER, pci bus id: 0000:0a:00.0, compute capability: 7.5\"\n",
      "xla_global_id: 416903419\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from math import ceil\n",
    "from numba import njit, prange\n",
    "from sklearn.utils.validation import check_array\n",
    "from pyts.preprocessing import MinMaxScaler\n",
    "from pyts.approximation import PiecewiseAggregateApproximation\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.utils import np_utils\n",
    "import os \n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Welding_data = np.load('E:/Result/ver.3.22/MTF/MTF.npz')\n",
    "\n",
    "X_data = Welding_data['X_data']\n",
    "y_data = Welding_data['y_data']\n",
    "i_data = Welding_data['i_data']\n",
    "\n",
    "Welding_data.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 7\n",
    "np.random.seed()\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test,i_train, i_test = train_test_split(X_data, y_data, i_data, test_size= 0.2, shuffle= True, random_state= seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = np.concatenate((X_train, X_test))\n",
    "targets = np.concatenate((y_train, y_test))\n",
    "index = np.concatenate((i_train, i_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = np_utils.to_categorical(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.efficientnet import EfficientNetB3\n",
    "from keras.layers import Dense, Input, Activation, Flatten, Dropout\n",
    "from keras.models import Model\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras import optimizers\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers.normalization.batch_normalization import BatchNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 300, 300, 2  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " rescaling (Rescaling)          (None, 300, 300, 2)  0           ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " normalization (Normalization)  (None, 300, 300, 2)  5           ['rescaling[0][0]']              \n",
      "                                                                                                  \n",
      " stem_conv_pad (ZeroPadding2D)  (None, 301, 301, 2)  0           ['normalization[0][0]']          \n",
      "                                                                                                  \n",
      " stem_conv (Conv2D)             (None, 150, 150, 40  720         ['stem_conv_pad[0][0]']          \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " stem_bn (BatchNormalization)   (None, 150, 150, 40  160         ['stem_conv[0][0]']              \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " stem_activation (Activation)   (None, 150, 150, 40  0           ['stem_bn[0][0]']                \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " block1a_dwconv (DepthwiseConv2  (None, 150, 150, 40  360        ['stem_activation[0][0]']        \n",
      " D)                             )                                                                 \n",
      "                                                                                                  \n",
      " block1a_bn (BatchNormalization  (None, 150, 150, 40  160        ['block1a_dwconv[0][0]']         \n",
      " )                              )                                                                 \n",
      "                                                                                                  \n",
      " block1a_activation (Activation  (None, 150, 150, 40  0          ['block1a_bn[0][0]']             \n",
      " )                              )                                                                 \n",
      "                                                                                                  \n",
      " block1a_se_squeeze (GlobalAver  (None, 40)          0           ['block1a_activation[0][0]']     \n",
      " agePooling2D)                                                                                    \n",
      "                                                                                                  \n",
      " block1a_se_reshape (Reshape)   (None, 1, 1, 40)     0           ['block1a_se_squeeze[0][0]']     \n",
      "                                                                                                  \n",
      " block1a_se_reduce (Conv2D)     (None, 1, 1, 10)     410         ['block1a_se_reshape[0][0]']     \n",
      "                                                                                                  \n",
      " block1a_se_expand (Conv2D)     (None, 1, 1, 40)     440         ['block1a_se_reduce[0][0]']      \n",
      "                                                                                                  \n",
      " block1a_se_excite (Multiply)   (None, 150, 150, 40  0           ['block1a_activation[0][0]',     \n",
      "                                )                                 'block1a_se_expand[0][0]']      \n",
      "                                                                                                  \n",
      " block1a_project_conv (Conv2D)  (None, 150, 150, 24  960         ['block1a_se_excite[0][0]']      \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " block1a_project_bn (BatchNorma  (None, 150, 150, 24  96         ['block1a_project_conv[0][0]']   \n",
      " lization)                      )                                                                 \n",
      "                                                                                                  \n",
      " block1b_dwconv (DepthwiseConv2  (None, 150, 150, 24  216        ['block1a_project_bn[0][0]']     \n",
      " D)                             )                                                                 \n",
      "                                                                                                  \n",
      " block1b_bn (BatchNormalization  (None, 150, 150, 24  96         ['block1b_dwconv[0][0]']         \n",
      " )                              )                                                                 \n",
      "                                                                                                  \n",
      " block1b_activation (Activation  (None, 150, 150, 24  0          ['block1b_bn[0][0]']             \n",
      " )                              )                                                                 \n",
      "                                                                                                  \n",
      " block1b_se_squeeze (GlobalAver  (None, 24)          0           ['block1b_activation[0][0]']     \n",
      " agePooling2D)                                                                                    \n",
      "                                                                                                  \n",
      " block1b_se_reshape (Reshape)   (None, 1, 1, 24)     0           ['block1b_se_squeeze[0][0]']     \n",
      "                                                                                                  \n",
      " block1b_se_reduce (Conv2D)     (None, 1, 1, 6)      150         ['block1b_se_reshape[0][0]']     \n",
      "                                                                                                  \n",
      " block1b_se_expand (Conv2D)     (None, 1, 1, 24)     168         ['block1b_se_reduce[0][0]']      \n",
      "                                                                                                  \n",
      " block1b_se_excite (Multiply)   (None, 150, 150, 24  0           ['block1b_activation[0][0]',     \n",
      "                                )                                 'block1b_se_expand[0][0]']      \n",
      "                                                                                                  \n",
      " block1b_project_conv (Conv2D)  (None, 150, 150, 24  576         ['block1b_se_excite[0][0]']      \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " block1b_project_bn (BatchNorma  (None, 150, 150, 24  96         ['block1b_project_conv[0][0]']   \n",
      " lization)                      )                                                                 \n",
      "                                                                                                  \n",
      " block1b_drop (Dropout)         (None, 150, 150, 24  0           ['block1b_project_bn[0][0]']     \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " block1b_add (Add)              (None, 150, 150, 24  0           ['block1b_drop[0][0]',           \n",
      "                                )                                 'block1a_project_bn[0][0]']     \n",
      "                                                                                                  \n",
      " block2a_expand_conv (Conv2D)   (None, 150, 150, 14  3456        ['block1b_add[0][0]']            \n",
      "                                4)                                                                \n",
      "                                                                                                  \n",
      " block2a_expand_bn (BatchNormal  (None, 150, 150, 14  576        ['block2a_expand_conv[0][0]']    \n",
      " ization)                       4)                                                                \n",
      "                                                                                                  \n",
      " block2a_expand_activation (Act  (None, 150, 150, 14  0          ['block2a_expand_bn[0][0]']      \n",
      " ivation)                       4)                                                                \n",
      "                                                                                                  \n",
      " block2a_dwconv_pad (ZeroPaddin  (None, 151, 151, 14  0          ['block2a_expand_activation[0][0]\n",
      " g2D)                           4)                               ']                               \n",
      "                                                                                                  \n",
      " block2a_dwconv (DepthwiseConv2  (None, 75, 75, 144)  1296       ['block2a_dwconv_pad[0][0]']     \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " block2a_bn (BatchNormalization  (None, 75, 75, 144)  576        ['block2a_dwconv[0][0]']         \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block2a_activation (Activation  (None, 75, 75, 144)  0          ['block2a_bn[0][0]']             \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block2a_se_squeeze (GlobalAver  (None, 144)         0           ['block2a_activation[0][0]']     \n",
      " agePooling2D)                                                                                    \n",
      "                                                                                                  \n",
      " block2a_se_reshape (Reshape)   (None, 1, 1, 144)    0           ['block2a_se_squeeze[0][0]']     \n",
      "                                                                                                  \n",
      " block2a_se_reduce (Conv2D)     (None, 1, 1, 6)      870         ['block2a_se_reshape[0][0]']     \n",
      "                                                                                                  \n",
      " block2a_se_expand (Conv2D)     (None, 1, 1, 144)    1008        ['block2a_se_reduce[0][0]']      \n",
      "                                                                                                  \n",
      " block2a_se_excite (Multiply)   (None, 75, 75, 144)  0           ['block2a_activation[0][0]',     \n",
      "                                                                  'block2a_se_expand[0][0]']      \n",
      "                                                                                                  \n",
      " block2a_project_conv (Conv2D)  (None, 75, 75, 32)   4608        ['block2a_se_excite[0][0]']      \n",
      "                                                                                                  \n",
      " block2a_project_bn (BatchNorma  (None, 75, 75, 32)  128         ['block2a_project_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block2b_expand_conv (Conv2D)   (None, 75, 75, 192)  6144        ['block2a_project_bn[0][0]']     \n",
      "                                                                                                  \n",
      " block2b_expand_bn (BatchNormal  (None, 75, 75, 192)  768        ['block2b_expand_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block2b_expand_activation (Act  (None, 75, 75, 192)  0          ['block2b_expand_bn[0][0]']      \n",
      " ivation)                                                                                         \n",
      "                                                                                                  \n",
      " block2b_dwconv (DepthwiseConv2  (None, 75, 75, 192)  1728       ['block2b_expand_activation[0][0]\n",
      " D)                                                              ']                               \n",
      "                                                                                                  \n",
      " block2b_bn (BatchNormalization  (None, 75, 75, 192)  768        ['block2b_dwconv[0][0]']         \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block2b_activation (Activation  (None, 75, 75, 192)  0          ['block2b_bn[0][0]']             \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block2b_se_squeeze (GlobalAver  (None, 192)         0           ['block2b_activation[0][0]']     \n",
      " agePooling2D)                                                                                    \n",
      "                                                                                                  \n",
      " block2b_se_reshape (Reshape)   (None, 1, 1, 192)    0           ['block2b_se_squeeze[0][0]']     \n",
      "                                                                                                  \n",
      " block2b_se_reduce (Conv2D)     (None, 1, 1, 8)      1544        ['block2b_se_reshape[0][0]']     \n",
      "                                                                                                  \n",
      " block2b_se_expand (Conv2D)     (None, 1, 1, 192)    1728        ['block2b_se_reduce[0][0]']      \n",
      "                                                                                                  \n",
      " block2b_se_excite (Multiply)   (None, 75, 75, 192)  0           ['block2b_activation[0][0]',     \n",
      "                                                                  'block2b_se_expand[0][0]']      \n",
      "                                                                                                  \n",
      " block2b_project_conv (Conv2D)  (None, 75, 75, 32)   6144        ['block2b_se_excite[0][0]']      \n",
      "                                                                                                  \n",
      " block2b_project_bn (BatchNorma  (None, 75, 75, 32)  128         ['block2b_project_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block2b_drop (Dropout)         (None, 75, 75, 32)   0           ['block2b_project_bn[0][0]']     \n",
      "                                                                                                  \n",
      " block2b_add (Add)              (None, 75, 75, 32)   0           ['block2b_drop[0][0]',           \n",
      "                                                                  'block2a_project_bn[0][0]']     \n",
      "                                                                                                  \n",
      " block2c_expand_conv (Conv2D)   (None, 75, 75, 192)  6144        ['block2b_add[0][0]']            \n",
      "                                                                                                  \n",
      " block2c_expand_bn (BatchNormal  (None, 75, 75, 192)  768        ['block2c_expand_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block2c_expand_activation (Act  (None, 75, 75, 192)  0          ['block2c_expand_bn[0][0]']      \n",
      " ivation)                                                                                         \n",
      "                                                                                                  \n",
      " block2c_dwconv (DepthwiseConv2  (None, 75, 75, 192)  1728       ['block2c_expand_activation[0][0]\n",
      " D)                                                              ']                               \n",
      "                                                                                                  \n",
      " block2c_bn (BatchNormalization  (None, 75, 75, 192)  768        ['block2c_dwconv[0][0]']         \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block2c_activation (Activation  (None, 75, 75, 192)  0          ['block2c_bn[0][0]']             \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block2c_se_squeeze (GlobalAver  (None, 192)         0           ['block2c_activation[0][0]']     \n",
      " agePooling2D)                                                                                    \n",
      "                                                                                                  \n",
      " block2c_se_reshape (Reshape)   (None, 1, 1, 192)    0           ['block2c_se_squeeze[0][0]']     \n",
      "                                                                                                  \n",
      " block2c_se_reduce (Conv2D)     (None, 1, 1, 8)      1544        ['block2c_se_reshape[0][0]']     \n",
      "                                                                                                  \n",
      " block2c_se_expand (Conv2D)     (None, 1, 1, 192)    1728        ['block2c_se_reduce[0][0]']      \n",
      "                                                                                                  \n",
      " block2c_se_excite (Multiply)   (None, 75, 75, 192)  0           ['block2c_activation[0][0]',     \n",
      "                                                                  'block2c_se_expand[0][0]']      \n",
      "                                                                                                  \n",
      " block2c_project_conv (Conv2D)  (None, 75, 75, 32)   6144        ['block2c_se_excite[0][0]']      \n",
      "                                                                                                  \n",
      " block2c_project_bn (BatchNorma  (None, 75, 75, 32)  128         ['block2c_project_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block2c_drop (Dropout)         (None, 75, 75, 32)   0           ['block2c_project_bn[0][0]']     \n",
      "                                                                                                  \n",
      " block2c_add (Add)              (None, 75, 75, 32)   0           ['block2c_drop[0][0]',           \n",
      "                                                                  'block2b_add[0][0]']            \n",
      "                                                                                                  \n",
      " block3a_expand_conv (Conv2D)   (None, 75, 75, 192)  6144        ['block2c_add[0][0]']            \n",
      "                                                                                                  \n",
      " block3a_expand_bn (BatchNormal  (None, 75, 75, 192)  768        ['block3a_expand_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block3a_expand_activation (Act  (None, 75, 75, 192)  0          ['block3a_expand_bn[0][0]']      \n",
      " ivation)                                                                                         \n",
      "                                                                                                  \n",
      " block3a_dwconv_pad (ZeroPaddin  (None, 79, 79, 192)  0          ['block3a_expand_activation[0][0]\n",
      " g2D)                                                            ']                               \n",
      "                                                                                                  \n",
      " block3a_dwconv (DepthwiseConv2  (None, 38, 38, 192)  4800       ['block3a_dwconv_pad[0][0]']     \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " block3a_bn (BatchNormalization  (None, 38, 38, 192)  768        ['block3a_dwconv[0][0]']         \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block3a_activation (Activation  (None, 38, 38, 192)  0          ['block3a_bn[0][0]']             \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block3a_se_squeeze (GlobalAver  (None, 192)         0           ['block3a_activation[0][0]']     \n",
      " agePooling2D)                                                                                    \n",
      "                                                                                                  \n",
      " block3a_se_reshape (Reshape)   (None, 1, 1, 192)    0           ['block3a_se_squeeze[0][0]']     \n",
      "                                                                                                  \n",
      " block3a_se_reduce (Conv2D)     (None, 1, 1, 8)      1544        ['block3a_se_reshape[0][0]']     \n",
      "                                                                                                  \n",
      " block3a_se_expand (Conv2D)     (None, 1, 1, 192)    1728        ['block3a_se_reduce[0][0]']      \n",
      "                                                                                                  \n",
      " block3a_se_excite (Multiply)   (None, 38, 38, 192)  0           ['block3a_activation[0][0]',     \n",
      "                                                                  'block3a_se_expand[0][0]']      \n",
      "                                                                                                  \n",
      " block3a_project_conv (Conv2D)  (None, 38, 38, 48)   9216        ['block3a_se_excite[0][0]']      \n",
      "                                                                                                  \n",
      " block3a_project_bn (BatchNorma  (None, 38, 38, 48)  192         ['block3a_project_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block3b_expand_conv (Conv2D)   (None, 38, 38, 288)  13824       ['block3a_project_bn[0][0]']     \n",
      "                                                                                                  \n",
      " block3b_expand_bn (BatchNormal  (None, 38, 38, 288)  1152       ['block3b_expand_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block3b_expand_activation (Act  (None, 38, 38, 288)  0          ['block3b_expand_bn[0][0]']      \n",
      " ivation)                                                                                         \n",
      "                                                                                                  \n",
      " block3b_dwconv (DepthwiseConv2  (None, 38, 38, 288)  7200       ['block3b_expand_activation[0][0]\n",
      " D)                                                              ']                               \n",
      "                                                                                                  \n",
      " block3b_bn (BatchNormalization  (None, 38, 38, 288)  1152       ['block3b_dwconv[0][0]']         \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block3b_activation (Activation  (None, 38, 38, 288)  0          ['block3b_bn[0][0]']             \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block3b_se_squeeze (GlobalAver  (None, 288)         0           ['block3b_activation[0][0]']     \n",
      " agePooling2D)                                                                                    \n",
      "                                                                                                  \n",
      " block3b_se_reshape (Reshape)   (None, 1, 1, 288)    0           ['block3b_se_squeeze[0][0]']     \n",
      "                                                                                                  \n",
      " block3b_se_reduce (Conv2D)     (None, 1, 1, 12)     3468        ['block3b_se_reshape[0][0]']     \n",
      "                                                                                                  \n",
      " block3b_se_expand (Conv2D)     (None, 1, 1, 288)    3744        ['block3b_se_reduce[0][0]']      \n",
      "                                                                                                  \n",
      " block3b_se_excite (Multiply)   (None, 38, 38, 288)  0           ['block3b_activation[0][0]',     \n",
      "                                                                  'block3b_se_expand[0][0]']      \n",
      "                                                                                                  \n",
      " block3b_project_conv (Conv2D)  (None, 38, 38, 48)   13824       ['block3b_se_excite[0][0]']      \n",
      "                                                                                                  \n",
      " block3b_project_bn (BatchNorma  (None, 38, 38, 48)  192         ['block3b_project_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block3b_drop (Dropout)         (None, 38, 38, 48)   0           ['block3b_project_bn[0][0]']     \n",
      "                                                                                                  \n",
      " block3b_add (Add)              (None, 38, 38, 48)   0           ['block3b_drop[0][0]',           \n",
      "                                                                  'block3a_project_bn[0][0]']     \n",
      "                                                                                                  \n",
      " block3c_expand_conv (Conv2D)   (None, 38, 38, 288)  13824       ['block3b_add[0][0]']            \n",
      "                                                                                                  \n",
      " block3c_expand_bn (BatchNormal  (None, 38, 38, 288)  1152       ['block3c_expand_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block3c_expand_activation (Act  (None, 38, 38, 288)  0          ['block3c_expand_bn[0][0]']      \n",
      " ivation)                                                                                         \n",
      "                                                                                                  \n",
      " block3c_dwconv (DepthwiseConv2  (None, 38, 38, 288)  7200       ['block3c_expand_activation[0][0]\n",
      " D)                                                              ']                               \n",
      "                                                                                                  \n",
      " block3c_bn (BatchNormalization  (None, 38, 38, 288)  1152       ['block3c_dwconv[0][0]']         \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block3c_activation (Activation  (None, 38, 38, 288)  0          ['block3c_bn[0][0]']             \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block3c_se_squeeze (GlobalAver  (None, 288)         0           ['block3c_activation[0][0]']     \n",
      " agePooling2D)                                                                                    \n",
      "                                                                                                  \n",
      " block3c_se_reshape (Reshape)   (None, 1, 1, 288)    0           ['block3c_se_squeeze[0][0]']     \n",
      "                                                                                                  \n",
      " block3c_se_reduce (Conv2D)     (None, 1, 1, 12)     3468        ['block3c_se_reshape[0][0]']     \n",
      "                                                                                                  \n",
      " block3c_se_expand (Conv2D)     (None, 1, 1, 288)    3744        ['block3c_se_reduce[0][0]']      \n",
      "                                                                                                  \n",
      " block3c_se_excite (Multiply)   (None, 38, 38, 288)  0           ['block3c_activation[0][0]',     \n",
      "                                                                  'block3c_se_expand[0][0]']      \n",
      "                                                                                                  \n",
      " block3c_project_conv (Conv2D)  (None, 38, 38, 48)   13824       ['block3c_se_excite[0][0]']      \n",
      "                                                                                                  \n",
      " block3c_project_bn (BatchNorma  (None, 38, 38, 48)  192         ['block3c_project_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block3c_drop (Dropout)         (None, 38, 38, 48)   0           ['block3c_project_bn[0][0]']     \n",
      "                                                                                                  \n",
      " block3c_add (Add)              (None, 38, 38, 48)   0           ['block3c_drop[0][0]',           \n",
      "                                                                  'block3b_add[0][0]']            \n",
      "                                                                                                  \n",
      " block4a_expand_conv (Conv2D)   (None, 38, 38, 288)  13824       ['block3c_add[0][0]']            \n",
      "                                                                                                  \n",
      " block4a_expand_bn (BatchNormal  (None, 38, 38, 288)  1152       ['block4a_expand_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block4a_expand_activation (Act  (None, 38, 38, 288)  0          ['block4a_expand_bn[0][0]']      \n",
      " ivation)                                                                                         \n",
      "                                                                                                  \n",
      " block4a_dwconv_pad (ZeroPaddin  (None, 39, 39, 288)  0          ['block4a_expand_activation[0][0]\n",
      " g2D)                                                            ']                               \n",
      "                                                                                                  \n",
      " block4a_dwconv (DepthwiseConv2  (None, 19, 19, 288)  2592       ['block4a_dwconv_pad[0][0]']     \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " block4a_bn (BatchNormalization  (None, 19, 19, 288)  1152       ['block4a_dwconv[0][0]']         \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block4a_activation (Activation  (None, 19, 19, 288)  0          ['block4a_bn[0][0]']             \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block4a_se_squeeze (GlobalAver  (None, 288)         0           ['block4a_activation[0][0]']     \n",
      " agePooling2D)                                                                                    \n",
      "                                                                                                  \n",
      " block4a_se_reshape (Reshape)   (None, 1, 1, 288)    0           ['block4a_se_squeeze[0][0]']     \n",
      "                                                                                                  \n",
      " block4a_se_reduce (Conv2D)     (None, 1, 1, 12)     3468        ['block4a_se_reshape[0][0]']     \n",
      "                                                                                                  \n",
      " block4a_se_expand (Conv2D)     (None, 1, 1, 288)    3744        ['block4a_se_reduce[0][0]']      \n",
      "                                                                                                  \n",
      " block4a_se_excite (Multiply)   (None, 19, 19, 288)  0           ['block4a_activation[0][0]',     \n",
      "                                                                  'block4a_se_expand[0][0]']      \n",
      "                                                                                                  \n",
      " block4a_project_conv (Conv2D)  (None, 19, 19, 96)   27648       ['block4a_se_excite[0][0]']      \n",
      "                                                                                                  \n",
      " block4a_project_bn (BatchNorma  (None, 19, 19, 96)  384         ['block4a_project_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block4b_expand_conv (Conv2D)   (None, 19, 19, 576)  55296       ['block4a_project_bn[0][0]']     \n",
      "                                                                                                  \n",
      " block4b_expand_bn (BatchNormal  (None, 19, 19, 576)  2304       ['block4b_expand_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block4b_expand_activation (Act  (None, 19, 19, 576)  0          ['block4b_expand_bn[0][0]']      \n",
      " ivation)                                                                                         \n",
      "                                                                                                  \n",
      " block4b_dwconv (DepthwiseConv2  (None, 19, 19, 576)  5184       ['block4b_expand_activation[0][0]\n",
      " D)                                                              ']                               \n",
      "                                                                                                  \n",
      " block4b_bn (BatchNormalization  (None, 19, 19, 576)  2304       ['block4b_dwconv[0][0]']         \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block4b_activation (Activation  (None, 19, 19, 576)  0          ['block4b_bn[0][0]']             \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block4b_se_squeeze (GlobalAver  (None, 576)         0           ['block4b_activation[0][0]']     \n",
      " agePooling2D)                                                                                    \n",
      "                                                                                                  \n",
      " block4b_se_reshape (Reshape)   (None, 1, 1, 576)    0           ['block4b_se_squeeze[0][0]']     \n",
      "                                                                                                  \n",
      " block4b_se_reduce (Conv2D)     (None, 1, 1, 24)     13848       ['block4b_se_reshape[0][0]']     \n",
      "                                                                                                  \n",
      " block4b_se_expand (Conv2D)     (None, 1, 1, 576)    14400       ['block4b_se_reduce[0][0]']      \n",
      "                                                                                                  \n",
      " block4b_se_excite (Multiply)   (None, 19, 19, 576)  0           ['block4b_activation[0][0]',     \n",
      "                                                                  'block4b_se_expand[0][0]']      \n",
      "                                                                                                  \n",
      " block4b_project_conv (Conv2D)  (None, 19, 19, 96)   55296       ['block4b_se_excite[0][0]']      \n",
      "                                                                                                  \n",
      " block4b_project_bn (BatchNorma  (None, 19, 19, 96)  384         ['block4b_project_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block4b_drop (Dropout)         (None, 19, 19, 96)   0           ['block4b_project_bn[0][0]']     \n",
      "                                                                                                  \n",
      " block4b_add (Add)              (None, 19, 19, 96)   0           ['block4b_drop[0][0]',           \n",
      "                                                                  'block4a_project_bn[0][0]']     \n",
      "                                                                                                  \n",
      " block4c_expand_conv (Conv2D)   (None, 19, 19, 576)  55296       ['block4b_add[0][0]']            \n",
      "                                                                                                  \n",
      " block4c_expand_bn (BatchNormal  (None, 19, 19, 576)  2304       ['block4c_expand_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block4c_expand_activation (Act  (None, 19, 19, 576)  0          ['block4c_expand_bn[0][0]']      \n",
      " ivation)                                                                                         \n",
      "                                                                                                  \n",
      " block4c_dwconv (DepthwiseConv2  (None, 19, 19, 576)  5184       ['block4c_expand_activation[0][0]\n",
      " D)                                                              ']                               \n",
      "                                                                                                  \n",
      " block4c_bn (BatchNormalization  (None, 19, 19, 576)  2304       ['block4c_dwconv[0][0]']         \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block4c_activation (Activation  (None, 19, 19, 576)  0          ['block4c_bn[0][0]']             \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block4c_se_squeeze (GlobalAver  (None, 576)         0           ['block4c_activation[0][0]']     \n",
      " agePooling2D)                                                                                    \n",
      "                                                                                                  \n",
      " block4c_se_reshape (Reshape)   (None, 1, 1, 576)    0           ['block4c_se_squeeze[0][0]']     \n",
      "                                                                                                  \n",
      " block4c_se_reduce (Conv2D)     (None, 1, 1, 24)     13848       ['block4c_se_reshape[0][0]']     \n",
      "                                                                                                  \n",
      " block4c_se_expand (Conv2D)     (None, 1, 1, 576)    14400       ['block4c_se_reduce[0][0]']      \n",
      "                                                                                                  \n",
      " block4c_se_excite (Multiply)   (None, 19, 19, 576)  0           ['block4c_activation[0][0]',     \n",
      "                                                                  'block4c_se_expand[0][0]']      \n",
      "                                                                                                  \n",
      " block4c_project_conv (Conv2D)  (None, 19, 19, 96)   55296       ['block4c_se_excite[0][0]']      \n",
      "                                                                                                  \n",
      " block4c_project_bn (BatchNorma  (None, 19, 19, 96)  384         ['block4c_project_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block4c_drop (Dropout)         (None, 19, 19, 96)   0           ['block4c_project_bn[0][0]']     \n",
      "                                                                                                  \n",
      " block4c_add (Add)              (None, 19, 19, 96)   0           ['block4c_drop[0][0]',           \n",
      "                                                                  'block4b_add[0][0]']            \n",
      "                                                                                                  \n",
      " block4d_expand_conv (Conv2D)   (None, 19, 19, 576)  55296       ['block4c_add[0][0]']            \n",
      "                                                                                                  \n",
      " block4d_expand_bn (BatchNormal  (None, 19, 19, 576)  2304       ['block4d_expand_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block4d_expand_activation (Act  (None, 19, 19, 576)  0          ['block4d_expand_bn[0][0]']      \n",
      " ivation)                                                                                         \n",
      "                                                                                                  \n",
      " block4d_dwconv (DepthwiseConv2  (None, 19, 19, 576)  5184       ['block4d_expand_activation[0][0]\n",
      " D)                                                              ']                               \n",
      "                                                                                                  \n",
      " block4d_bn (BatchNormalization  (None, 19, 19, 576)  2304       ['block4d_dwconv[0][0]']         \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block4d_activation (Activation  (None, 19, 19, 576)  0          ['block4d_bn[0][0]']             \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block4d_se_squeeze (GlobalAver  (None, 576)         0           ['block4d_activation[0][0]']     \n",
      " agePooling2D)                                                                                    \n",
      "                                                                                                  \n",
      " block4d_se_reshape (Reshape)   (None, 1, 1, 576)    0           ['block4d_se_squeeze[0][0]']     \n",
      "                                                                                                  \n",
      " block4d_se_reduce (Conv2D)     (None, 1, 1, 24)     13848       ['block4d_se_reshape[0][0]']     \n",
      "                                                                                                  \n",
      " block4d_se_expand (Conv2D)     (None, 1, 1, 576)    14400       ['block4d_se_reduce[0][0]']      \n",
      "                                                                                                  \n",
      " block4d_se_excite (Multiply)   (None, 19, 19, 576)  0           ['block4d_activation[0][0]',     \n",
      "                                                                  'block4d_se_expand[0][0]']      \n",
      "                                                                                                  \n",
      " block4d_project_conv (Conv2D)  (None, 19, 19, 96)   55296       ['block4d_se_excite[0][0]']      \n",
      "                                                                                                  \n",
      " block4d_project_bn (BatchNorma  (None, 19, 19, 96)  384         ['block4d_project_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block4d_drop (Dropout)         (None, 19, 19, 96)   0           ['block4d_project_bn[0][0]']     \n",
      "                                                                                                  \n",
      " block4d_add (Add)              (None, 19, 19, 96)   0           ['block4d_drop[0][0]',           \n",
      "                                                                  'block4c_add[0][0]']            \n",
      "                                                                                                  \n",
      " block4e_expand_conv (Conv2D)   (None, 19, 19, 576)  55296       ['block4d_add[0][0]']            \n",
      "                                                                                                  \n",
      " block4e_expand_bn (BatchNormal  (None, 19, 19, 576)  2304       ['block4e_expand_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block4e_expand_activation (Act  (None, 19, 19, 576)  0          ['block4e_expand_bn[0][0]']      \n",
      " ivation)                                                                                         \n",
      "                                                                                                  \n",
      " block4e_dwconv (DepthwiseConv2  (None, 19, 19, 576)  5184       ['block4e_expand_activation[0][0]\n",
      " D)                                                              ']                               \n",
      "                                                                                                  \n",
      " block4e_bn (BatchNormalization  (None, 19, 19, 576)  2304       ['block4e_dwconv[0][0]']         \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block4e_activation (Activation  (None, 19, 19, 576)  0          ['block4e_bn[0][0]']             \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block4e_se_squeeze (GlobalAver  (None, 576)         0           ['block4e_activation[0][0]']     \n",
      " agePooling2D)                                                                                    \n",
      "                                                                                                  \n",
      " block4e_se_reshape (Reshape)   (None, 1, 1, 576)    0           ['block4e_se_squeeze[0][0]']     \n",
      "                                                                                                  \n",
      " block4e_se_reduce (Conv2D)     (None, 1, 1, 24)     13848       ['block4e_se_reshape[0][0]']     \n",
      "                                                                                                  \n",
      " block4e_se_expand (Conv2D)     (None, 1, 1, 576)    14400       ['block4e_se_reduce[0][0]']      \n",
      "                                                                                                  \n",
      " block4e_se_excite (Multiply)   (None, 19, 19, 576)  0           ['block4e_activation[0][0]',     \n",
      "                                                                  'block4e_se_expand[0][0]']      \n",
      "                                                                                                  \n",
      " block4e_project_conv (Conv2D)  (None, 19, 19, 96)   55296       ['block4e_se_excite[0][0]']      \n",
      "                                                                                                  \n",
      " block4e_project_bn (BatchNorma  (None, 19, 19, 96)  384         ['block4e_project_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block4e_drop (Dropout)         (None, 19, 19, 96)   0           ['block4e_project_bn[0][0]']     \n",
      "                                                                                                  \n",
      " block4e_add (Add)              (None, 19, 19, 96)   0           ['block4e_drop[0][0]',           \n",
      "                                                                  'block4d_add[0][0]']            \n",
      "                                                                                                  \n",
      " block5a_expand_conv (Conv2D)   (None, 19, 19, 576)  55296       ['block4e_add[0][0]']            \n",
      "                                                                                                  \n",
      " block5a_expand_bn (BatchNormal  (None, 19, 19, 576)  2304       ['block5a_expand_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block5a_expand_activation (Act  (None, 19, 19, 576)  0          ['block5a_expand_bn[0][0]']      \n",
      " ivation)                                                                                         \n",
      "                                                                                                  \n",
      " block5a_dwconv (DepthwiseConv2  (None, 19, 19, 576)  14400      ['block5a_expand_activation[0][0]\n",
      " D)                                                              ']                               \n",
      "                                                                                                  \n",
      " block5a_bn (BatchNormalization  (None, 19, 19, 576)  2304       ['block5a_dwconv[0][0]']         \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block5a_activation (Activation  (None, 19, 19, 576)  0          ['block5a_bn[0][0]']             \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block5a_se_squeeze (GlobalAver  (None, 576)         0           ['block5a_activation[0][0]']     \n",
      " agePooling2D)                                                                                    \n",
      "                                                                                                  \n",
      " block5a_se_reshape (Reshape)   (None, 1, 1, 576)    0           ['block5a_se_squeeze[0][0]']     \n",
      "                                                                                                  \n",
      " block5a_se_reduce (Conv2D)     (None, 1, 1, 24)     13848       ['block5a_se_reshape[0][0]']     \n",
      "                                                                                                  \n",
      " block5a_se_expand (Conv2D)     (None, 1, 1, 576)    14400       ['block5a_se_reduce[0][0]']      \n",
      "                                                                                                  \n",
      " block5a_se_excite (Multiply)   (None, 19, 19, 576)  0           ['block5a_activation[0][0]',     \n",
      "                                                                  'block5a_se_expand[0][0]']      \n",
      "                                                                                                  \n",
      " block5a_project_conv (Conv2D)  (None, 19, 19, 136)  78336       ['block5a_se_excite[0][0]']      \n",
      "                                                                                                  \n",
      " block5a_project_bn (BatchNorma  (None, 19, 19, 136)  544        ['block5a_project_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block5b_expand_conv (Conv2D)   (None, 19, 19, 816)  110976      ['block5a_project_bn[0][0]']     \n",
      "                                                                                                  \n",
      " block5b_expand_bn (BatchNormal  (None, 19, 19, 816)  3264       ['block5b_expand_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block5b_expand_activation (Act  (None, 19, 19, 816)  0          ['block5b_expand_bn[0][0]']      \n",
      " ivation)                                                                                         \n",
      "                                                                                                  \n",
      " block5b_dwconv (DepthwiseConv2  (None, 19, 19, 816)  20400      ['block5b_expand_activation[0][0]\n",
      " D)                                                              ']                               \n",
      "                                                                                                  \n",
      " block5b_bn (BatchNormalization  (None, 19, 19, 816)  3264       ['block5b_dwconv[0][0]']         \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block5b_activation (Activation  (None, 19, 19, 816)  0          ['block5b_bn[0][0]']             \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block5b_se_squeeze (GlobalAver  (None, 816)         0           ['block5b_activation[0][0]']     \n",
      " agePooling2D)                                                                                    \n",
      "                                                                                                  \n",
      " block5b_se_reshape (Reshape)   (None, 1, 1, 816)    0           ['block5b_se_squeeze[0][0]']     \n",
      "                                                                                                  \n",
      " block5b_se_reduce (Conv2D)     (None, 1, 1, 34)     27778       ['block5b_se_reshape[0][0]']     \n",
      "                                                                                                  \n",
      " block5b_se_expand (Conv2D)     (None, 1, 1, 816)    28560       ['block5b_se_reduce[0][0]']      \n",
      "                                                                                                  \n",
      " block5b_se_excite (Multiply)   (None, 19, 19, 816)  0           ['block5b_activation[0][0]',     \n",
      "                                                                  'block5b_se_expand[0][0]']      \n",
      "                                                                                                  \n",
      " block5b_project_conv (Conv2D)  (None, 19, 19, 136)  110976      ['block5b_se_excite[0][0]']      \n",
      "                                                                                                  \n",
      " block5b_project_bn (BatchNorma  (None, 19, 19, 136)  544        ['block5b_project_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block5b_drop (Dropout)         (None, 19, 19, 136)  0           ['block5b_project_bn[0][0]']     \n",
      "                                                                                                  \n",
      " block5b_add (Add)              (None, 19, 19, 136)  0           ['block5b_drop[0][0]',           \n",
      "                                                                  'block5a_project_bn[0][0]']     \n",
      "                                                                                                  \n",
      " block5c_expand_conv (Conv2D)   (None, 19, 19, 816)  110976      ['block5b_add[0][0]']            \n",
      "                                                                                                  \n",
      " block5c_expand_bn (BatchNormal  (None, 19, 19, 816)  3264       ['block5c_expand_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block5c_expand_activation (Act  (None, 19, 19, 816)  0          ['block5c_expand_bn[0][0]']      \n",
      " ivation)                                                                                         \n",
      "                                                                                                  \n",
      " block5c_dwconv (DepthwiseConv2  (None, 19, 19, 816)  20400      ['block5c_expand_activation[0][0]\n",
      " D)                                                              ']                               \n",
      "                                                                                                  \n",
      " block5c_bn (BatchNormalization  (None, 19, 19, 816)  3264       ['block5c_dwconv[0][0]']         \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block5c_activation (Activation  (None, 19, 19, 816)  0          ['block5c_bn[0][0]']             \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block5c_se_squeeze (GlobalAver  (None, 816)         0           ['block5c_activation[0][0]']     \n",
      " agePooling2D)                                                                                    \n",
      "                                                                                                  \n",
      " block5c_se_reshape (Reshape)   (None, 1, 1, 816)    0           ['block5c_se_squeeze[0][0]']     \n",
      "                                                                                                  \n",
      " block5c_se_reduce (Conv2D)     (None, 1, 1, 34)     27778       ['block5c_se_reshape[0][0]']     \n",
      "                                                                                                  \n",
      " block5c_se_expand (Conv2D)     (None, 1, 1, 816)    28560       ['block5c_se_reduce[0][0]']      \n",
      "                                                                                                  \n",
      " block5c_se_excite (Multiply)   (None, 19, 19, 816)  0           ['block5c_activation[0][0]',     \n",
      "                                                                  'block5c_se_expand[0][0]']      \n",
      "                                                                                                  \n",
      " block5c_project_conv (Conv2D)  (None, 19, 19, 136)  110976      ['block5c_se_excite[0][0]']      \n",
      "                                                                                                  \n",
      " block5c_project_bn (BatchNorma  (None, 19, 19, 136)  544        ['block5c_project_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block5c_drop (Dropout)         (None, 19, 19, 136)  0           ['block5c_project_bn[0][0]']     \n",
      "                                                                                                  \n",
      " block5c_add (Add)              (None, 19, 19, 136)  0           ['block5c_drop[0][0]',           \n",
      "                                                                  'block5b_add[0][0]']            \n",
      "                                                                                                  \n",
      " block5d_expand_conv (Conv2D)   (None, 19, 19, 816)  110976      ['block5c_add[0][0]']            \n",
      "                                                                                                  \n",
      " block5d_expand_bn (BatchNormal  (None, 19, 19, 816)  3264       ['block5d_expand_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block5d_expand_activation (Act  (None, 19, 19, 816)  0          ['block5d_expand_bn[0][0]']      \n",
      " ivation)                                                                                         \n",
      "                                                                                                  \n",
      " block5d_dwconv (DepthwiseConv2  (None, 19, 19, 816)  20400      ['block5d_expand_activation[0][0]\n",
      " D)                                                              ']                               \n",
      "                                                                                                  \n",
      " block5d_bn (BatchNormalization  (None, 19, 19, 816)  3264       ['block5d_dwconv[0][0]']         \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block5d_activation (Activation  (None, 19, 19, 816)  0          ['block5d_bn[0][0]']             \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block5d_se_squeeze (GlobalAver  (None, 816)         0           ['block5d_activation[0][0]']     \n",
      " agePooling2D)                                                                                    \n",
      "                                                                                                  \n",
      " block5d_se_reshape (Reshape)   (None, 1, 1, 816)    0           ['block5d_se_squeeze[0][0]']     \n",
      "                                                                                                  \n",
      " block5d_se_reduce (Conv2D)     (None, 1, 1, 34)     27778       ['block5d_se_reshape[0][0]']     \n",
      "                                                                                                  \n",
      " block5d_se_expand (Conv2D)     (None, 1, 1, 816)    28560       ['block5d_se_reduce[0][0]']      \n",
      "                                                                                                  \n",
      " block5d_se_excite (Multiply)   (None, 19, 19, 816)  0           ['block5d_activation[0][0]',     \n",
      "                                                                  'block5d_se_expand[0][0]']      \n",
      "                                                                                                  \n",
      " block5d_project_conv (Conv2D)  (None, 19, 19, 136)  110976      ['block5d_se_excite[0][0]']      \n",
      "                                                                                                  \n",
      " block5d_project_bn (BatchNorma  (None, 19, 19, 136)  544        ['block5d_project_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block5d_drop (Dropout)         (None, 19, 19, 136)  0           ['block5d_project_bn[0][0]']     \n",
      "                                                                                                  \n",
      " block5d_add (Add)              (None, 19, 19, 136)  0           ['block5d_drop[0][0]',           \n",
      "                                                                  'block5c_add[0][0]']            \n",
      "                                                                                                  \n",
      " block5e_expand_conv (Conv2D)   (None, 19, 19, 816)  110976      ['block5d_add[0][0]']            \n",
      "                                                                                                  \n",
      " block5e_expand_bn (BatchNormal  (None, 19, 19, 816)  3264       ['block5e_expand_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block5e_expand_activation (Act  (None, 19, 19, 816)  0          ['block5e_expand_bn[0][0]']      \n",
      " ivation)                                                                                         \n",
      "                                                                                                  \n",
      " block5e_dwconv (DepthwiseConv2  (None, 19, 19, 816)  20400      ['block5e_expand_activation[0][0]\n",
      " D)                                                              ']                               \n",
      "                                                                                                  \n",
      " block5e_bn (BatchNormalization  (None, 19, 19, 816)  3264       ['block5e_dwconv[0][0]']         \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block5e_activation (Activation  (None, 19, 19, 816)  0          ['block5e_bn[0][0]']             \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block5e_se_squeeze (GlobalAver  (None, 816)         0           ['block5e_activation[0][0]']     \n",
      " agePooling2D)                                                                                    \n",
      "                                                                                                  \n",
      " block5e_se_reshape (Reshape)   (None, 1, 1, 816)    0           ['block5e_se_squeeze[0][0]']     \n",
      "                                                                                                  \n",
      " block5e_se_reduce (Conv2D)     (None, 1, 1, 34)     27778       ['block5e_se_reshape[0][0]']     \n",
      "                                                                                                  \n",
      " block5e_se_expand (Conv2D)     (None, 1, 1, 816)    28560       ['block5e_se_reduce[0][0]']      \n",
      "                                                                                                  \n",
      " block5e_se_excite (Multiply)   (None, 19, 19, 816)  0           ['block5e_activation[0][0]',     \n",
      "                                                                  'block5e_se_expand[0][0]']      \n",
      "                                                                                                  \n",
      " block5e_project_conv (Conv2D)  (None, 19, 19, 136)  110976      ['block5e_se_excite[0][0]']      \n",
      "                                                                                                  \n",
      " block5e_project_bn (BatchNorma  (None, 19, 19, 136)  544        ['block5e_project_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block5e_drop (Dropout)         (None, 19, 19, 136)  0           ['block5e_project_bn[0][0]']     \n",
      "                                                                                                  \n",
      " block5e_add (Add)              (None, 19, 19, 136)  0           ['block5e_drop[0][0]',           \n",
      "                                                                  'block5d_add[0][0]']            \n",
      "                                                                                                  \n",
      " block6a_expand_conv (Conv2D)   (None, 19, 19, 816)  110976      ['block5e_add[0][0]']            \n",
      "                                                                                                  \n",
      " block6a_expand_bn (BatchNormal  (None, 19, 19, 816)  3264       ['block6a_expand_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block6a_expand_activation (Act  (None, 19, 19, 816)  0          ['block6a_expand_bn[0][0]']      \n",
      " ivation)                                                                                         \n",
      "                                                                                                  \n",
      " block6a_dwconv_pad (ZeroPaddin  (None, 23, 23, 816)  0          ['block6a_expand_activation[0][0]\n",
      " g2D)                                                            ']                               \n",
      "                                                                                                  \n",
      " block6a_dwconv (DepthwiseConv2  (None, 10, 10, 816)  20400      ['block6a_dwconv_pad[0][0]']     \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " block6a_bn (BatchNormalization  (None, 10, 10, 816)  3264       ['block6a_dwconv[0][0]']         \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block6a_activation (Activation  (None, 10, 10, 816)  0          ['block6a_bn[0][0]']             \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block6a_se_squeeze (GlobalAver  (None, 816)         0           ['block6a_activation[0][0]']     \n",
      " agePooling2D)                                                                                    \n",
      "                                                                                                  \n",
      " block6a_se_reshape (Reshape)   (None, 1, 1, 816)    0           ['block6a_se_squeeze[0][0]']     \n",
      "                                                                                                  \n",
      " block6a_se_reduce (Conv2D)     (None, 1, 1, 34)     27778       ['block6a_se_reshape[0][0]']     \n",
      "                                                                                                  \n",
      " block6a_se_expand (Conv2D)     (None, 1, 1, 816)    28560       ['block6a_se_reduce[0][0]']      \n",
      "                                                                                                  \n",
      " block6a_se_excite (Multiply)   (None, 10, 10, 816)  0           ['block6a_activation[0][0]',     \n",
      "                                                                  'block6a_se_expand[0][0]']      \n",
      "                                                                                                  \n",
      " block6a_project_conv (Conv2D)  (None, 10, 10, 232)  189312      ['block6a_se_excite[0][0]']      \n",
      "                                                                                                  \n",
      " block6a_project_bn (BatchNorma  (None, 10, 10, 232)  928        ['block6a_project_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block6b_expand_conv (Conv2D)   (None, 10, 10, 1392  322944      ['block6a_project_bn[0][0]']     \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " block6b_expand_bn (BatchNormal  (None, 10, 10, 1392  5568       ['block6b_expand_conv[0][0]']    \n",
      " ization)                       )                                                                 \n",
      "                                                                                                  \n",
      " block6b_expand_activation (Act  (None, 10, 10, 1392  0          ['block6b_expand_bn[0][0]']      \n",
      " ivation)                       )                                                                 \n",
      "                                                                                                  \n",
      " block6b_dwconv (DepthwiseConv2  (None, 10, 10, 1392  34800      ['block6b_expand_activation[0][0]\n",
      " D)                             )                                ']                               \n",
      "                                                                                                  \n",
      " block6b_bn (BatchNormalization  (None, 10, 10, 1392  5568       ['block6b_dwconv[0][0]']         \n",
      " )                              )                                                                 \n",
      "                                                                                                  \n",
      " block6b_activation (Activation  (None, 10, 10, 1392  0          ['block6b_bn[0][0]']             \n",
      " )                              )                                                                 \n",
      "                                                                                                  \n",
      " block6b_se_squeeze (GlobalAver  (None, 1392)        0           ['block6b_activation[0][0]']     \n",
      " agePooling2D)                                                                                    \n",
      "                                                                                                  \n",
      " block6b_se_reshape (Reshape)   (None, 1, 1, 1392)   0           ['block6b_se_squeeze[0][0]']     \n",
      "                                                                                                  \n",
      " block6b_se_reduce (Conv2D)     (None, 1, 1, 58)     80794       ['block6b_se_reshape[0][0]']     \n",
      "                                                                                                  \n",
      " block6b_se_expand (Conv2D)     (None, 1, 1, 1392)   82128       ['block6b_se_reduce[0][0]']      \n",
      "                                                                                                  \n",
      " block6b_se_excite (Multiply)   (None, 10, 10, 1392  0           ['block6b_activation[0][0]',     \n",
      "                                )                                 'block6b_se_expand[0][0]']      \n",
      "                                                                                                  \n",
      " block6b_project_conv (Conv2D)  (None, 10, 10, 232)  322944      ['block6b_se_excite[0][0]']      \n",
      "                                                                                                  \n",
      " block6b_project_bn (BatchNorma  (None, 10, 10, 232)  928        ['block6b_project_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block6b_drop (Dropout)         (None, 10, 10, 232)  0           ['block6b_project_bn[0][0]']     \n",
      "                                                                                                  \n",
      " block6b_add (Add)              (None, 10, 10, 232)  0           ['block6b_drop[0][0]',           \n",
      "                                                                  'block6a_project_bn[0][0]']     \n",
      "                                                                                                  \n",
      " block6c_expand_conv (Conv2D)   (None, 10, 10, 1392  322944      ['block6b_add[0][0]']            \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " block6c_expand_bn (BatchNormal  (None, 10, 10, 1392  5568       ['block6c_expand_conv[0][0]']    \n",
      " ization)                       )                                                                 \n",
      "                                                                                                  \n",
      " block6c_expand_activation (Act  (None, 10, 10, 1392  0          ['block6c_expand_bn[0][0]']      \n",
      " ivation)                       )                                                                 \n",
      "                                                                                                  \n",
      " block6c_dwconv (DepthwiseConv2  (None, 10, 10, 1392  34800      ['block6c_expand_activation[0][0]\n",
      " D)                             )                                ']                               \n",
      "                                                                                                  \n",
      " block6c_bn (BatchNormalization  (None, 10, 10, 1392  5568       ['block6c_dwconv[0][0]']         \n",
      " )                              )                                                                 \n",
      "                                                                                                  \n",
      " block6c_activation (Activation  (None, 10, 10, 1392  0          ['block6c_bn[0][0]']             \n",
      " )                              )                                                                 \n",
      "                                                                                                  \n",
      " block6c_se_squeeze (GlobalAver  (None, 1392)        0           ['block6c_activation[0][0]']     \n",
      " agePooling2D)                                                                                    \n",
      "                                                                                                  \n",
      " block6c_se_reshape (Reshape)   (None, 1, 1, 1392)   0           ['block6c_se_squeeze[0][0]']     \n",
      "                                                                                                  \n",
      " block6c_se_reduce (Conv2D)     (None, 1, 1, 58)     80794       ['block6c_se_reshape[0][0]']     \n",
      "                                                                                                  \n",
      " block6c_se_expand (Conv2D)     (None, 1, 1, 1392)   82128       ['block6c_se_reduce[0][0]']      \n",
      "                                                                                                  \n",
      " block6c_se_excite (Multiply)   (None, 10, 10, 1392  0           ['block6c_activation[0][0]',     \n",
      "                                )                                 'block6c_se_expand[0][0]']      \n",
      "                                                                                                  \n",
      " block6c_project_conv (Conv2D)  (None, 10, 10, 232)  322944      ['block6c_se_excite[0][0]']      \n",
      "                                                                                                  \n",
      " block6c_project_bn (BatchNorma  (None, 10, 10, 232)  928        ['block6c_project_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block6c_drop (Dropout)         (None, 10, 10, 232)  0           ['block6c_project_bn[0][0]']     \n",
      "                                                                                                  \n",
      " block6c_add (Add)              (None, 10, 10, 232)  0           ['block6c_drop[0][0]',           \n",
      "                                                                  'block6b_add[0][0]']            \n",
      "                                                                                                  \n",
      " block6d_expand_conv (Conv2D)   (None, 10, 10, 1392  322944      ['block6c_add[0][0]']            \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " block6d_expand_bn (BatchNormal  (None, 10, 10, 1392  5568       ['block6d_expand_conv[0][0]']    \n",
      " ization)                       )                                                                 \n",
      "                                                                                                  \n",
      " block6d_expand_activation (Act  (None, 10, 10, 1392  0          ['block6d_expand_bn[0][0]']      \n",
      " ivation)                       )                                                                 \n",
      "                                                                                                  \n",
      " block6d_dwconv (DepthwiseConv2  (None, 10, 10, 1392  34800      ['block6d_expand_activation[0][0]\n",
      " D)                             )                                ']                               \n",
      "                                                                                                  \n",
      " block6d_bn (BatchNormalization  (None, 10, 10, 1392  5568       ['block6d_dwconv[0][0]']         \n",
      " )                              )                                                                 \n",
      "                                                                                                  \n",
      " block6d_activation (Activation  (None, 10, 10, 1392  0          ['block6d_bn[0][0]']             \n",
      " )                              )                                                                 \n",
      "                                                                                                  \n",
      " block6d_se_squeeze (GlobalAver  (None, 1392)        0           ['block6d_activation[0][0]']     \n",
      " agePooling2D)                                                                                    \n",
      "                                                                                                  \n",
      " block6d_se_reshape (Reshape)   (None, 1, 1, 1392)   0           ['block6d_se_squeeze[0][0]']     \n",
      "                                                                                                  \n",
      " block6d_se_reduce (Conv2D)     (None, 1, 1, 58)     80794       ['block6d_se_reshape[0][0]']     \n",
      "                                                                                                  \n",
      " block6d_se_expand (Conv2D)     (None, 1, 1, 1392)   82128       ['block6d_se_reduce[0][0]']      \n",
      "                                                                                                  \n",
      " block6d_se_excite (Multiply)   (None, 10, 10, 1392  0           ['block6d_activation[0][0]',     \n",
      "                                )                                 'block6d_se_expand[0][0]']      \n",
      "                                                                                                  \n",
      " block6d_project_conv (Conv2D)  (None, 10, 10, 232)  322944      ['block6d_se_excite[0][0]']      \n",
      "                                                                                                  \n",
      " block6d_project_bn (BatchNorma  (None, 10, 10, 232)  928        ['block6d_project_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block6d_drop (Dropout)         (None, 10, 10, 232)  0           ['block6d_project_bn[0][0]']     \n",
      "                                                                                                  \n",
      " block6d_add (Add)              (None, 10, 10, 232)  0           ['block6d_drop[0][0]',           \n",
      "                                                                  'block6c_add[0][0]']            \n",
      "                                                                                                  \n",
      " block6e_expand_conv (Conv2D)   (None, 10, 10, 1392  322944      ['block6d_add[0][0]']            \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " block6e_expand_bn (BatchNormal  (None, 10, 10, 1392  5568       ['block6e_expand_conv[0][0]']    \n",
      " ization)                       )                                                                 \n",
      "                                                                                                  \n",
      " block6e_expand_activation (Act  (None, 10, 10, 1392  0          ['block6e_expand_bn[0][0]']      \n",
      " ivation)                       )                                                                 \n",
      "                                                                                                  \n",
      " block6e_dwconv (DepthwiseConv2  (None, 10, 10, 1392  34800      ['block6e_expand_activation[0][0]\n",
      " D)                             )                                ']                               \n",
      "                                                                                                  \n",
      " block6e_bn (BatchNormalization  (None, 10, 10, 1392  5568       ['block6e_dwconv[0][0]']         \n",
      " )                              )                                                                 \n",
      "                                                                                                  \n",
      " block6e_activation (Activation  (None, 10, 10, 1392  0          ['block6e_bn[0][0]']             \n",
      " )                              )                                                                 \n",
      "                                                                                                  \n",
      " block6e_se_squeeze (GlobalAver  (None, 1392)        0           ['block6e_activation[0][0]']     \n",
      " agePooling2D)                                                                                    \n",
      "                                                                                                  \n",
      " block6e_se_reshape (Reshape)   (None, 1, 1, 1392)   0           ['block6e_se_squeeze[0][0]']     \n",
      "                                                                                                  \n",
      " block6e_se_reduce (Conv2D)     (None, 1, 1, 58)     80794       ['block6e_se_reshape[0][0]']     \n",
      "                                                                                                  \n",
      " block6e_se_expand (Conv2D)     (None, 1, 1, 1392)   82128       ['block6e_se_reduce[0][0]']      \n",
      "                                                                                                  \n",
      " block6e_se_excite (Multiply)   (None, 10, 10, 1392  0           ['block6e_activation[0][0]',     \n",
      "                                )                                 'block6e_se_expand[0][0]']      \n",
      "                                                                                                  \n",
      " block6e_project_conv (Conv2D)  (None, 10, 10, 232)  322944      ['block6e_se_excite[0][0]']      \n",
      "                                                                                                  \n",
      " block6e_project_bn (BatchNorma  (None, 10, 10, 232)  928        ['block6e_project_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block6e_drop (Dropout)         (None, 10, 10, 232)  0           ['block6e_project_bn[0][0]']     \n",
      "                                                                                                  \n",
      " block6e_add (Add)              (None, 10, 10, 232)  0           ['block6e_drop[0][0]',           \n",
      "                                                                  'block6d_add[0][0]']            \n",
      "                                                                                                  \n",
      " block6f_expand_conv (Conv2D)   (None, 10, 10, 1392  322944      ['block6e_add[0][0]']            \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " block6f_expand_bn (BatchNormal  (None, 10, 10, 1392  5568       ['block6f_expand_conv[0][0]']    \n",
      " ization)                       )                                                                 \n",
      "                                                                                                  \n",
      " block6f_expand_activation (Act  (None, 10, 10, 1392  0          ['block6f_expand_bn[0][0]']      \n",
      " ivation)                       )                                                                 \n",
      "                                                                                                  \n",
      " block6f_dwconv (DepthwiseConv2  (None, 10, 10, 1392  34800      ['block6f_expand_activation[0][0]\n",
      " D)                             )                                ']                               \n",
      "                                                                                                  \n",
      " block6f_bn (BatchNormalization  (None, 10, 10, 1392  5568       ['block6f_dwconv[0][0]']         \n",
      " )                              )                                                                 \n",
      "                                                                                                  \n",
      " block6f_activation (Activation  (None, 10, 10, 1392  0          ['block6f_bn[0][0]']             \n",
      " )                              )                                                                 \n",
      "                                                                                                  \n",
      " block6f_se_squeeze (GlobalAver  (None, 1392)        0           ['block6f_activation[0][0]']     \n",
      " agePooling2D)                                                                                    \n",
      "                                                                                                  \n",
      " block6f_se_reshape (Reshape)   (None, 1, 1, 1392)   0           ['block6f_se_squeeze[0][0]']     \n",
      "                                                                                                  \n",
      " block6f_se_reduce (Conv2D)     (None, 1, 1, 58)     80794       ['block6f_se_reshape[0][0]']     \n",
      "                                                                                                  \n",
      " block6f_se_expand (Conv2D)     (None, 1, 1, 1392)   82128       ['block6f_se_reduce[0][0]']      \n",
      "                                                                                                  \n",
      " block6f_se_excite (Multiply)   (None, 10, 10, 1392  0           ['block6f_activation[0][0]',     \n",
      "                                )                                 'block6f_se_expand[0][0]']      \n",
      "                                                                                                  \n",
      " block6f_project_conv (Conv2D)  (None, 10, 10, 232)  322944      ['block6f_se_excite[0][0]']      \n",
      "                                                                                                  \n",
      " block6f_project_bn (BatchNorma  (None, 10, 10, 232)  928        ['block6f_project_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block6f_drop (Dropout)         (None, 10, 10, 232)  0           ['block6f_project_bn[0][0]']     \n",
      "                                                                                                  \n",
      " block6f_add (Add)              (None, 10, 10, 232)  0           ['block6f_drop[0][0]',           \n",
      "                                                                  'block6e_add[0][0]']            \n",
      "                                                                                                  \n",
      " block7a_expand_conv (Conv2D)   (None, 10, 10, 1392  322944      ['block6f_add[0][0]']            \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " block7a_expand_bn (BatchNormal  (None, 10, 10, 1392  5568       ['block7a_expand_conv[0][0]']    \n",
      " ization)                       )                                                                 \n",
      "                                                                                                  \n",
      " block7a_expand_activation (Act  (None, 10, 10, 1392  0          ['block7a_expand_bn[0][0]']      \n",
      " ivation)                       )                                                                 \n",
      "                                                                                                  \n",
      " block7a_dwconv (DepthwiseConv2  (None, 10, 10, 1392  12528      ['block7a_expand_activation[0][0]\n",
      " D)                             )                                ']                               \n",
      "                                                                                                  \n",
      " block7a_bn (BatchNormalization  (None, 10, 10, 1392  5568       ['block7a_dwconv[0][0]']         \n",
      " )                              )                                                                 \n",
      "                                                                                                  \n",
      " block7a_activation (Activation  (None, 10, 10, 1392  0          ['block7a_bn[0][0]']             \n",
      " )                              )                                                                 \n",
      "                                                                                                  \n",
      " block7a_se_squeeze (GlobalAver  (None, 1392)        0           ['block7a_activation[0][0]']     \n",
      " agePooling2D)                                                                                    \n",
      "                                                                                                  \n",
      " block7a_se_reshape (Reshape)   (None, 1, 1, 1392)   0           ['block7a_se_squeeze[0][0]']     \n",
      "                                                                                                  \n",
      " block7a_se_reduce (Conv2D)     (None, 1, 1, 58)     80794       ['block7a_se_reshape[0][0]']     \n",
      "                                                                                                  \n",
      " block7a_se_expand (Conv2D)     (None, 1, 1, 1392)   82128       ['block7a_se_reduce[0][0]']      \n",
      "                                                                                                  \n",
      " block7a_se_excite (Multiply)   (None, 10, 10, 1392  0           ['block7a_activation[0][0]',     \n",
      "                                )                                 'block7a_se_expand[0][0]']      \n",
      "                                                                                                  \n",
      " block7a_project_conv (Conv2D)  (None, 10, 10, 384)  534528      ['block7a_se_excite[0][0]']      \n",
      "                                                                                                  \n",
      " block7a_project_bn (BatchNorma  (None, 10, 10, 384)  1536       ['block7a_project_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block7b_expand_conv (Conv2D)   (None, 10, 10, 2304  884736      ['block7a_project_bn[0][0]']     \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " block7b_expand_bn (BatchNormal  (None, 10, 10, 2304  9216       ['block7b_expand_conv[0][0]']    \n",
      " ization)                       )                                                                 \n",
      "                                                                                                  \n",
      " block7b_expand_activation (Act  (None, 10, 10, 2304  0          ['block7b_expand_bn[0][0]']      \n",
      " ivation)                       )                                                                 \n",
      "                                                                                                  \n",
      " block7b_dwconv (DepthwiseConv2  (None, 10, 10, 2304  20736      ['block7b_expand_activation[0][0]\n",
      " D)                             )                                ']                               \n",
      "                                                                                                  \n",
      " block7b_bn (BatchNormalization  (None, 10, 10, 2304  9216       ['block7b_dwconv[0][0]']         \n",
      " )                              )                                                                 \n",
      "                                                                                                  \n",
      " block7b_activation (Activation  (None, 10, 10, 2304  0          ['block7b_bn[0][0]']             \n",
      " )                              )                                                                 \n",
      "                                                                                                  \n",
      " block7b_se_squeeze (GlobalAver  (None, 2304)        0           ['block7b_activation[0][0]']     \n",
      " agePooling2D)                                                                                    \n",
      "                                                                                                  \n",
      " block7b_se_reshape (Reshape)   (None, 1, 1, 2304)   0           ['block7b_se_squeeze[0][0]']     \n",
      "                                                                                                  \n",
      " block7b_se_reduce (Conv2D)     (None, 1, 1, 96)     221280      ['block7b_se_reshape[0][0]']     \n",
      "                                                                                                  \n",
      " block7b_se_expand (Conv2D)     (None, 1, 1, 2304)   223488      ['block7b_se_reduce[0][0]']      \n",
      "                                                                                                  \n",
      " block7b_se_excite (Multiply)   (None, 10, 10, 2304  0           ['block7b_activation[0][0]',     \n",
      "                                )                                 'block7b_se_expand[0][0]']      \n",
      "                                                                                                  \n",
      " block7b_project_conv (Conv2D)  (None, 10, 10, 384)  884736      ['block7b_se_excite[0][0]']      \n",
      "                                                                                                  \n",
      " block7b_project_bn (BatchNorma  (None, 10, 10, 384)  1536       ['block7b_project_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block7b_drop (Dropout)         (None, 10, 10, 384)  0           ['block7b_project_bn[0][0]']     \n",
      "                                                                                                  \n",
      " block7b_add (Add)              (None, 10, 10, 384)  0           ['block7b_drop[0][0]',           \n",
      "                                                                  'block7a_project_bn[0][0]']     \n",
      "                                                                                                  \n",
      " top_conv (Conv2D)              (None, 10, 10, 1536  589824      ['block7b_add[0][0]']            \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " top_bn (BatchNormalization)    (None, 10, 10, 1536  6144        ['top_conv[0][0]']               \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " top_activation (Activation)    (None, 10, 10, 1536  0           ['top_bn[0][0]']                 \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " avg_pool (GlobalAveragePooling  (None, 1536)        0           ['top_activation[0][0]']         \n",
      " 2D)                                                                                              \n",
      "                                                                                                  \n",
      " softmax (Dense)                (None, 3)            4611        ['avg_pool[0][0]']               \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 10,787,784\n",
      "Trainable params: 10,700,483\n",
      "Non-trainable params: 87,301\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input = Input(shape= (300,300,2))\n",
    "model = EfficientNetB3(\n",
    "    input_tensor= input,\n",
    "    include_top= False,\n",
    "    weights= None,\n",
    "    pooling= 'avg'\n",
    ")\n",
    "\n",
    "x = model.output\n",
    "#x = Dropout(0,2)(x)\n",
    "x = Dense(3, activation= 'softmax', name= 'softmax')(x)\n",
    "\n",
    "model = Model(model.input, x)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf.keras.utils.plot_model(model, show_shapes= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "import datetime\n",
    "\n",
    "log_dir = \"logs_EffiB3/my_board/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir= log_dir, histogram_freq= 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118\n",
      "1061\n",
      "118\n",
      "1061\n",
      "118\n",
      "1061\n",
      "118\n",
      "1061\n",
      "118\n",
      "1061\n",
      "118\n",
      "1061\n",
      "118\n",
      "1061\n",
      "118\n",
      "1061\n",
      "118\n",
      "1061\n",
      "117\n",
      "1062\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "kfold = KFold(n_splits=10, shuffle= True, random_state= seed)\n",
    "\n",
    "test= []\n",
    "train= []\n",
    "test_= []\n",
    "train_= []\n",
    "\n",
    "for train, test in kfold.split(inputs, targets):\n",
    "    \n",
    "    print(len(test))\n",
    "    print(len(train))\n",
    "    \n",
    "    for i in zip(test):\n",
    "        test_.append(i)\n",
    "    for i in zip(train):\n",
    "        train_.append(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train_[0:1061]\n",
    "train = np.reshape(train, 1061)\n",
    "test = test_[0:117]\n",
    "test = np.reshape(test, 117)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1179, 300, 300, 2)\n",
      "(1179, 3)\n",
      "(1179,)\n"
     ]
    }
   ],
   "source": [
    "print(inputs.shape)\n",
    "print(targets.shape)\n",
    "print(index.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(848,) (95,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jwhyu\\anaconda3\\envs\\venv\\lib\\site-packages\\keras\\optimizer_v2\\gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------\n",
      "Training for fold 1 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jwhyu\\anaconda3\\envs\\venv\\lib\\site-packages\\keras\\engine\\functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  layer_config = serialize_layer_fn(layer)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "212/212 [==============================] - 48s 174ms/step - loss: 0.5956 - accuracy: 0.4776 - val_loss: 1.1965 - val_accuracy: 0.3474\n",
      "Epoch 2/100\n",
      "212/212 [==============================] - 34s 162ms/step - loss: 0.5231 - accuracy: 0.5743 - val_loss: 1.3695 - val_accuracy: 0.3474\n",
      "Epoch 3/100\n",
      "212/212 [==============================] - 34s 159ms/step - loss: 0.4833 - accuracy: 0.6238 - val_loss: 1.0550 - val_accuracy: 0.3474\n",
      "Epoch 4/100\n",
      "212/212 [==============================] - 34s 159ms/step - loss: 0.4457 - accuracy: 0.6710 - val_loss: 2.3959 - val_accuracy: 0.1895\n",
      "Epoch 5/100\n",
      "212/212 [==============================] - 34s 159ms/step - loss: 0.4470 - accuracy: 0.6851 - val_loss: 0.4055 - val_accuracy: 0.6842\n",
      "Epoch 6/100\n",
      "212/212 [==============================] - 34s 159ms/step - loss: 0.4129 - accuracy: 0.7064 - val_loss: 5.8599 - val_accuracy: 0.2105\n",
      "Epoch 7/100\n",
      "212/212 [==============================] - 34s 159ms/step - loss: 0.3895 - accuracy: 0.7252 - val_loss: 2.3008 - val_accuracy: 0.5895\n",
      "Epoch 8/100\n",
      "212/212 [==============================] - 34s 159ms/step - loss: 0.3821 - accuracy: 0.7111 - val_loss: 1.8115 - val_accuracy: 0.4842\n",
      "Epoch 9/100\n",
      "212/212 [==============================] - 34s 159ms/step - loss: 0.3522 - accuracy: 0.7453 - val_loss: 4.2736 - val_accuracy: 0.1474\n",
      "Epoch 10/100\n",
      "212/212 [==============================] - 34s 159ms/step - loss: 0.3569 - accuracy: 0.7736 - val_loss: 1.4989 - val_accuracy: 0.6632\n",
      "Epoch 11/100\n",
      "212/212 [==============================] - 34s 159ms/step - loss: 0.3389 - accuracy: 0.7866 - val_loss: 8.9543 - val_accuracy: 0.3474\n",
      "Epoch 12/100\n",
      "212/212 [==============================] - 34s 159ms/step - loss: 0.3150 - accuracy: 0.8054 - val_loss: 1.2635 - val_accuracy: 0.6105\n",
      "Epoch 13/100\n",
      "212/212 [==============================] - 34s 159ms/step - loss: 0.3002 - accuracy: 0.8101 - val_loss: 0.7828 - val_accuracy: 0.5474\n",
      "Epoch 14/100\n",
      "212/212 [==============================] - 34s 159ms/step - loss: 0.3278 - accuracy: 0.8149 - val_loss: 1.4206 - val_accuracy: 0.4842\n",
      "Epoch 15/100\n",
      "212/212 [==============================] - 34s 159ms/step - loss: 0.2727 - accuracy: 0.8302 - val_loss: 0.6683 - val_accuracy: 0.7895\n",
      "Epoch 16/100\n",
      "212/212 [==============================] - 34s 160ms/step - loss: 0.2984 - accuracy: 0.8078 - val_loss: 1.6630 - val_accuracy: 0.6316\n",
      "Epoch 17/100\n",
      "212/212 [==============================] - 34s 159ms/step - loss: 0.2744 - accuracy: 0.8267 - val_loss: 0.3315 - val_accuracy: 0.8105\n",
      "Epoch 18/100\n",
      "212/212 [==============================] - 34s 159ms/step - loss: 0.2447 - accuracy: 0.8597 - val_loss: 1.0465 - val_accuracy: 0.5263\n",
      "Epoch 19/100\n",
      "212/212 [==============================] - 34s 159ms/step - loss: 0.2103 - accuracy: 0.8797 - val_loss: 0.7652 - val_accuracy: 0.7579\n",
      "Epoch 20/100\n",
      "212/212 [==============================] - 34s 159ms/step - loss: 0.2096 - accuracy: 0.8880 - val_loss: 1.3502 - val_accuracy: 0.5263\n",
      "Epoch 21/100\n",
      "212/212 [==============================] - 34s 159ms/step - loss: 0.2278 - accuracy: 0.8573 - val_loss: 1.5478 - val_accuracy: 0.6000\n",
      "Epoch 22/100\n",
      "212/212 [==============================] - 34s 159ms/step - loss: 0.2039 - accuracy: 0.8892 - val_loss: 1.3211 - val_accuracy: 0.6526\n",
      "Epoch 23/100\n",
      "212/212 [==============================] - 34s 159ms/step - loss: 0.1960 - accuracy: 0.8833 - val_loss: 0.4674 - val_accuracy: 0.7579\n",
      "Epoch 24/100\n",
      "212/212 [==============================] - 34s 160ms/step - loss: 0.1962 - accuracy: 0.8915 - val_loss: 0.3596 - val_accuracy: 0.8000\n",
      "Epoch 25/100\n",
      "212/212 [==============================] - 34s 159ms/step - loss: 0.1807 - accuracy: 0.8974 - val_loss: 0.7476 - val_accuracy: 0.6947\n",
      "Epoch 26/100\n",
      "212/212 [==============================] - 34s 159ms/step - loss: 0.1635 - accuracy: 0.9175 - val_loss: 2.3380 - val_accuracy: 0.5579\n",
      "Epoch 27/100\n",
      "212/212 [==============================] - 34s 159ms/step - loss: 0.1851 - accuracy: 0.8998 - val_loss: 2.1593 - val_accuracy: 0.4632\n",
      "Epoch 28/100\n",
      "212/212 [==============================] - 34s 159ms/step - loss: 0.1447 - accuracy: 0.9340 - val_loss: 0.4211 - val_accuracy: 0.7684\n",
      "Epoch 29/100\n",
      "212/212 [==============================] - 34s 159ms/step - loss: 0.1374 - accuracy: 0.9292 - val_loss: 0.3772 - val_accuracy: 0.8000\n",
      "Epoch 30/100\n",
      "212/212 [==============================] - 34s 159ms/step - loss: 0.1682 - accuracy: 0.9127 - val_loss: 0.5924 - val_accuracy: 0.6947\n",
      "Epoch 31/100\n",
      "212/212 [==============================] - 34s 159ms/step - loss: 0.1679 - accuracy: 0.9045 - val_loss: 0.3705 - val_accuracy: 0.7789\n",
      "Epoch 32/100\n",
      "212/212 [==============================] - 34s 159ms/step - loss: 0.1494 - accuracy: 0.9269 - val_loss: 2.0215 - val_accuracy: 0.5053\n",
      "Epoch 33/100\n",
      "212/212 [==============================] - 34s 159ms/step - loss: 0.1417 - accuracy: 0.9233 - val_loss: 0.4673 - val_accuracy: 0.7579\n",
      "Epoch 34/100\n",
      "212/212 [==============================] - 34s 159ms/step - loss: 0.1333 - accuracy: 0.9363 - val_loss: 0.4354 - val_accuracy: 0.7474\n",
      "Epoch 35/100\n",
      "212/212 [==============================] - 34s 159ms/step - loss: 0.1354 - accuracy: 0.9363 - val_loss: 1.1022 - val_accuracy: 0.6842\n",
      "Epoch 36/100\n",
      "212/212 [==============================] - 34s 159ms/step - loss: 0.1371 - accuracy: 0.9304 - val_loss: 1.9034 - val_accuracy: 0.4316\n",
      "Epoch 37/100\n",
      "212/212 [==============================] - 34s 160ms/step - loss: 0.1211 - accuracy: 0.9434 - val_loss: 0.5022 - val_accuracy: 0.7684\n",
      "Epoch 38/100\n",
      "212/212 [==============================] - 34s 159ms/step - loss: 0.1499 - accuracy: 0.9410 - val_loss: 1.1269 - val_accuracy: 0.5368\n",
      "Epoch 39/100\n",
      "212/212 [==============================] - 34s 159ms/step - loss: 0.1266 - accuracy: 0.9422 - val_loss: 1.8584 - val_accuracy: 0.6737\n",
      "Epoch 40/100\n",
      "212/212 [==============================] - 34s 159ms/step - loss: 0.1427 - accuracy: 0.9316 - val_loss: 0.5531 - val_accuracy: 0.6842\n",
      "Epoch 41/100\n",
      "212/212 [==============================] - 34s 159ms/step - loss: 0.1005 - accuracy: 0.9658 - val_loss: 0.3210 - val_accuracy: 0.8421\n",
      "Epoch 42/100\n",
      "212/212 [==============================] - 34s 160ms/step - loss: 0.0852 - accuracy: 0.9634 - val_loss: 0.3212 - val_accuracy: 0.8105\n",
      "Epoch 43/100\n",
      "212/212 [==============================] - 34s 159ms/step - loss: 0.0961 - accuracy: 0.9646 - val_loss: 0.6863 - val_accuracy: 0.7895\n",
      "Epoch 44/100\n",
      "212/212 [==============================] - 34s 160ms/step - loss: 0.1103 - accuracy: 0.9528 - val_loss: 1.7859 - val_accuracy: 0.6421\n",
      "Epoch 45/100\n",
      "212/212 [==============================] - 34s 159ms/step - loss: 0.0779 - accuracy: 0.9776 - val_loss: 0.3185 - val_accuracy: 0.8526\n",
      "Epoch 46/100\n",
      "212/212 [==============================] - 34s 160ms/step - loss: 0.0915 - accuracy: 0.9623 - val_loss: 1.9098 - val_accuracy: 0.5789\n",
      "Epoch 47/100\n",
      "212/212 [==============================] - 34s 159ms/step - loss: 0.0973 - accuracy: 0.9540 - val_loss: 0.3785 - val_accuracy: 0.7789\n",
      "Epoch 48/100\n",
      "212/212 [==============================] - 34s 160ms/step - loss: 0.0907 - accuracy: 0.9658 - val_loss: 0.6331 - val_accuracy: 0.6632\n",
      "Epoch 49/100\n",
      "212/212 [==============================] - 34s 159ms/step - loss: 0.0792 - accuracy: 0.9717 - val_loss: 0.4688 - val_accuracy: 0.7895\n",
      "Epoch 50/100\n",
      "212/212 [==============================] - 34s 159ms/step - loss: 0.0832 - accuracy: 0.9741 - val_loss: 0.3847 - val_accuracy: 0.8211\n",
      "Epoch 51/100\n",
      "212/212 [==============================] - 34s 159ms/step - loss: 0.0788 - accuracy: 0.9729 - val_loss: 0.6109 - val_accuracy: 0.7895\n",
      "Epoch 52/100\n",
      "212/212 [==============================] - 34s 159ms/step - loss: 0.0719 - accuracy: 0.9693 - val_loss: 0.4499 - val_accuracy: 0.7895\n",
      "Epoch 53/100\n",
      "212/212 [==============================] - 34s 159ms/step - loss: 0.0983 - accuracy: 0.9599 - val_loss: 0.4742 - val_accuracy: 0.8105\n",
      "Epoch 54/100\n",
      "212/212 [==============================] - 34s 159ms/step - loss: 0.0852 - accuracy: 0.9611 - val_loss: 0.6906 - val_accuracy: 0.7474\n",
      "Epoch 55/100\n",
      "212/212 [==============================] - 34s 159ms/step - loss: 0.0966 - accuracy: 0.9611 - val_loss: 0.4624 - val_accuracy: 0.8211\n",
      "Epoch 56/100\n",
      "212/212 [==============================] - 34s 159ms/step - loss: 0.0937 - accuracy: 0.9634 - val_loss: 0.9321 - val_accuracy: 0.6000\n",
      "Epoch 57/100\n",
      "212/212 [==============================] - 34s 159ms/step - loss: 0.0744 - accuracy: 0.9729 - val_loss: 0.3095 - val_accuracy: 0.8105\n",
      "Epoch 58/100\n",
      "212/212 [==============================] - 34s 159ms/step - loss: 0.0819 - accuracy: 0.9646 - val_loss: 0.3161 - val_accuracy: 0.8632\n",
      "Epoch 59/100\n",
      "212/212 [==============================] - 34s 159ms/step - loss: 0.0982 - accuracy: 0.9599 - val_loss: 0.8843 - val_accuracy: 0.7474\n",
      "Epoch 60/100\n",
      "212/212 [==============================] - 34s 159ms/step - loss: 0.0766 - accuracy: 0.9717 - val_loss: 0.3068 - val_accuracy: 0.8316\n",
      "Epoch 61/100\n",
      "212/212 [==============================] - 34s 159ms/step - loss: 0.0636 - accuracy: 0.9752 - val_loss: 0.4869 - val_accuracy: 0.8105\n",
      "Epoch 62/100\n",
      "212/212 [==============================] - 34s 159ms/step - loss: 0.0935 - accuracy: 0.9564 - val_loss: 0.3158 - val_accuracy: 0.8211\n",
      "Epoch 63/100\n",
      "212/212 [==============================] - 34s 160ms/step - loss: 0.0815 - accuracy: 0.9693 - val_loss: 0.3933 - val_accuracy: 0.8211\n",
      "Epoch 64/100\n",
      "212/212 [==============================] - 34s 159ms/step - loss: 0.0532 - accuracy: 0.9788 - val_loss: 0.3307 - val_accuracy: 0.8526\n",
      "Epoch 65/100\n",
      "212/212 [==============================] - 34s 159ms/step - loss: 0.0852 - accuracy: 0.9729 - val_loss: 0.6919 - val_accuracy: 0.8105\n",
      "Epoch 66/100\n",
      "212/212 [==============================] - 34s 159ms/step - loss: 0.0655 - accuracy: 0.9752 - val_loss: 0.3135 - val_accuracy: 0.8842\n",
      "Epoch 67/100\n",
      "212/212 [==============================] - 34s 159ms/step - loss: 0.0871 - accuracy: 0.9599 - val_loss: 0.3026 - val_accuracy: 0.8421\n",
      "Epoch 68/100\n",
      "212/212 [==============================] - 34s 159ms/step - loss: 0.0835 - accuracy: 0.9682 - val_loss: 0.2915 - val_accuracy: 0.8526\n",
      "Epoch 69/100\n",
      "212/212 [==============================] - 34s 159ms/step - loss: 0.0749 - accuracy: 0.9682 - val_loss: 0.4098 - val_accuracy: 0.7895\n",
      "Epoch 70/100\n",
      "212/212 [==============================] - 34s 159ms/step - loss: 0.0618 - accuracy: 0.9741 - val_loss: 0.5349 - val_accuracy: 0.7474\n",
      "Epoch 71/100\n",
      "212/212 [==============================] - 34s 159ms/step - loss: 0.0638 - accuracy: 0.9717 - val_loss: 0.5518 - val_accuracy: 0.7684\n",
      "Epoch 72/100\n",
      "212/212 [==============================] - 34s 159ms/step - loss: 0.0554 - accuracy: 0.9823 - val_loss: 0.3781 - val_accuracy: 0.8000\n",
      "Epoch 73/100\n",
      "212/212 [==============================] - 34s 159ms/step - loss: 0.0675 - accuracy: 0.9705 - val_loss: 0.3079 - val_accuracy: 0.8211\n",
      "Epoch 74/100\n",
      "212/212 [==============================] - 34s 159ms/step - loss: 0.0909 - accuracy: 0.9611 - val_loss: 0.4505 - val_accuracy: 0.8632\n",
      "Epoch 75/100\n",
      "212/212 [==============================] - 34s 159ms/step - loss: 0.0397 - accuracy: 0.9870 - val_loss: 0.3897 - val_accuracy: 0.8316\n",
      "Epoch 76/100\n",
      "212/212 [==============================] - 34s 159ms/step - loss: 0.0426 - accuracy: 0.9894 - val_loss: 0.3269 - val_accuracy: 0.8316\n",
      "Epoch 77/100\n",
      "212/212 [==============================] - 34s 159ms/step - loss: 0.0568 - accuracy: 0.9752 - val_loss: 0.3190 - val_accuracy: 0.8316\n",
      "Epoch 78/100\n",
      "212/212 [==============================] - 34s 159ms/step - loss: 0.0553 - accuracy: 0.9776 - val_loss: 0.3014 - val_accuracy: 0.8526\n",
      "Epoch 79/100\n",
      "212/212 [==============================] - 34s 160ms/step - loss: 0.0931 - accuracy: 0.9634 - val_loss: 0.2979 - val_accuracy: 0.8632\n",
      "Epoch 80/100\n",
      "212/212 [==============================] - 34s 159ms/step - loss: 0.0440 - accuracy: 0.9870 - val_loss: 0.3075 - val_accuracy: 0.8421\n",
      "Epoch 81/100\n",
      "212/212 [==============================] - 34s 159ms/step - loss: 0.0580 - accuracy: 0.9741 - val_loss: 0.3128 - val_accuracy: 0.8316\n",
      "Epoch 82/100\n",
      "212/212 [==============================] - 34s 159ms/step - loss: 0.0608 - accuracy: 0.9741 - val_loss: 0.3085 - val_accuracy: 0.8842\n",
      "Epoch 83/100\n",
      "212/212 [==============================] - 34s 160ms/step - loss: 0.0388 - accuracy: 0.9858 - val_loss: 0.5135 - val_accuracy: 0.7895\n",
      "Epoch 84/100\n",
      "212/212 [==============================] - 34s 159ms/step - loss: 0.0674 - accuracy: 0.9705 - val_loss: 0.3192 - val_accuracy: 0.8421\n",
      "Epoch 85/100\n",
      "212/212 [==============================] - 34s 159ms/step - loss: 0.0703 - accuracy: 0.9670 - val_loss: 0.3499 - val_accuracy: 0.8632\n",
      "Epoch 86/100\n",
      "212/212 [==============================] - 34s 159ms/step - loss: 0.0623 - accuracy: 0.9764 - val_loss: 0.3114 - val_accuracy: 0.8421\n",
      "Epoch 87/100\n",
      "212/212 [==============================] - 34s 159ms/step - loss: 0.0517 - accuracy: 0.9800 - val_loss: 0.2923 - val_accuracy: 0.8316\n",
      "Epoch 88/100\n",
      "212/212 [==============================] - 34s 159ms/step - loss: 0.0630 - accuracy: 0.9729 - val_loss: 0.3116 - val_accuracy: 0.8421\n",
      "Epoch 89/100\n",
      "212/212 [==============================] - 34s 159ms/step - loss: 0.0676 - accuracy: 0.9693 - val_loss: 0.3620 - val_accuracy: 0.8000\n",
      "Epoch 90/100\n",
      "212/212 [==============================] - 34s 159ms/step - loss: 0.0911 - accuracy: 0.9505 - val_loss: 0.3854 - val_accuracy: 0.8316\n",
      "Epoch 91/100\n",
      "212/212 [==============================] - 34s 159ms/step - loss: 0.0697 - accuracy: 0.9693 - val_loss: 0.2992 - val_accuracy: 0.8421\n",
      "Epoch 92/100\n",
      "212/212 [==============================] - 34s 159ms/step - loss: 0.0678 - accuracy: 0.9717 - val_loss: 0.4402 - val_accuracy: 0.8316\n",
      "Epoch 93/100\n",
      "212/212 [==============================] - 34s 159ms/step - loss: 0.0647 - accuracy: 0.9752 - val_loss: 0.3910 - val_accuracy: 0.8105\n",
      "Epoch 94/100\n",
      "212/212 [==============================] - 34s 159ms/step - loss: 0.0524 - accuracy: 0.9800 - val_loss: 0.3073 - val_accuracy: 0.8211\n",
      "Epoch 95/100\n",
      "212/212 [==============================] - 34s 159ms/step - loss: 0.0628 - accuracy: 0.9705 - val_loss: 0.3099 - val_accuracy: 0.8211\n",
      "Epoch 96/100\n",
      "212/212 [==============================] - 34s 159ms/step - loss: 0.0583 - accuracy: 0.9752 - val_loss: 0.3254 - val_accuracy: 0.8632\n",
      "Epoch 97/100\n",
      "212/212 [==============================] - 34s 159ms/step - loss: 0.0808 - accuracy: 0.9658 - val_loss: 0.3389 - val_accuracy: 0.8316\n",
      "Epoch 98/100\n",
      "212/212 [==============================] - 34s 159ms/step - loss: 0.0398 - accuracy: 0.9811 - val_loss: 0.3057 - val_accuracy: 0.8316\n",
      "Epoch 99/100\n",
      "212/212 [==============================] - 34s 159ms/step - loss: 0.0505 - accuracy: 0.9811 - val_loss: 0.2725 - val_accuracy: 0.8211\n",
      "Epoch 100/100\n",
      "212/212 [==============================] - 34s 159ms/step - loss: 0.0604 - accuracy: 0.9729 - val_loss: 0.3028 - val_accuracy: 0.8316\n",
      "Score for fold 1: loss of 0.3027554452419281; accuracy of 83.15789699554443%\n",
      "(848,) (95,)\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 2 ...\n",
      "Epoch 1/100\n",
      "212/212 [==============================] - 41s 164ms/step - loss: 0.5849 - accuracy: 0.5118 - val_loss: 0.8090 - val_accuracy: 0.3474\n",
      "Epoch 2/100\n",
      "212/212 [==============================] - 34s 158ms/step - loss: 0.4901 - accuracy: 0.5896 - val_loss: 0.6334 - val_accuracy: 0.3263\n",
      "Epoch 3/100\n",
      "212/212 [==============================] - 34s 158ms/step - loss: 0.4554 - accuracy: 0.6439 - val_loss: 1.3136 - val_accuracy: 0.3263\n",
      "Epoch 4/100\n",
      "212/212 [==============================] - 34s 159ms/step - loss: 0.4482 - accuracy: 0.6580 - val_loss: 0.5723 - val_accuracy: 0.5474\n",
      "Epoch 5/100\n",
      "212/212 [==============================] - 34s 160ms/step - loss: 0.4355 - accuracy: 0.6769 - val_loss: 0.3440 - val_accuracy: 0.7684\n",
      "Epoch 6/100\n",
      "212/212 [==============================] - 34s 159ms/step - loss: 0.4067 - accuracy: 0.7040 - val_loss: 0.7016 - val_accuracy: 0.4737\n",
      "Epoch 7/100\n",
      "212/212 [==============================] - 34s 159ms/step - loss: 0.3563 - accuracy: 0.7594 - val_loss: 4.5267 - val_accuracy: 0.2632\n",
      "Epoch 8/100\n",
      "212/212 [==============================] - 34s 160ms/step - loss: 0.3890 - accuracy: 0.7347 - val_loss: 0.6501 - val_accuracy: 0.7053\n",
      "Epoch 9/100\n",
      "212/212 [==============================] - 34s 159ms/step - loss: 0.3682 - accuracy: 0.7712 - val_loss: 0.5696 - val_accuracy: 0.6105\n",
      "Epoch 10/100\n",
      "212/212 [==============================] - 34s 159ms/step - loss: 0.3431 - accuracy: 0.7818 - val_loss: 1.2249 - val_accuracy: 0.3474\n",
      "Epoch 11/100\n",
      "212/212 [==============================] - 34s 159ms/step - loss: 0.3172 - accuracy: 0.7948 - val_loss: 0.3084 - val_accuracy: 0.7789\n",
      "Epoch 12/100\n",
      "212/212 [==============================] - 34s 159ms/step - loss: 0.3116 - accuracy: 0.8125 - val_loss: 0.8560 - val_accuracy: 0.5684\n",
      "Epoch 13/100\n",
      "212/212 [==============================] - 34s 159ms/step - loss: 0.3163 - accuracy: 0.7889 - val_loss: 0.2748 - val_accuracy: 0.8421\n",
      "Epoch 14/100\n",
      "212/212 [==============================] - 34s 159ms/step - loss: 0.2789 - accuracy: 0.8361 - val_loss: 1.8390 - val_accuracy: 0.3263\n",
      "Epoch 15/100\n",
      "212/212 [==============================] - 34s 159ms/step - loss: 0.2830 - accuracy: 0.8349 - val_loss: 1.7763 - val_accuracy: 0.3895\n",
      "Epoch 16/100\n",
      "212/212 [==============================] - 34s 159ms/step - loss: 0.2529 - accuracy: 0.8514 - val_loss: 0.3271 - val_accuracy: 0.8000\n",
      "Epoch 17/100\n",
      "212/212 [==============================] - 34s 159ms/step - loss: 0.2406 - accuracy: 0.8538 - val_loss: 0.2980 - val_accuracy: 0.8632\n",
      "Epoch 18/100\n",
      "212/212 [==============================] - 34s 160ms/step - loss: 0.2357 - accuracy: 0.8703 - val_loss: 0.2614 - val_accuracy: 0.8737\n",
      "Epoch 19/100\n",
      "212/212 [==============================] - 34s 159ms/step - loss: 0.2371 - accuracy: 0.8703 - val_loss: 0.4578 - val_accuracy: 0.7474\n",
      "Epoch 20/100\n",
      "212/212 [==============================] - 34s 159ms/step - loss: 0.2204 - accuracy: 0.8774 - val_loss: 1.0156 - val_accuracy: 0.6316\n",
      "Epoch 21/100\n",
      "212/212 [==============================] - 34s 159ms/step - loss: 0.1956 - accuracy: 0.8844 - val_loss: 0.4634 - val_accuracy: 0.7895\n",
      "Epoch 22/100\n",
      "212/212 [==============================] - 34s 159ms/step - loss: 0.1724 - accuracy: 0.9163 - val_loss: 0.4035 - val_accuracy: 0.7263\n",
      "Epoch 23/100\n",
      "212/212 [==============================] - 34s 159ms/step - loss: 0.1987 - accuracy: 0.8880 - val_loss: 0.5227 - val_accuracy: 0.7158\n",
      "Epoch 24/100\n",
      "212/212 [==============================] - 34s 160ms/step - loss: 0.1795 - accuracy: 0.9068 - val_loss: 0.5543 - val_accuracy: 0.6842\n",
      "Epoch 25/100\n",
      "212/212 [==============================] - 34s 160ms/step - loss: 0.1730 - accuracy: 0.9057 - val_loss: 0.2006 - val_accuracy: 0.8842\n",
      "Epoch 26/100\n",
      "212/212 [==============================] - 34s 160ms/step - loss: 0.1662 - accuracy: 0.9186 - val_loss: 1.1657 - val_accuracy: 0.4842\n",
      "Epoch 27/100\n",
      "212/212 [==============================] - 34s 160ms/step - loss: 0.1719 - accuracy: 0.9092 - val_loss: 0.2508 - val_accuracy: 0.8737\n",
      "Epoch 28/100\n",
      "212/212 [==============================] - 34s 159ms/step - loss: 0.1691 - accuracy: 0.9104 - val_loss: 1.5925 - val_accuracy: 0.6105\n",
      "Epoch 29/100\n",
      "212/212 [==============================] - 34s 159ms/step - loss: 0.1613 - accuracy: 0.9068 - val_loss: 0.2049 - val_accuracy: 0.8526\n",
      "Epoch 30/100\n",
      "212/212 [==============================] - 34s 159ms/step - loss: 0.1315 - accuracy: 0.9422 - val_loss: 0.7821 - val_accuracy: 0.6737\n",
      "Epoch 31/100\n",
      "212/212 [==============================] - 34s 159ms/step - loss: 0.1389 - accuracy: 0.9399 - val_loss: 0.2247 - val_accuracy: 0.8842\n",
      "Epoch 32/100\n",
      "212/212 [==============================] - 34s 159ms/step - loss: 0.1262 - accuracy: 0.9469 - val_loss: 0.2487 - val_accuracy: 0.8947\n",
      "Epoch 33/100\n",
      "212/212 [==============================] - 34s 159ms/step - loss: 0.1301 - accuracy: 0.9458 - val_loss: 0.4789 - val_accuracy: 0.7474\n",
      "Epoch 34/100\n",
      "212/212 [==============================] - 34s 159ms/step - loss: 0.1290 - accuracy: 0.9458 - val_loss: 0.1782 - val_accuracy: 0.8842\n",
      "Epoch 35/100\n",
      "212/212 [==============================] - 34s 159ms/step - loss: 0.1095 - accuracy: 0.9517 - val_loss: 0.3978 - val_accuracy: 0.7263\n",
      "Epoch 36/100\n",
      "212/212 [==============================] - 34s 159ms/step - loss: 0.1142 - accuracy: 0.9540 - val_loss: 0.4072 - val_accuracy: 0.7579\n",
      "Epoch 37/100\n",
      "212/212 [==============================] - 34s 159ms/step - loss: 0.0914 - accuracy: 0.9646 - val_loss: 0.6104 - val_accuracy: 0.7579\n",
      "Epoch 38/100\n",
      "212/212 [==============================] - 34s 159ms/step - loss: 0.1074 - accuracy: 0.9564 - val_loss: 0.2147 - val_accuracy: 0.8737\n",
      "Epoch 39/100\n",
      "212/212 [==============================] - 34s 159ms/step - loss: 0.0836 - accuracy: 0.9693 - val_loss: 0.2901 - val_accuracy: 0.8421\n",
      "Epoch 40/100\n",
      "212/212 [==============================] - 34s 159ms/step - loss: 0.0864 - accuracy: 0.9670 - val_loss: 0.2358 - val_accuracy: 0.8526\n",
      "Epoch 41/100\n",
      "212/212 [==============================] - 34s 159ms/step - loss: 0.0989 - accuracy: 0.9517 - val_loss: 0.1869 - val_accuracy: 0.9158\n",
      "Epoch 42/100\n",
      "212/212 [==============================] - 34s 159ms/step - loss: 0.1275 - accuracy: 0.9434 - val_loss: 0.3251 - val_accuracy: 0.8421\n",
      "Epoch 43/100\n",
      "212/212 [==============================] - 34s 159ms/step - loss: 0.0870 - accuracy: 0.9670 - val_loss: 0.3140 - val_accuracy: 0.8526\n",
      "Epoch 44/100\n",
      "212/212 [==============================] - 34s 159ms/step - loss: 0.1017 - accuracy: 0.9599 - val_loss: 0.3641 - val_accuracy: 0.8000\n",
      "Epoch 45/100\n",
      "212/212 [==============================] - 34s 159ms/step - loss: 0.0847 - accuracy: 0.9611 - val_loss: 0.1733 - val_accuracy: 0.8842\n",
      "Epoch 46/100\n",
      "212/212 [==============================] - 34s 159ms/step - loss: 0.1074 - accuracy: 0.9552 - val_loss: 0.3451 - val_accuracy: 0.8316\n",
      "Epoch 47/100\n",
      "212/212 [==============================] - 34s 159ms/step - loss: 0.0889 - accuracy: 0.9634 - val_loss: 0.2598 - val_accuracy: 0.8842\n",
      "Epoch 48/100\n",
      "212/212 [==============================] - 34s 159ms/step - loss: 0.0707 - accuracy: 0.9788 - val_loss: 0.2688 - val_accuracy: 0.8842\n",
      "Epoch 49/100\n",
      "212/212 [==============================] - 34s 159ms/step - loss: 0.0847 - accuracy: 0.9670 - val_loss: 0.2937 - val_accuracy: 0.8632\n",
      "Epoch 50/100\n",
      "212/212 [==============================] - 34s 159ms/step - loss: 0.0653 - accuracy: 0.9811 - val_loss: 0.2914 - val_accuracy: 0.8526\n",
      "Epoch 51/100\n",
      "212/212 [==============================] - 34s 160ms/step - loss: 0.0632 - accuracy: 0.9823 - val_loss: 0.1919 - val_accuracy: 0.8737\n",
      "Epoch 52/100\n",
      "212/212 [==============================] - 34s 159ms/step - loss: 0.0695 - accuracy: 0.9729 - val_loss: 0.1898 - val_accuracy: 0.8842\n",
      "Epoch 53/100\n",
      "212/212 [==============================] - 34s 159ms/step - loss: 0.0574 - accuracy: 0.9858 - val_loss: 0.2011 - val_accuracy: 0.8632\n",
      "Epoch 54/100\n",
      "212/212 [==============================] - 34s 159ms/step - loss: 0.0940 - accuracy: 0.9670 - val_loss: 0.2229 - val_accuracy: 0.8526\n",
      "Epoch 55/100\n",
      "212/212 [==============================] - 34s 159ms/step - loss: 0.0752 - accuracy: 0.9693 - val_loss: 0.2140 - val_accuracy: 0.8842\n",
      "Epoch 56/100\n",
      "212/212 [==============================] - 34s 159ms/step - loss: 0.0869 - accuracy: 0.9611 - val_loss: 0.1638 - val_accuracy: 0.9158\n",
      "Epoch 57/100\n",
      "212/212 [==============================] - 34s 160ms/step - loss: 0.0905 - accuracy: 0.9646 - val_loss: 0.2806 - val_accuracy: 0.8632\n",
      "Epoch 58/100\n",
      "212/212 [==============================] - 34s 159ms/step - loss: 0.0516 - accuracy: 0.9788 - val_loss: 0.2096 - val_accuracy: 0.8947\n",
      "Epoch 59/100\n",
      "212/212 [==============================] - 34s 159ms/step - loss: 0.0479 - accuracy: 0.9823 - val_loss: 0.1757 - val_accuracy: 0.8947\n",
      "Epoch 60/100\n",
      "212/212 [==============================] - 34s 159ms/step - loss: 0.0658 - accuracy: 0.9776 - val_loss: 0.2265 - val_accuracy: 0.8737\n",
      "Epoch 61/100\n",
      "212/212 [==============================] - 34s 159ms/step - loss: 0.0705 - accuracy: 0.9705 - val_loss: 0.3046 - val_accuracy: 0.8737\n",
      "Epoch 62/100\n",
      "212/212 [==============================] - 34s 159ms/step - loss: 0.0726 - accuracy: 0.9717 - val_loss: 0.3293 - val_accuracy: 0.8526\n",
      "Epoch 63/100\n",
      "212/212 [==============================] - 34s 159ms/step - loss: 0.0716 - accuracy: 0.9693 - val_loss: 0.2229 - val_accuracy: 0.8842\n",
      "Epoch 64/100\n",
      "212/212 [==============================] - 34s 159ms/step - loss: 0.0646 - accuracy: 0.9741 - val_loss: 0.1773 - val_accuracy: 0.9158\n",
      "Epoch 65/100\n",
      "212/212 [==============================] - 34s 159ms/step - loss: 0.0691 - accuracy: 0.9729 - val_loss: 0.1936 - val_accuracy: 0.8947\n",
      "Epoch 66/100\n",
      "212/212 [==============================] - 34s 160ms/step - loss: 0.0435 - accuracy: 0.9870 - val_loss: 0.2056 - val_accuracy: 0.9053\n",
      "Epoch 67/100\n",
      "212/212 [==============================] - 34s 159ms/step - loss: 0.0818 - accuracy: 0.9634 - val_loss: 0.1774 - val_accuracy: 0.8947\n",
      "Epoch 68/100\n",
      "212/212 [==============================] - 34s 159ms/step - loss: 0.0804 - accuracy: 0.9646 - val_loss: 0.3525 - val_accuracy: 0.8632\n",
      "Epoch 69/100\n",
      "212/212 [==============================] - 34s 159ms/step - loss: 0.0770 - accuracy: 0.9634 - val_loss: 0.2689 - val_accuracy: 0.8632\n",
      "Epoch 70/100\n",
      "212/212 [==============================] - 34s 159ms/step - loss: 0.0560 - accuracy: 0.9811 - val_loss: 0.1929 - val_accuracy: 0.9158\n",
      "Epoch 71/100\n",
      "212/212 [==============================] - 34s 160ms/step - loss: 0.0586 - accuracy: 0.9729 - val_loss: 0.1696 - val_accuracy: 0.9053\n",
      "Epoch 72/100\n",
      "212/212 [==============================] - 34s 159ms/step - loss: 0.0650 - accuracy: 0.9776 - val_loss: 0.1961 - val_accuracy: 0.9053\n",
      "Epoch 73/100\n",
      "212/212 [==============================] - 34s 159ms/step - loss: 0.0669 - accuracy: 0.9729 - val_loss: 0.1693 - val_accuracy: 0.9158\n",
      "Epoch 74/100\n",
      "212/212 [==============================] - 34s 159ms/step - loss: 0.0750 - accuracy: 0.9658 - val_loss: 0.2912 - val_accuracy: 0.8842\n",
      "Epoch 75/100\n",
      "212/212 [==============================] - 34s 159ms/step - loss: 0.0735 - accuracy: 0.9705 - val_loss: 0.2170 - val_accuracy: 0.8947\n",
      "Epoch 76/100\n",
      "212/212 [==============================] - 34s 159ms/step - loss: 0.0405 - accuracy: 0.9823 - val_loss: 0.2096 - val_accuracy: 0.9053\n",
      "Epoch 77/100\n",
      "212/212 [==============================] - 34s 159ms/step - loss: 0.0607 - accuracy: 0.9752 - val_loss: 0.1652 - val_accuracy: 0.9158\n",
      "Epoch 78/100\n",
      "212/212 [==============================] - 34s 159ms/step - loss: 0.0716 - accuracy: 0.9705 - val_loss: 0.1651 - val_accuracy: 0.9158\n",
      "Epoch 79/100\n",
      "212/212 [==============================] - 34s 159ms/step - loss: 0.0676 - accuracy: 0.9729 - val_loss: 0.1787 - val_accuracy: 0.9158\n",
      "Epoch 80/100\n",
      "212/212 [==============================] - 34s 159ms/step - loss: 0.0439 - accuracy: 0.9811 - val_loss: 0.1604 - val_accuracy: 0.9158\n",
      "Epoch 81/100\n",
      "212/212 [==============================] - 34s 160ms/step - loss: 0.0628 - accuracy: 0.9776 - val_loss: 0.1796 - val_accuracy: 0.9158\n",
      "Epoch 82/100\n",
      "212/212 [==============================] - 34s 159ms/step - loss: 0.0679 - accuracy: 0.9752 - val_loss: 0.1744 - val_accuracy: 0.9158\n",
      "Epoch 83/100\n",
      "212/212 [==============================] - 34s 159ms/step - loss: 0.0766 - accuracy: 0.9658 - val_loss: 0.1617 - val_accuracy: 0.8947\n",
      "Epoch 84/100\n",
      "212/212 [==============================] - 34s 159ms/step - loss: 0.0751 - accuracy: 0.9634 - val_loss: 0.1619 - val_accuracy: 0.9053\n",
      "Epoch 85/100\n",
      "212/212 [==============================] - 34s 159ms/step - loss: 0.0776 - accuracy: 0.9611 - val_loss: 0.1527 - val_accuracy: 0.9158\n",
      "Epoch 86/100\n",
      "212/212 [==============================] - 34s 159ms/step - loss: 0.0743 - accuracy: 0.9682 - val_loss: 0.1650 - val_accuracy: 0.9053\n",
      "Epoch 87/100\n",
      "212/212 [==============================] - 34s 159ms/step - loss: 0.0861 - accuracy: 0.9646 - val_loss: 0.1728 - val_accuracy: 0.9158\n",
      "Epoch 88/100\n",
      "212/212 [==============================] - 34s 159ms/step - loss: 0.0531 - accuracy: 0.9776 - val_loss: 0.1646 - val_accuracy: 0.9053\n",
      "Epoch 89/100\n",
      "212/212 [==============================] - 34s 159ms/step - loss: 0.0550 - accuracy: 0.9776 - val_loss: 0.1796 - val_accuracy: 0.9158\n",
      "Epoch 90/100\n",
      "212/212 [==============================] - 34s 159ms/step - loss: 0.0872 - accuracy: 0.9611 - val_loss: 0.1762 - val_accuracy: 0.9158\n",
      "Epoch 91/100\n",
      "212/212 [==============================] - 34s 159ms/step - loss: 0.0605 - accuracy: 0.9717 - val_loss: 0.2298 - val_accuracy: 0.8737\n",
      "Epoch 92/100\n",
      "212/212 [==============================] - 34s 159ms/step - loss: 0.0435 - accuracy: 0.9847 - val_loss: 0.3118 - val_accuracy: 0.8737\n",
      "Epoch 93/100\n",
      "212/212 [==============================] - 34s 160ms/step - loss: 0.0650 - accuracy: 0.9682 - val_loss: 0.1620 - val_accuracy: 0.9158\n",
      "Epoch 94/100\n",
      "212/212 [==============================] - 34s 159ms/step - loss: 0.0566 - accuracy: 0.9752 - val_loss: 0.1804 - val_accuracy: 0.9053\n",
      "Epoch 95/100\n",
      "212/212 [==============================] - 34s 159ms/step - loss: 0.0443 - accuracy: 0.9823 - val_loss: 0.1904 - val_accuracy: 0.9158\n",
      "Epoch 96/100\n",
      "212/212 [==============================] - 34s 159ms/step - loss: 0.0449 - accuracy: 0.9858 - val_loss: 0.1761 - val_accuracy: 0.9158\n",
      "Epoch 97/100\n",
      "212/212 [==============================] - 34s 159ms/step - loss: 0.0876 - accuracy: 0.9611 - val_loss: 0.1728 - val_accuracy: 0.9053\n",
      "Epoch 98/100\n",
      "212/212 [==============================] - 34s 159ms/step - loss: 0.0636 - accuracy: 0.9658 - val_loss: 0.1850 - val_accuracy: 0.9158\n",
      "Epoch 99/100\n",
      "212/212 [==============================] - 34s 159ms/step - loss: 0.0492 - accuracy: 0.9788 - val_loss: 0.2568 - val_accuracy: 0.8737\n",
      "Epoch 100/100\n",
      "212/212 [==============================] - 34s 159ms/step - loss: 0.0662 - accuracy: 0.9717 - val_loss: 0.1716 - val_accuracy: 0.9053\n",
      "Score for fold 2: loss of 0.1716012805700302; accuracy of 90.52631855010986%\n",
      "(848,) (95,)\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 3 ...\n",
      "Epoch 1/100\n",
      "212/212 [==============================] - 41s 163ms/step - loss: 0.6305 - accuracy: 0.4257 - val_loss: 0.7998 - val_accuracy: 0.3474\n",
      "Epoch 2/100\n",
      "212/212 [==============================] - 34s 158ms/step - loss: 0.5214 - accuracy: 0.5991 - val_loss: 1.0595 - val_accuracy: 0.3263\n",
      "Epoch 3/100\n",
      "212/212 [==============================] - 34s 158ms/step - loss: 0.4581 - accuracy: 0.6545 - val_loss: 22.2209 - val_accuracy: 0.2737\n",
      "Epoch 4/100\n",
      "212/212 [==============================] - 34s 158ms/step - loss: 0.4376 - accuracy: 0.6769 - val_loss: 0.6554 - val_accuracy: 0.3684\n",
      "Epoch 5/100\n",
      "212/212 [==============================] - 34s 158ms/step - loss: 0.4488 - accuracy: 0.6521 - val_loss: 1.2855 - val_accuracy: 0.3579\n",
      "Epoch 6/100\n",
      "212/212 [==============================] - 34s 158ms/step - loss: 0.4031 - accuracy: 0.7252 - val_loss: 1.7968 - val_accuracy: 0.3895\n",
      "Epoch 7/100\n",
      "212/212 [==============================] - 34s 159ms/step - loss: 0.3833 - accuracy: 0.7417 - val_loss: 1.0140 - val_accuracy: 0.4842\n",
      "Epoch 8/100\n",
      "212/212 [==============================] - 34s 158ms/step - loss: 0.4011 - accuracy: 0.7111 - val_loss: 1.4054 - val_accuracy: 0.7579\n",
      "Epoch 9/100\n",
      "212/212 [==============================] - 34s 158ms/step - loss: 0.3635 - accuracy: 0.7476 - val_loss: 0.8157 - val_accuracy: 0.4947\n",
      "Epoch 10/100\n",
      "212/212 [==============================] - 34s 159ms/step - loss: 0.3471 - accuracy: 0.7689 - val_loss: 4.4866 - val_accuracy: 0.3474\n",
      "Epoch 11/100\n",
      "212/212 [==============================] - 34s 158ms/step - loss: 0.3331 - accuracy: 0.7830 - val_loss: 0.6231 - val_accuracy: 0.5684\n",
      "Epoch 12/100\n",
      "212/212 [==============================] - 34s 158ms/step - loss: 0.3306 - accuracy: 0.7748 - val_loss: 3.4890 - val_accuracy: 0.4105\n",
      "Epoch 13/100\n",
      "212/212 [==============================] - 34s 158ms/step - loss: 0.3125 - accuracy: 0.8042 - val_loss: 0.4502 - val_accuracy: 0.7263\n",
      "Epoch 14/100\n",
      "212/212 [==============================] - 34s 159ms/step - loss: 0.3108 - accuracy: 0.7925 - val_loss: 1.1542 - val_accuracy: 0.7158\n",
      "Epoch 15/100\n",
      "212/212 [==============================] - 34s 158ms/step - loss: 0.2865 - accuracy: 0.8219 - val_loss: 1.0774 - val_accuracy: 0.6947\n",
      "Epoch 16/100\n",
      "212/212 [==============================] - 34s 159ms/step - loss: 0.2634 - accuracy: 0.8479 - val_loss: 0.2374 - val_accuracy: 0.8737\n",
      "Epoch 17/100\n",
      "212/212 [==============================] - 34s 158ms/step - loss: 0.2642 - accuracy: 0.8502 - val_loss: 1.3008 - val_accuracy: 0.5579\n",
      "Epoch 18/100\n",
      "212/212 [==============================] - 34s 158ms/step - loss: 0.2350 - accuracy: 0.8608 - val_loss: 0.1994 - val_accuracy: 0.8526\n",
      "Epoch 19/100\n",
      "212/212 [==============================] - 34s 159ms/step - loss: 0.2671 - accuracy: 0.8396 - val_loss: 0.5104 - val_accuracy: 0.7474\n",
      "Epoch 20/100\n",
      "212/212 [==============================] - 34s 159ms/step - loss: 0.2475 - accuracy: 0.8573 - val_loss: 0.3299 - val_accuracy: 0.7684\n",
      "Epoch 21/100\n",
      "212/212 [==============================] - 34s 159ms/step - loss: 0.2063 - accuracy: 0.8915 - val_loss: 0.3908 - val_accuracy: 0.7263\n",
      "Epoch 22/100\n",
      "212/212 [==============================] - 34s 159ms/step - loss: 0.2090 - accuracy: 0.8821 - val_loss: 0.5123 - val_accuracy: 0.7053\n",
      "Epoch 23/100\n",
      "212/212 [==============================] - 34s 159ms/step - loss: 0.2058 - accuracy: 0.8762 - val_loss: 0.6245 - val_accuracy: 0.6947\n",
      "Epoch 24/100\n",
      "212/212 [==============================] - 34s 158ms/step - loss: 0.2125 - accuracy: 0.8785 - val_loss: 0.3384 - val_accuracy: 0.8316\n",
      "Epoch 25/100\n",
      "212/212 [==============================] - 34s 158ms/step - loss: 0.2085 - accuracy: 0.8880 - val_loss: 0.3103 - val_accuracy: 0.8421\n",
      "Epoch 26/100\n",
      "212/212 [==============================] - 34s 158ms/step - loss: 0.1935 - accuracy: 0.8939 - val_loss: 0.2479 - val_accuracy: 0.8526\n",
      "Epoch 27/100\n",
      "212/212 [==============================] - 34s 159ms/step - loss: 0.1731 - accuracy: 0.9057 - val_loss: 0.6408 - val_accuracy: 0.6842\n",
      "Epoch 28/100\n",
      "212/212 [==============================] - 34s 158ms/step - loss: 0.1562 - accuracy: 0.9222 - val_loss: 0.2920 - val_accuracy: 0.8316\n",
      "Epoch 29/100\n",
      "212/212 [==============================] - 34s 160ms/step - loss: 0.1522 - accuracy: 0.9340 - val_loss: 0.2171 - val_accuracy: 0.8526\n",
      "Epoch 30/100\n",
      "212/212 [==============================] - 34s 158ms/step - loss: 0.1442 - accuracy: 0.9281 - val_loss: 0.2680 - val_accuracy: 0.8000\n",
      "Epoch 31/100\n",
      "212/212 [==============================] - 34s 159ms/step - loss: 0.1348 - accuracy: 0.9304 - val_loss: 0.2707 - val_accuracy: 0.8211\n",
      "Epoch 32/100\n",
      "212/212 [==============================] - 34s 159ms/step - loss: 0.1453 - accuracy: 0.9186 - val_loss: 0.3299 - val_accuracy: 0.7789\n",
      "Epoch 33/100\n",
      "212/212 [==============================] - 34s 159ms/step - loss: 0.1435 - accuracy: 0.9257 - val_loss: 0.4999 - val_accuracy: 0.7579\n",
      "Epoch 34/100\n",
      "212/212 [==============================] - 34s 159ms/step - loss: 0.1598 - accuracy: 0.9328 - val_loss: 0.2596 - val_accuracy: 0.8316\n",
      "Epoch 35/100\n",
      "212/212 [==============================] - 34s 159ms/step - loss: 0.1499 - accuracy: 0.9304 - val_loss: 0.5226 - val_accuracy: 0.7263\n",
      "Epoch 36/100\n",
      "212/212 [==============================] - 34s 158ms/step - loss: 0.1031 - accuracy: 0.9575 - val_loss: 0.6399 - val_accuracy: 0.7263\n",
      "Epoch 37/100\n",
      "212/212 [==============================] - 34s 158ms/step - loss: 0.1555 - accuracy: 0.9399 - val_loss: 0.4938 - val_accuracy: 0.7474\n",
      "Epoch 38/100\n",
      "212/212 [==============================] - 34s 158ms/step - loss: 0.1016 - accuracy: 0.9611 - val_loss: 0.2258 - val_accuracy: 0.8421\n",
      "Epoch 39/100\n",
      "212/212 [==============================] - 34s 158ms/step - loss: 0.1132 - accuracy: 0.9469 - val_loss: 0.3695 - val_accuracy: 0.7789\n",
      "Epoch 40/100\n",
      "212/212 [==============================] - 34s 158ms/step - loss: 0.1228 - accuracy: 0.9458 - val_loss: 1.1994 - val_accuracy: 0.5579\n",
      "Epoch 41/100\n",
      "212/212 [==============================] - 34s 158ms/step - loss: 0.0898 - accuracy: 0.9658 - val_loss: 0.2110 - val_accuracy: 0.8421\n",
      "Epoch 42/100\n",
      "212/212 [==============================] - 34s 158ms/step - loss: 0.1067 - accuracy: 0.9611 - val_loss: 0.2273 - val_accuracy: 0.8421\n",
      "Epoch 43/100\n",
      "212/212 [==============================] - 34s 158ms/step - loss: 0.0950 - accuracy: 0.9611 - val_loss: 0.3881 - val_accuracy: 0.7895\n",
      "Epoch 44/100\n",
      "212/212 [==============================] - 34s 158ms/step - loss: 0.0877 - accuracy: 0.9646 - val_loss: 0.3157 - val_accuracy: 0.8000\n",
      "Epoch 45/100\n",
      "212/212 [==============================] - 34s 158ms/step - loss: 0.0802 - accuracy: 0.9693 - val_loss: 0.3468 - val_accuracy: 0.8211\n",
      "Epoch 46/100\n",
      "212/212 [==============================] - 34s 158ms/step - loss: 0.0868 - accuracy: 0.9693 - val_loss: 0.9346 - val_accuracy: 0.6421\n",
      "Epoch 47/100\n",
      "212/212 [==============================] - 34s 158ms/step - loss: 0.0965 - accuracy: 0.9611 - val_loss: 0.2655 - val_accuracy: 0.8421\n",
      "Epoch 48/100\n",
      "212/212 [==============================] - 34s 158ms/step - loss: 0.1002 - accuracy: 0.9670 - val_loss: 0.2298 - val_accuracy: 0.8526\n",
      "Epoch 49/100\n",
      "212/212 [==============================] - 34s 158ms/step - loss: 0.1012 - accuracy: 0.9646 - val_loss: 0.3052 - val_accuracy: 0.8632\n",
      "Epoch 50/100\n",
      "212/212 [==============================] - 34s 158ms/step - loss: 0.1032 - accuracy: 0.9599 - val_loss: 0.5255 - val_accuracy: 0.7368\n",
      "Epoch 51/100\n",
      "212/212 [==============================] - 34s 158ms/step - loss: 0.0972 - accuracy: 0.9611 - val_loss: 0.3517 - val_accuracy: 0.8000\n",
      "Epoch 52/100\n",
      "212/212 [==============================] - 34s 158ms/step - loss: 0.0897 - accuracy: 0.9623 - val_loss: 0.3111 - val_accuracy: 0.8211\n",
      "Epoch 53/100\n",
      "212/212 [==============================] - 34s 159ms/step - loss: 0.0911 - accuracy: 0.9623 - val_loss: 0.2769 - val_accuracy: 0.8421\n",
      "Epoch 54/100\n",
      "212/212 [==============================] - 34s 158ms/step - loss: 0.1125 - accuracy: 0.9422 - val_loss: 0.9404 - val_accuracy: 0.6737\n",
      "Epoch 55/100\n",
      "212/212 [==============================] - 34s 158ms/step - loss: 0.0516 - accuracy: 0.9823 - val_loss: 0.2114 - val_accuracy: 0.8632\n",
      "Epoch 56/100\n",
      "212/212 [==============================] - 34s 158ms/step - loss: 0.0931 - accuracy: 0.9658 - val_loss: 0.2059 - val_accuracy: 0.8842\n",
      "Epoch 57/100\n",
      "212/212 [==============================] - 34s 159ms/step - loss: 0.0661 - accuracy: 0.9800 - val_loss: 0.2042 - val_accuracy: 0.8842\n",
      "Epoch 58/100\n",
      "212/212 [==============================] - 34s 158ms/step - loss: 0.0825 - accuracy: 0.9717 - val_loss: 0.2742 - val_accuracy: 0.8211\n",
      "Epoch 59/100\n",
      "212/212 [==============================] - 34s 159ms/step - loss: 0.0640 - accuracy: 0.9741 - val_loss: 0.2137 - val_accuracy: 0.8632\n",
      "Epoch 60/100\n",
      "212/212 [==============================] - 34s 158ms/step - loss: 0.0671 - accuracy: 0.9717 - val_loss: 0.3102 - val_accuracy: 0.8211\n",
      "Epoch 61/100\n",
      "212/212 [==============================] - 34s 158ms/step - loss: 0.0649 - accuracy: 0.9764 - val_loss: 0.3164 - val_accuracy: 0.7895\n",
      "Epoch 62/100\n",
      "212/212 [==============================] - 34s 159ms/step - loss: 0.0801 - accuracy: 0.9634 - val_loss: 0.2075 - val_accuracy: 0.8632\n",
      "Epoch 63/100\n",
      "212/212 [==============================] - 34s 158ms/step - loss: 0.0698 - accuracy: 0.9729 - val_loss: 0.2323 - val_accuracy: 0.8526\n",
      "Epoch 64/100\n",
      "212/212 [==============================] - 34s 159ms/step - loss: 0.0974 - accuracy: 0.9505 - val_loss: 0.3425 - val_accuracy: 0.8211\n",
      "Epoch 65/100\n",
      "212/212 [==============================] - 34s 158ms/step - loss: 0.0539 - accuracy: 0.9811 - val_loss: 0.2163 - val_accuracy: 0.8632\n",
      "Epoch 66/100\n",
      "212/212 [==============================] - 34s 159ms/step - loss: 0.0734 - accuracy: 0.9670 - val_loss: 0.2355 - val_accuracy: 0.8632\n",
      "Epoch 67/100\n",
      "212/212 [==============================] - 34s 158ms/step - loss: 0.1015 - accuracy: 0.9564 - val_loss: 0.3444 - val_accuracy: 0.8526\n",
      "Epoch 68/100\n",
      "212/212 [==============================] - 34s 159ms/step - loss: 0.0648 - accuracy: 0.9729 - val_loss: 0.4318 - val_accuracy: 0.8105\n",
      "Epoch 69/100\n",
      "212/212 [==============================] - 34s 159ms/step - loss: 0.0861 - accuracy: 0.9646 - val_loss: 0.2055 - val_accuracy: 0.8737\n",
      "Epoch 70/100\n",
      "212/212 [==============================] - 34s 159ms/step - loss: 0.0644 - accuracy: 0.9741 - val_loss: 0.2515 - val_accuracy: 0.8526\n",
      "Epoch 71/100\n",
      "212/212 [==============================] - 34s 159ms/step - loss: 0.0659 - accuracy: 0.9764 - val_loss: 0.2684 - val_accuracy: 0.8421\n",
      "Epoch 72/100\n",
      "212/212 [==============================] - 34s 158ms/step - loss: 0.1025 - accuracy: 0.9540 - val_loss: 0.2104 - val_accuracy: 0.8842\n",
      "Epoch 73/100\n",
      "212/212 [==============================] - 34s 158ms/step - loss: 0.0667 - accuracy: 0.9705 - val_loss: 0.2443 - val_accuracy: 0.8526\n",
      "Epoch 74/100\n",
      "212/212 [==============================] - 34s 158ms/step - loss: 0.0701 - accuracy: 0.9682 - val_loss: 0.2187 - val_accuracy: 0.8526\n",
      "Epoch 75/100\n",
      "212/212 [==============================] - 34s 158ms/step - loss: 0.0531 - accuracy: 0.9776 - val_loss: 0.2117 - val_accuracy: 0.8842\n",
      "Epoch 76/100\n",
      "212/212 [==============================] - 34s 158ms/step - loss: 0.0715 - accuracy: 0.9658 - val_loss: 0.2871 - val_accuracy: 0.8211\n",
      "Epoch 77/100\n",
      "212/212 [==============================] - 34s 159ms/step - loss: 0.0724 - accuracy: 0.9682 - val_loss: 0.2356 - val_accuracy: 0.8632\n",
      "Epoch 78/100\n",
      "212/212 [==============================] - 34s 158ms/step - loss: 0.0762 - accuracy: 0.9705 - val_loss: 0.2737 - val_accuracy: 0.8632\n",
      "Epoch 79/100\n",
      "212/212 [==============================] - 34s 158ms/step - loss: 0.0741 - accuracy: 0.9741 - val_loss: 0.2654 - val_accuracy: 0.8526\n",
      "Epoch 80/100\n",
      "212/212 [==============================] - 34s 158ms/step - loss: 0.0610 - accuracy: 0.9752 - val_loss: 0.2497 - val_accuracy: 0.8421\n",
      "Epoch 81/100\n",
      "212/212 [==============================] - 34s 158ms/step - loss: 0.0541 - accuracy: 0.9823 - val_loss: 0.2920 - val_accuracy: 0.8316\n",
      "Epoch 82/100\n",
      "212/212 [==============================] - 34s 159ms/step - loss: 0.0738 - accuracy: 0.9693 - val_loss: 0.2522 - val_accuracy: 0.8526\n",
      "Epoch 83/100\n",
      "212/212 [==============================] - 34s 158ms/step - loss: 0.0658 - accuracy: 0.9764 - val_loss: 0.2229 - val_accuracy: 0.8737\n",
      "Epoch 84/100\n",
      "212/212 [==============================] - 34s 158ms/step - loss: 0.0494 - accuracy: 0.9823 - val_loss: 0.2298 - val_accuracy: 0.8526\n",
      "Epoch 85/100\n",
      "212/212 [==============================] - 34s 158ms/step - loss: 0.0637 - accuracy: 0.9752 - val_loss: 0.2222 - val_accuracy: 0.8632\n",
      "Epoch 86/100\n",
      "212/212 [==============================] - 34s 159ms/step - loss: 0.0625 - accuracy: 0.9764 - val_loss: 0.2192 - val_accuracy: 0.8737\n",
      "Epoch 87/100\n",
      "212/212 [==============================] - 34s 158ms/step - loss: 0.0585 - accuracy: 0.9752 - val_loss: 0.2861 - val_accuracy: 0.8421\n",
      "Epoch 88/100\n",
      "212/212 [==============================] - 34s 159ms/step - loss: 0.0567 - accuracy: 0.9764 - val_loss: 0.2536 - val_accuracy: 0.8632\n",
      "Epoch 89/100\n",
      "212/212 [==============================] - 34s 158ms/step - loss: 0.0282 - accuracy: 0.9917 - val_loss: 0.2391 - val_accuracy: 0.8526\n",
      "Epoch 90/100\n",
      "212/212 [==============================] - 34s 158ms/step - loss: 0.0368 - accuracy: 0.9858 - val_loss: 0.2424 - val_accuracy: 0.8526\n",
      "Epoch 91/100\n",
      "212/212 [==============================] - 34s 158ms/step - loss: 0.0563 - accuracy: 0.9752 - val_loss: 0.2261 - val_accuracy: 0.8526\n",
      "Epoch 92/100\n",
      "212/212 [==============================] - 34s 158ms/step - loss: 0.0792 - accuracy: 0.9682 - val_loss: 0.2228 - val_accuracy: 0.8632\n",
      "Epoch 93/100\n",
      "212/212 [==============================] - 34s 158ms/step - loss: 0.0459 - accuracy: 0.9870 - val_loss: 0.2286 - val_accuracy: 0.8737\n",
      "Epoch 94/100\n",
      "212/212 [==============================] - 34s 158ms/step - loss: 0.0535 - accuracy: 0.9800 - val_loss: 0.2191 - val_accuracy: 0.8632\n",
      "Epoch 95/100\n",
      "212/212 [==============================] - 34s 158ms/step - loss: 0.0573 - accuracy: 0.9776 - val_loss: 0.2289 - val_accuracy: 0.8632\n",
      "Epoch 96/100\n",
      "212/212 [==============================] - 34s 158ms/step - loss: 0.0519 - accuracy: 0.9788 - val_loss: 0.3345 - val_accuracy: 0.8316\n",
      "Epoch 97/100\n",
      "212/212 [==============================] - 34s 158ms/step - loss: 0.0706 - accuracy: 0.9741 - val_loss: 0.2711 - val_accuracy: 0.8421\n",
      "Epoch 98/100\n",
      "212/212 [==============================] - 34s 158ms/step - loss: 0.0636 - accuracy: 0.9752 - val_loss: 0.2230 - val_accuracy: 0.8632\n",
      "Epoch 99/100\n",
      "212/212 [==============================] - 34s 159ms/step - loss: 0.0354 - accuracy: 0.9894 - val_loss: 0.2768 - val_accuracy: 0.8526\n",
      "Epoch 100/100\n",
      "212/212 [==============================] - 34s 159ms/step - loss: 0.0612 - accuracy: 0.9682 - val_loss: 0.2256 - val_accuracy: 0.8632\n",
      "Score for fold 3: loss of 0.2256220430135727; accuracy of 86.3157868385315%\n",
      "(849,) (94,)\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 4 ...\n",
      "Epoch 1/100\n",
      "213/213 [==============================] - 47s 185ms/step - loss: 0.6229 - accuracy: 0.4264 - val_loss: 0.9438 - val_accuracy: 0.3511\n",
      "Epoch 2/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.5346 - accuracy: 0.5512 - val_loss: 1.0109 - val_accuracy: 0.3511\n",
      "Epoch 3/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.4811 - accuracy: 0.5948 - val_loss: 5.3169 - val_accuracy: 0.3191\n",
      "Epoch 4/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.4389 - accuracy: 0.6890 - val_loss: 1.0735 - val_accuracy: 0.3511\n",
      "Epoch 5/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.4205 - accuracy: 0.6902 - val_loss: 0.7373 - val_accuracy: 0.3830\n",
      "Epoch 6/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.4180 - accuracy: 0.6808 - val_loss: 0.5489 - val_accuracy: 0.7553\n",
      "Epoch 7/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.3988 - accuracy: 0.7102 - val_loss: 0.2773 - val_accuracy: 0.8511\n",
      "Epoch 8/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.3977 - accuracy: 0.7126 - val_loss: 0.2637 - val_accuracy: 0.8191\n",
      "Epoch 9/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.3652 - accuracy: 0.7468 - val_loss: 1.0657 - val_accuracy: 0.4787\n",
      "Epoch 10/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.3411 - accuracy: 0.7927 - val_loss: 2.8848 - val_accuracy: 0.3723\n",
      "Epoch 11/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.3144 - accuracy: 0.8021 - val_loss: 1.6937 - val_accuracy: 0.3830\n",
      "Epoch 12/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.3681 - accuracy: 0.7597 - val_loss: 17.1359 - val_accuracy: 0.3191\n",
      "Epoch 13/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.3326 - accuracy: 0.7892 - val_loss: 0.2767 - val_accuracy: 0.7872\n",
      "Epoch 14/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.3057 - accuracy: 0.8080 - val_loss: 0.4087 - val_accuracy: 0.7872\n",
      "Epoch 15/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.2785 - accuracy: 0.8245 - val_loss: 0.4437 - val_accuracy: 0.7660\n",
      "Epoch 16/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.2764 - accuracy: 0.8492 - val_loss: 1.0237 - val_accuracy: 0.4149\n",
      "Epoch 17/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.2725 - accuracy: 0.8422 - val_loss: 1.0132 - val_accuracy: 0.3723\n",
      "Epoch 18/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.2637 - accuracy: 0.8292 - val_loss: 1.5740 - val_accuracy: 0.6596\n",
      "Epoch 19/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.2320 - accuracy: 0.8681 - val_loss: 0.2655 - val_accuracy: 0.7872\n",
      "Epoch 20/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.2208 - accuracy: 0.8634 - val_loss: 0.9267 - val_accuracy: 0.3936\n",
      "Epoch 21/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.2234 - accuracy: 0.8740 - val_loss: 0.5396 - val_accuracy: 0.7128\n",
      "Epoch 22/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.2177 - accuracy: 0.8787 - val_loss: 0.2173 - val_accuracy: 0.8404\n",
      "Epoch 23/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.2270 - accuracy: 0.8645 - val_loss: 0.2547 - val_accuracy: 0.8191\n",
      "Epoch 24/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.1971 - accuracy: 0.8975 - val_loss: 0.3391 - val_accuracy: 0.7660\n",
      "Epoch 25/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.1760 - accuracy: 0.9034 - val_loss: 0.9049 - val_accuracy: 0.4043\n",
      "Epoch 26/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.1986 - accuracy: 0.8963 - val_loss: 0.4283 - val_accuracy: 0.6809\n",
      "Epoch 27/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.1985 - accuracy: 0.8822 - val_loss: 0.6383 - val_accuracy: 0.6383\n",
      "Epoch 28/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.1475 - accuracy: 0.9270 - val_loss: 0.2792 - val_accuracy: 0.8617\n",
      "Epoch 29/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.1739 - accuracy: 0.9058 - val_loss: 0.7594 - val_accuracy: 0.6915\n",
      "Epoch 30/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.1595 - accuracy: 0.9164 - val_loss: 0.2818 - val_accuracy: 0.8723\n",
      "Epoch 31/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.1722 - accuracy: 0.9105 - val_loss: 0.7352 - val_accuracy: 0.6489\n",
      "Epoch 32/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.1660 - accuracy: 0.9293 - val_loss: 0.4875 - val_accuracy: 0.7660\n",
      "Epoch 33/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.1408 - accuracy: 0.9317 - val_loss: 0.4420 - val_accuracy: 0.7234\n",
      "Epoch 34/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.1207 - accuracy: 0.9411 - val_loss: 0.3367 - val_accuracy: 0.7553\n",
      "Epoch 35/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.1341 - accuracy: 0.9399 - val_loss: 6.1183 - val_accuracy: 0.3298\n",
      "Epoch 36/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.1520 - accuracy: 0.9234 - val_loss: 0.5108 - val_accuracy: 0.7660\n",
      "Epoch 37/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.1072 - accuracy: 0.9576 - val_loss: 0.3277 - val_accuracy: 0.8085\n",
      "Epoch 38/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.1109 - accuracy: 0.9494 - val_loss: 0.2880 - val_accuracy: 0.8511\n",
      "Epoch 39/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.1191 - accuracy: 0.9564 - val_loss: 0.2279 - val_accuracy: 0.8723\n",
      "Epoch 40/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.1052 - accuracy: 0.9588 - val_loss: 0.5893 - val_accuracy: 0.7660\n",
      "Epoch 41/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.1116 - accuracy: 0.9458 - val_loss: 0.4019 - val_accuracy: 0.7979\n",
      "Epoch 42/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.1198 - accuracy: 0.9470 - val_loss: 0.4120 - val_accuracy: 0.8511\n",
      "Epoch 43/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.1117 - accuracy: 0.9494 - val_loss: 0.4757 - val_accuracy: 0.7234\n",
      "Epoch 44/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0865 - accuracy: 0.9623 - val_loss: 0.5568 - val_accuracy: 0.7979\n",
      "Epoch 45/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0832 - accuracy: 0.9682 - val_loss: 0.3127 - val_accuracy: 0.8404\n",
      "Epoch 46/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0961 - accuracy: 0.9600 - val_loss: 0.3836 - val_accuracy: 0.7766\n",
      "Epoch 47/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0696 - accuracy: 0.9776 - val_loss: 0.4679 - val_accuracy: 0.7553\n",
      "Epoch 48/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0945 - accuracy: 0.9588 - val_loss: 0.2014 - val_accuracy: 0.8723\n",
      "Epoch 49/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0874 - accuracy: 0.9647 - val_loss: 0.2543 - val_accuracy: 0.8404\n",
      "Epoch 50/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0770 - accuracy: 0.9729 - val_loss: 0.3370 - val_accuracy: 0.8617\n",
      "Epoch 51/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0696 - accuracy: 0.9741 - val_loss: 0.2914 - val_accuracy: 0.8617\n",
      "Epoch 52/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0781 - accuracy: 0.9682 - val_loss: 0.2661 - val_accuracy: 0.8617\n",
      "Epoch 53/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0923 - accuracy: 0.9658 - val_loss: 0.3340 - val_accuracy: 0.8404\n",
      "Epoch 54/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0687 - accuracy: 0.9764 - val_loss: 0.2616 - val_accuracy: 0.8511\n",
      "Epoch 55/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0880 - accuracy: 0.9647 - val_loss: 0.5939 - val_accuracy: 0.7553\n",
      "Epoch 56/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0537 - accuracy: 0.9823 - val_loss: 0.3099 - val_accuracy: 0.7979\n",
      "Epoch 57/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0632 - accuracy: 0.9776 - val_loss: 0.2689 - val_accuracy: 0.7979\n",
      "Epoch 58/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0694 - accuracy: 0.9753 - val_loss: 0.2971 - val_accuracy: 0.8085\n",
      "Epoch 59/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0745 - accuracy: 0.9717 - val_loss: 0.2687 - val_accuracy: 0.8723\n",
      "Epoch 60/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.1015 - accuracy: 0.9541 - val_loss: 0.3962 - val_accuracy: 0.7553\n",
      "Epoch 61/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0619 - accuracy: 0.9788 - val_loss: 0.2387 - val_accuracy: 0.8404\n",
      "Epoch 62/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0759 - accuracy: 0.9658 - val_loss: 0.2767 - val_accuracy: 0.8830\n",
      "Epoch 63/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0659 - accuracy: 0.9717 - val_loss: 0.2464 - val_accuracy: 0.8830\n",
      "Epoch 64/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.1019 - accuracy: 0.9600 - val_loss: 0.2237 - val_accuracy: 0.8830\n",
      "Epoch 65/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0696 - accuracy: 0.9764 - val_loss: 0.3279 - val_accuracy: 0.7979\n",
      "Epoch 66/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0722 - accuracy: 0.9647 - val_loss: 0.2740 - val_accuracy: 0.8723\n",
      "Epoch 67/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0567 - accuracy: 0.9753 - val_loss: 0.2366 - val_accuracy: 0.8936\n",
      "Epoch 68/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0442 - accuracy: 0.9894 - val_loss: 0.2386 - val_accuracy: 0.8830\n",
      "Epoch 69/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0831 - accuracy: 0.9611 - val_loss: 0.2552 - val_accuracy: 0.8723\n",
      "Epoch 70/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0695 - accuracy: 0.9694 - val_loss: 0.2410 - val_accuracy: 0.8830\n",
      "Epoch 71/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0506 - accuracy: 0.9835 - val_loss: 0.2822 - val_accuracy: 0.8723\n",
      "Epoch 72/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0837 - accuracy: 0.9635 - val_loss: 0.2769 - val_accuracy: 0.8617\n",
      "Epoch 73/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0463 - accuracy: 0.9823 - val_loss: 0.2584 - val_accuracy: 0.8404\n",
      "Epoch 74/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0505 - accuracy: 0.9823 - val_loss: 0.2673 - val_accuracy: 0.8936\n",
      "Epoch 75/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0760 - accuracy: 0.9753 - val_loss: 0.2390 - val_accuracy: 0.8617\n",
      "Epoch 76/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0777 - accuracy: 0.9670 - val_loss: 0.2698 - val_accuracy: 0.8723\n",
      "Epoch 77/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0556 - accuracy: 0.9823 - val_loss: 0.4226 - val_accuracy: 0.8511\n",
      "Epoch 78/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0747 - accuracy: 0.9658 - val_loss: 0.2601 - val_accuracy: 0.8617\n",
      "Epoch 79/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0593 - accuracy: 0.9753 - val_loss: 0.2944 - val_accuracy: 0.8298\n",
      "Epoch 80/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0373 - accuracy: 0.9882 - val_loss: 0.3347 - val_accuracy: 0.7872\n",
      "Epoch 81/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0890 - accuracy: 0.9623 - val_loss: 0.2974 - val_accuracy: 0.8404\n",
      "Epoch 82/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0761 - accuracy: 0.9647 - val_loss: 0.2647 - val_accuracy: 0.8404\n",
      "Epoch 83/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0779 - accuracy: 0.9600 - val_loss: 0.3207 - val_accuracy: 0.8298\n",
      "Epoch 84/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0746 - accuracy: 0.9623 - val_loss: 0.3233 - val_accuracy: 0.8723\n",
      "Epoch 85/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0694 - accuracy: 0.9717 - val_loss: 0.2801 - val_accuracy: 0.8830\n",
      "Epoch 86/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0527 - accuracy: 0.9788 - val_loss: 0.2688 - val_accuracy: 0.8723\n",
      "Epoch 87/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0833 - accuracy: 0.9588 - val_loss: 0.2848 - val_accuracy: 0.8830\n",
      "Epoch 88/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0848 - accuracy: 0.9588 - val_loss: 0.2794 - val_accuracy: 0.8404\n",
      "Epoch 89/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0585 - accuracy: 0.9729 - val_loss: 0.3068 - val_accuracy: 0.8191\n",
      "Epoch 90/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0799 - accuracy: 0.9635 - val_loss: 0.2854 - val_accuracy: 0.8723\n",
      "Epoch 91/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0657 - accuracy: 0.9753 - val_loss: 0.2694 - val_accuracy: 0.8511\n",
      "Epoch 92/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0821 - accuracy: 0.9588 - val_loss: 0.2784 - val_accuracy: 0.8617\n",
      "Epoch 93/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0323 - accuracy: 0.9906 - val_loss: 0.2628 - val_accuracy: 0.8617\n",
      "Epoch 94/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0796 - accuracy: 0.9647 - val_loss: 0.2762 - val_accuracy: 0.8404\n",
      "Epoch 95/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0702 - accuracy: 0.9635 - val_loss: 0.2733 - val_accuracy: 0.8511\n",
      "Epoch 96/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0582 - accuracy: 0.9764 - val_loss: 0.2500 - val_accuracy: 0.8511\n",
      "Epoch 97/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0426 - accuracy: 0.9847 - val_loss: 0.2825 - val_accuracy: 0.8298\n",
      "Epoch 98/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0528 - accuracy: 0.9764 - val_loss: 0.2703 - val_accuracy: 0.8511\n",
      "Epoch 99/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0542 - accuracy: 0.9800 - val_loss: 0.2788 - val_accuracy: 0.8617\n",
      "Epoch 100/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0407 - accuracy: 0.9882 - val_loss: 0.2629 - val_accuracy: 0.8617\n",
      "Score for fold 4: loss of 0.2628939747810364; accuracy of 86.17021441459656%\n",
      "(849,) (94,)\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 5 ...\n",
      "Epoch 1/100\n",
      "213/213 [==============================] - 45s 177ms/step - loss: 0.5986 - accuracy: 0.5171 - val_loss: 0.9138 - val_accuracy: 0.3511\n",
      "Epoch 2/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.5159 - accuracy: 0.5830 - val_loss: 1.2470 - val_accuracy: 0.3511\n",
      "Epoch 3/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.4854 - accuracy: 0.6196 - val_loss: 1.3405 - val_accuracy: 0.3511\n",
      "Epoch 4/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.4516 - accuracy: 0.6313 - val_loss: 0.7649 - val_accuracy: 0.5106\n",
      "Epoch 5/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.4202 - accuracy: 0.6843 - val_loss: 4.4926 - val_accuracy: 0.3723\n",
      "Epoch 6/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.4271 - accuracy: 0.6949 - val_loss: 1.6093 - val_accuracy: 0.4894\n",
      "Epoch 7/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.3867 - accuracy: 0.7220 - val_loss: 0.4743 - val_accuracy: 0.7660\n",
      "Epoch 8/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.3726 - accuracy: 0.7420 - val_loss: 0.6401 - val_accuracy: 0.6489\n",
      "Epoch 9/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.3629 - accuracy: 0.7432 - val_loss: 1.5509 - val_accuracy: 0.3298\n",
      "Epoch 10/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.3505 - accuracy: 0.7774 - val_loss: 1.1984 - val_accuracy: 0.6596\n",
      "Epoch 11/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.3089 - accuracy: 0.7974 - val_loss: 0.5810 - val_accuracy: 0.6064\n",
      "Epoch 12/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.3108 - accuracy: 0.8139 - val_loss: 0.3908 - val_accuracy: 0.7979\n",
      "Epoch 13/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.2780 - accuracy: 0.8245 - val_loss: 1.0535 - val_accuracy: 0.5957\n",
      "Epoch 14/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.3046 - accuracy: 0.8092 - val_loss: 0.6964 - val_accuracy: 0.6809\n",
      "Epoch 15/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.2383 - accuracy: 0.8740 - val_loss: 0.2183 - val_accuracy: 0.8617\n",
      "Epoch 16/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.2062 - accuracy: 0.8905 - val_loss: 0.2344 - val_accuracy: 0.8617\n",
      "Epoch 17/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.2483 - accuracy: 0.8634 - val_loss: 0.7802 - val_accuracy: 0.5426\n",
      "Epoch 18/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.2178 - accuracy: 0.8834 - val_loss: 0.9132 - val_accuracy: 0.4574\n",
      "Epoch 19/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.2110 - accuracy: 0.8857 - val_loss: 0.5277 - val_accuracy: 0.6277\n",
      "Epoch 20/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.2156 - accuracy: 0.8834 - val_loss: 0.2480 - val_accuracy: 0.8298\n",
      "Epoch 21/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.2071 - accuracy: 0.8857 - val_loss: 0.4257 - val_accuracy: 0.7979\n",
      "Epoch 22/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.1913 - accuracy: 0.8928 - val_loss: 1.4109 - val_accuracy: 0.3617\n",
      "Epoch 23/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.1855 - accuracy: 0.9117 - val_loss: 0.9716 - val_accuracy: 0.5638\n",
      "Epoch 24/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.1472 - accuracy: 0.9258 - val_loss: 0.7243 - val_accuracy: 0.7021\n",
      "Epoch 25/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.1722 - accuracy: 0.9058 - val_loss: 0.5977 - val_accuracy: 0.6809\n",
      "Epoch 26/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.1722 - accuracy: 0.9176 - val_loss: 2.4389 - val_accuracy: 0.3830\n",
      "Epoch 27/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.1537 - accuracy: 0.9234 - val_loss: 0.3872 - val_accuracy: 0.7660\n",
      "Epoch 28/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.1465 - accuracy: 0.9329 - val_loss: 0.2602 - val_accuracy: 0.8617\n",
      "Epoch 29/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.1167 - accuracy: 0.9517 - val_loss: 0.2544 - val_accuracy: 0.8298\n",
      "Epoch 30/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.1290 - accuracy: 0.9446 - val_loss: 1.0009 - val_accuracy: 0.5319\n",
      "Epoch 31/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.1229 - accuracy: 0.9494 - val_loss: 0.3424 - val_accuracy: 0.7872\n",
      "Epoch 32/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.1212 - accuracy: 0.9529 - val_loss: 0.3932 - val_accuracy: 0.8191\n",
      "Epoch 33/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.1383 - accuracy: 0.9364 - val_loss: 0.3219 - val_accuracy: 0.8511\n",
      "Epoch 34/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.1170 - accuracy: 0.9482 - val_loss: 0.2997 - val_accuracy: 0.8511\n",
      "Epoch 35/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.1402 - accuracy: 0.9364 - val_loss: 0.4405 - val_accuracy: 0.7766\n",
      "Epoch 36/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.1059 - accuracy: 0.9670 - val_loss: 1.7084 - val_accuracy: 0.5213\n",
      "Epoch 37/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.1009 - accuracy: 0.9600 - val_loss: 0.3167 - val_accuracy: 0.8298\n",
      "Epoch 38/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.1392 - accuracy: 0.9376 - val_loss: 0.5153 - val_accuracy: 0.7553\n",
      "Epoch 39/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.1072 - accuracy: 0.9600 - val_loss: 0.2684 - val_accuracy: 0.8298\n",
      "Epoch 40/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0798 - accuracy: 0.9753 - val_loss: 0.7356 - val_accuracy: 0.7128\n",
      "Epoch 41/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0967 - accuracy: 0.9564 - val_loss: 0.5717 - val_accuracy: 0.7021\n",
      "Epoch 42/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0872 - accuracy: 0.9682 - val_loss: 0.3479 - val_accuracy: 0.8191\n",
      "Epoch 43/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0836 - accuracy: 0.9706 - val_loss: 0.2659 - val_accuracy: 0.8511\n",
      "Epoch 44/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.1364 - accuracy: 0.9411 - val_loss: 0.2261 - val_accuracy: 0.8511\n",
      "Epoch 45/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.1200 - accuracy: 0.9470 - val_loss: 0.3476 - val_accuracy: 0.7979\n",
      "Epoch 46/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0559 - accuracy: 0.9847 - val_loss: 0.2403 - val_accuracy: 0.8723\n",
      "Epoch 47/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0739 - accuracy: 0.9729 - val_loss: 0.4434 - val_accuracy: 0.7872\n",
      "Epoch 48/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0963 - accuracy: 0.9588 - val_loss: 0.3420 - val_accuracy: 0.8298\n",
      "Epoch 49/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0997 - accuracy: 0.9588 - val_loss: 0.2958 - val_accuracy: 0.8617\n",
      "Epoch 50/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0719 - accuracy: 0.9753 - val_loss: 0.3714 - val_accuracy: 0.8085\n",
      "Epoch 51/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0913 - accuracy: 0.9670 - val_loss: 0.7994 - val_accuracy: 0.6915\n",
      "Epoch 52/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.1021 - accuracy: 0.9576 - val_loss: 0.2971 - val_accuracy: 0.8298\n",
      "Epoch 53/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0663 - accuracy: 0.9729 - val_loss: 0.2888 - val_accuracy: 0.8404\n",
      "Epoch 54/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0686 - accuracy: 0.9741 - val_loss: 0.2968 - val_accuracy: 0.8511\n",
      "Epoch 55/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0798 - accuracy: 0.9717 - val_loss: 0.3781 - val_accuracy: 0.7979\n",
      "Epoch 56/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0718 - accuracy: 0.9753 - val_loss: 0.3324 - val_accuracy: 0.8511\n",
      "Epoch 57/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0671 - accuracy: 0.9682 - val_loss: 0.2674 - val_accuracy: 0.8404\n",
      "Epoch 58/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0703 - accuracy: 0.9694 - val_loss: 0.3506 - val_accuracy: 0.8085\n",
      "Epoch 59/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0494 - accuracy: 0.9847 - val_loss: 0.3670 - val_accuracy: 0.8191\n",
      "Epoch 60/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0756 - accuracy: 0.9729 - val_loss: 0.4270 - val_accuracy: 0.7979\n",
      "Epoch 61/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0878 - accuracy: 0.9611 - val_loss: 0.3102 - val_accuracy: 0.7979\n",
      "Epoch 62/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0731 - accuracy: 0.9658 - val_loss: 0.2935 - val_accuracy: 0.8617\n",
      "Epoch 63/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0846 - accuracy: 0.9588 - val_loss: 0.3019 - val_accuracy: 0.8511\n",
      "Epoch 64/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0610 - accuracy: 0.9753 - val_loss: 0.3328 - val_accuracy: 0.8298\n",
      "Epoch 65/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0524 - accuracy: 0.9788 - val_loss: 0.3888 - val_accuracy: 0.8085\n",
      "Epoch 66/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0713 - accuracy: 0.9670 - val_loss: 0.3850 - val_accuracy: 0.8191\n",
      "Epoch 67/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0515 - accuracy: 0.9823 - val_loss: 0.3758 - val_accuracy: 0.8298\n",
      "Epoch 68/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0957 - accuracy: 0.9576 - val_loss: 0.3506 - val_accuracy: 0.8191\n",
      "Epoch 69/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0799 - accuracy: 0.9623 - val_loss: 0.3200 - val_accuracy: 0.8191\n",
      "Epoch 70/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0538 - accuracy: 0.9788 - val_loss: 0.3127 - val_accuracy: 0.8404\n",
      "Epoch 71/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0513 - accuracy: 0.9823 - val_loss: 0.5992 - val_accuracy: 0.7660\n",
      "Epoch 72/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0681 - accuracy: 0.9729 - val_loss: 0.3339 - val_accuracy: 0.8404\n",
      "Epoch 73/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0802 - accuracy: 0.9670 - val_loss: 0.3235 - val_accuracy: 0.8191\n",
      "Epoch 74/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0588 - accuracy: 0.9800 - val_loss: 0.2933 - val_accuracy: 0.8617\n",
      "Epoch 75/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0754 - accuracy: 0.9670 - val_loss: 0.3179 - val_accuracy: 0.8404\n",
      "Epoch 76/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0672 - accuracy: 0.9753 - val_loss: 0.3050 - val_accuracy: 0.8404\n",
      "Epoch 77/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0754 - accuracy: 0.9611 - val_loss: 0.2944 - val_accuracy: 0.8404\n",
      "Epoch 78/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0685 - accuracy: 0.9729 - val_loss: 0.3122 - val_accuracy: 0.8191\n",
      "Epoch 79/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0837 - accuracy: 0.9600 - val_loss: 0.3315 - val_accuracy: 0.8191\n",
      "Epoch 80/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0452 - accuracy: 0.9812 - val_loss: 0.3512 - val_accuracy: 0.8298\n",
      "Epoch 81/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0511 - accuracy: 0.9812 - val_loss: 0.3083 - val_accuracy: 0.8511\n",
      "Epoch 82/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0694 - accuracy: 0.9717 - val_loss: 0.5320 - val_accuracy: 0.7979\n",
      "Epoch 83/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0646 - accuracy: 0.9706 - val_loss: 0.3285 - val_accuracy: 0.8191\n",
      "Epoch 84/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0887 - accuracy: 0.9576 - val_loss: 0.3096 - val_accuracy: 0.8191\n",
      "Epoch 85/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0717 - accuracy: 0.9647 - val_loss: 0.3278 - val_accuracy: 0.8723\n",
      "Epoch 86/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0508 - accuracy: 0.9812 - val_loss: 0.3083 - val_accuracy: 0.8298\n",
      "Epoch 87/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0684 - accuracy: 0.9776 - val_loss: 0.7478 - val_accuracy: 0.7021\n",
      "Epoch 88/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0663 - accuracy: 0.9682 - val_loss: 0.3084 - val_accuracy: 0.8511\n",
      "Epoch 89/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0542 - accuracy: 0.9788 - val_loss: 0.3163 - val_accuracy: 0.8298\n",
      "Epoch 90/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0581 - accuracy: 0.9776 - val_loss: 0.3049 - val_accuracy: 0.8298\n",
      "Epoch 91/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0489 - accuracy: 0.9847 - val_loss: 0.2771 - val_accuracy: 0.8617\n",
      "Epoch 92/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0459 - accuracy: 0.9823 - val_loss: 0.2873 - val_accuracy: 0.8617\n",
      "Epoch 93/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0470 - accuracy: 0.9823 - val_loss: 0.2835 - val_accuracy: 0.8617\n",
      "Epoch 94/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0684 - accuracy: 0.9717 - val_loss: 0.2887 - val_accuracy: 0.8404\n",
      "Epoch 95/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0575 - accuracy: 0.9729 - val_loss: 0.4153 - val_accuracy: 0.8191\n",
      "Epoch 96/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0542 - accuracy: 0.9800 - val_loss: 0.3078 - val_accuracy: 0.8298\n",
      "Epoch 97/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0649 - accuracy: 0.9741 - val_loss: 0.3329 - val_accuracy: 0.8085\n",
      "Epoch 98/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0828 - accuracy: 0.9564 - val_loss: 0.2920 - val_accuracy: 0.8298\n",
      "Epoch 99/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0658 - accuracy: 0.9729 - val_loss: 0.2956 - val_accuracy: 0.8298\n",
      "Epoch 100/100\n",
      "213/213 [==============================] - 37s 171ms/step - loss: 0.0524 - accuracy: 0.9823 - val_loss: 0.3296 - val_accuracy: 0.8191\n",
      "Score for fold 5: loss of 0.32961198687553406; accuracy of 81.91489577293396%\n",
      "(849,) (94,)\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 6 ...\n",
      "Epoch 1/100\n",
      "213/213 [==============================] - 45s 178ms/step - loss: 0.6040 - accuracy: 0.4782 - val_loss: 1.0700 - val_accuracy: 0.3511\n",
      "Epoch 2/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.5259 - accuracy: 0.5630 - val_loss: 1.4155 - val_accuracy: 0.3511\n",
      "Epoch 3/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.4691 - accuracy: 0.6349 - val_loss: 0.7691 - val_accuracy: 0.4043\n",
      "Epoch 4/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.4634 - accuracy: 0.6266 - val_loss: 1.5690 - val_accuracy: 0.3404\n",
      "Epoch 5/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.4298 - accuracy: 0.6796 - val_loss: 1.0981 - val_accuracy: 0.3936\n",
      "Epoch 6/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.3869 - accuracy: 0.7173 - val_loss: 0.3727 - val_accuracy: 0.7979\n",
      "Epoch 7/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.3842 - accuracy: 0.7079 - val_loss: 0.9672 - val_accuracy: 0.5000\n",
      "Epoch 8/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.3634 - accuracy: 0.7515 - val_loss: 1.1434 - val_accuracy: 0.4787\n",
      "Epoch 9/100\n",
      "213/213 [==============================] - 37s 174ms/step - loss: 0.3768 - accuracy: 0.7479 - val_loss: 0.4629 - val_accuracy: 0.6489\n",
      "Epoch 10/100\n",
      "213/213 [==============================] - 37s 174ms/step - loss: 0.3439 - accuracy: 0.7845 - val_loss: 1.8491 - val_accuracy: 0.4043\n",
      "Epoch 11/100\n",
      "213/213 [==============================] - 37s 174ms/step - loss: 0.3337 - accuracy: 0.7739 - val_loss: 0.5191 - val_accuracy: 0.7660\n",
      "Epoch 12/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.3114 - accuracy: 0.7927 - val_loss: 0.1999 - val_accuracy: 0.9362\n",
      "Epoch 13/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.3147 - accuracy: 0.7962 - val_loss: 0.3235 - val_accuracy: 0.7872\n",
      "Epoch 14/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.3268 - accuracy: 0.8174 - val_loss: 0.3269 - val_accuracy: 0.7979\n",
      "Epoch 15/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.2787 - accuracy: 0.8351 - val_loss: 0.3335 - val_accuracy: 0.8191\n",
      "Epoch 16/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.2855 - accuracy: 0.8304 - val_loss: 0.2642 - val_accuracy: 0.8511\n",
      "Epoch 17/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.2536 - accuracy: 0.8575 - val_loss: 0.3480 - val_accuracy: 0.7447\n",
      "Epoch 18/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.2197 - accuracy: 0.8716 - val_loss: 0.2097 - val_accuracy: 0.8723\n",
      "Epoch 19/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.1924 - accuracy: 0.8928 - val_loss: 0.1175 - val_accuracy: 0.9255\n",
      "Epoch 20/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.2256 - accuracy: 0.8775 - val_loss: 0.2765 - val_accuracy: 0.8723\n",
      "Epoch 21/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.2029 - accuracy: 0.8905 - val_loss: 0.9044 - val_accuracy: 0.6383\n",
      "Epoch 22/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.1847 - accuracy: 0.9069 - val_loss: 1.8711 - val_accuracy: 0.4574\n",
      "Epoch 23/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.1773 - accuracy: 0.9105 - val_loss: 0.8888 - val_accuracy: 0.6596\n",
      "Epoch 24/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.1475 - accuracy: 0.9399 - val_loss: 0.9905 - val_accuracy: 0.7234\n",
      "Epoch 25/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.1709 - accuracy: 0.9140 - val_loss: 0.1721 - val_accuracy: 0.8936\n",
      "Epoch 26/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.1635 - accuracy: 0.9152 - val_loss: 0.2676 - val_accuracy: 0.8830\n",
      "Epoch 27/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.1410 - accuracy: 0.9388 - val_loss: 0.4264 - val_accuracy: 0.8191\n",
      "Epoch 28/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.1421 - accuracy: 0.9305 - val_loss: 0.2003 - val_accuracy: 0.8830\n",
      "Epoch 29/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.1469 - accuracy: 0.9352 - val_loss: 0.4424 - val_accuracy: 0.8191\n",
      "Epoch 30/100\n",
      "213/213 [==============================] - 37s 174ms/step - loss: 0.1634 - accuracy: 0.9329 - val_loss: 0.2426 - val_accuracy: 0.8936\n",
      "Epoch 31/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.1305 - accuracy: 0.9446 - val_loss: 0.3476 - val_accuracy: 0.8298\n",
      "Epoch 32/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.1295 - accuracy: 0.9435 - val_loss: 0.2200 - val_accuracy: 0.8723\n",
      "Epoch 33/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.1092 - accuracy: 0.9541 - val_loss: 0.1588 - val_accuracy: 0.9255\n",
      "Epoch 34/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.1038 - accuracy: 0.9576 - val_loss: 0.1517 - val_accuracy: 0.8936\n",
      "Epoch 35/100\n",
      "213/213 [==============================] - 37s 174ms/step - loss: 0.1388 - accuracy: 0.9305 - val_loss: 0.7703 - val_accuracy: 0.6915\n",
      "Epoch 36/100\n",
      "213/213 [==============================] - 37s 174ms/step - loss: 0.1260 - accuracy: 0.9494 - val_loss: 0.1650 - val_accuracy: 0.8936\n",
      "Epoch 37/100\n",
      "213/213 [==============================] - 37s 174ms/step - loss: 0.1193 - accuracy: 0.9446 - val_loss: 0.5274 - val_accuracy: 0.6383\n",
      "Epoch 38/100\n",
      "213/213 [==============================] - 37s 174ms/step - loss: 0.1002 - accuracy: 0.9647 - val_loss: 0.1619 - val_accuracy: 0.9043\n",
      "Epoch 39/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.1067 - accuracy: 0.9564 - val_loss: 0.1937 - val_accuracy: 0.9043\n",
      "Epoch 40/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0993 - accuracy: 0.9588 - val_loss: 0.3517 - val_accuracy: 0.7872\n",
      "Epoch 41/100\n",
      "213/213 [==============================] - 37s 174ms/step - loss: 0.0773 - accuracy: 0.9729 - val_loss: 0.1512 - val_accuracy: 0.8936\n",
      "Epoch 42/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0961 - accuracy: 0.9658 - val_loss: 0.3268 - val_accuracy: 0.8617\n",
      "Epoch 43/100\n",
      "213/213 [==============================] - 37s 174ms/step - loss: 0.0915 - accuracy: 0.9647 - val_loss: 0.2965 - val_accuracy: 0.8511\n",
      "Epoch 44/100\n",
      "213/213 [==============================] - 37s 174ms/step - loss: 0.0934 - accuracy: 0.9694 - val_loss: 0.2737 - val_accuracy: 0.8085\n",
      "Epoch 45/100\n",
      "213/213 [==============================] - 37s 174ms/step - loss: 0.0703 - accuracy: 0.9717 - val_loss: 0.1197 - val_accuracy: 0.9149\n",
      "Epoch 46/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.1012 - accuracy: 0.9576 - val_loss: 0.1748 - val_accuracy: 0.9255\n",
      "Epoch 47/100\n",
      "213/213 [==============================] - 37s 174ms/step - loss: 0.0854 - accuracy: 0.9706 - val_loss: 0.2780 - val_accuracy: 0.8191\n",
      "Epoch 48/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.1119 - accuracy: 0.9552 - val_loss: 0.2273 - val_accuracy: 0.8830\n",
      "Epoch 49/100\n",
      "213/213 [==============================] - 37s 174ms/step - loss: 0.0975 - accuracy: 0.9611 - val_loss: 0.2966 - val_accuracy: 0.8404\n",
      "Epoch 50/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.1014 - accuracy: 0.9600 - val_loss: 0.1964 - val_accuracy: 0.8830\n",
      "Epoch 51/100\n",
      "213/213 [==============================] - 37s 174ms/step - loss: 0.0814 - accuracy: 0.9694 - val_loss: 0.1520 - val_accuracy: 0.9043\n",
      "Epoch 52/100\n",
      "213/213 [==============================] - 37s 174ms/step - loss: 0.0818 - accuracy: 0.9706 - val_loss: 0.1974 - val_accuracy: 0.8936\n",
      "Epoch 53/100\n",
      "213/213 [==============================] - 37s 174ms/step - loss: 0.0634 - accuracy: 0.9800 - val_loss: 0.2440 - val_accuracy: 0.8617\n",
      "Epoch 54/100\n",
      "213/213 [==============================] - 37s 174ms/step - loss: 0.0920 - accuracy: 0.9647 - val_loss: 0.2425 - val_accuracy: 0.8511\n",
      "Epoch 55/100\n",
      "213/213 [==============================] - 37s 174ms/step - loss: 0.0989 - accuracy: 0.9576 - val_loss: 0.2419 - val_accuracy: 0.8511\n",
      "Epoch 56/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0888 - accuracy: 0.9658 - val_loss: 0.1644 - val_accuracy: 0.9043\n",
      "Epoch 57/100\n",
      "213/213 [==============================] - 37s 174ms/step - loss: 0.0614 - accuracy: 0.9741 - val_loss: 0.1259 - val_accuracy: 0.9255\n",
      "Epoch 58/100\n",
      "213/213 [==============================] - 37s 174ms/step - loss: 0.0791 - accuracy: 0.9682 - val_loss: 0.1648 - val_accuracy: 0.9043\n",
      "Epoch 59/100\n",
      "213/213 [==============================] - 37s 174ms/step - loss: 0.1125 - accuracy: 0.9458 - val_loss: 0.1301 - val_accuracy: 0.9255\n",
      "Epoch 60/100\n",
      "213/213 [==============================] - 37s 174ms/step - loss: 0.0651 - accuracy: 0.9764 - val_loss: 0.1254 - val_accuracy: 0.9468\n",
      "Epoch 61/100\n",
      "213/213 [==============================] - 37s 174ms/step - loss: 0.0791 - accuracy: 0.9682 - val_loss: 0.1272 - val_accuracy: 0.9149\n",
      "Epoch 62/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0590 - accuracy: 0.9788 - val_loss: 0.1687 - val_accuracy: 0.9255\n",
      "Epoch 63/100\n",
      "213/213 [==============================] - 37s 174ms/step - loss: 0.0649 - accuracy: 0.9800 - val_loss: 0.1173 - val_accuracy: 0.9149\n",
      "Epoch 64/100\n",
      "213/213 [==============================] - 37s 174ms/step - loss: 0.1066 - accuracy: 0.9529 - val_loss: 0.3422 - val_accuracy: 0.8617\n",
      "Epoch 65/100\n",
      "213/213 [==============================] - 37s 174ms/step - loss: 0.1030 - accuracy: 0.9576 - val_loss: 0.1388 - val_accuracy: 0.9362\n",
      "Epoch 66/100\n",
      "213/213 [==============================] - 37s 174ms/step - loss: 0.0774 - accuracy: 0.9658 - val_loss: 0.1878 - val_accuracy: 0.8830\n",
      "Epoch 67/100\n",
      "213/213 [==============================] - 37s 174ms/step - loss: 0.0848 - accuracy: 0.9682 - val_loss: 0.1257 - val_accuracy: 0.9255\n",
      "Epoch 68/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0782 - accuracy: 0.9611 - val_loss: 0.1214 - val_accuracy: 0.9255\n",
      "Epoch 69/100\n",
      "213/213 [==============================] - 37s 174ms/step - loss: 0.0923 - accuracy: 0.9529 - val_loss: 0.1394 - val_accuracy: 0.8830\n",
      "Epoch 70/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0698 - accuracy: 0.9753 - val_loss: 0.1401 - val_accuracy: 0.9149\n",
      "Epoch 71/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.1067 - accuracy: 0.9411 - val_loss: 0.1485 - val_accuracy: 0.9149\n",
      "Epoch 72/100\n",
      "213/213 [==============================] - 37s 174ms/step - loss: 0.1095 - accuracy: 0.9588 - val_loss: 0.1151 - val_accuracy: 0.9255\n",
      "Epoch 73/100\n",
      "213/213 [==============================] - 37s 174ms/step - loss: 0.0671 - accuracy: 0.9729 - val_loss: 0.1791 - val_accuracy: 0.8936\n",
      "Epoch 74/100\n",
      "213/213 [==============================] - 37s 174ms/step - loss: 0.0784 - accuracy: 0.9694 - val_loss: 0.1338 - val_accuracy: 0.9255\n",
      "Epoch 75/100\n",
      "213/213 [==============================] - 37s 174ms/step - loss: 0.0718 - accuracy: 0.9729 - val_loss: 0.1403 - val_accuracy: 0.8936\n",
      "Epoch 76/100\n",
      "213/213 [==============================] - 37s 174ms/step - loss: 0.0710 - accuracy: 0.9682 - val_loss: 0.1458 - val_accuracy: 0.9362\n",
      "Epoch 77/100\n",
      "213/213 [==============================] - 37s 174ms/step - loss: 0.0435 - accuracy: 0.9859 - val_loss: 0.1256 - val_accuracy: 0.9149\n",
      "Epoch 78/100\n",
      "213/213 [==============================] - 37s 174ms/step - loss: 0.0387 - accuracy: 0.9906 - val_loss: 0.1330 - val_accuracy: 0.9043\n",
      "Epoch 79/100\n",
      "213/213 [==============================] - 37s 174ms/step - loss: 0.0584 - accuracy: 0.9859 - val_loss: 0.1320 - val_accuracy: 0.9149\n",
      "Epoch 80/100\n",
      "213/213 [==============================] - 37s 174ms/step - loss: 0.0622 - accuracy: 0.9764 - val_loss: 0.1226 - val_accuracy: 0.9255\n",
      "Epoch 81/100\n",
      "213/213 [==============================] - 37s 174ms/step - loss: 0.0552 - accuracy: 0.9729 - val_loss: 0.1273 - val_accuracy: 0.9043\n",
      "Epoch 82/100\n",
      "213/213 [==============================] - 37s 174ms/step - loss: 0.0459 - accuracy: 0.9835 - val_loss: 0.1199 - val_accuracy: 0.9362\n",
      "Epoch 83/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0928 - accuracy: 0.9552 - val_loss: 0.1405 - val_accuracy: 0.9149\n",
      "Epoch 84/100\n",
      "213/213 [==============================] - 37s 174ms/step - loss: 0.0785 - accuracy: 0.9623 - val_loss: 0.1278 - val_accuracy: 0.9362\n",
      "Epoch 85/100\n",
      "213/213 [==============================] - 37s 174ms/step - loss: 0.0662 - accuracy: 0.9741 - val_loss: 0.1177 - val_accuracy: 0.9255\n",
      "Epoch 86/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0669 - accuracy: 0.9800 - val_loss: 0.1186 - val_accuracy: 0.9255\n",
      "Epoch 87/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0688 - accuracy: 0.9694 - val_loss: 0.1172 - val_accuracy: 0.9149\n",
      "Epoch 88/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0671 - accuracy: 0.9706 - val_loss: 0.1314 - val_accuracy: 0.9149\n",
      "Epoch 89/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0453 - accuracy: 0.9847 - val_loss: 0.1281 - val_accuracy: 0.8936\n",
      "Epoch 90/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0355 - accuracy: 0.9894 - val_loss: 0.1195 - val_accuracy: 0.9149\n",
      "Epoch 91/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0772 - accuracy: 0.9635 - val_loss: 0.1163 - val_accuracy: 0.9043\n",
      "Epoch 92/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0543 - accuracy: 0.9753 - val_loss: 0.1266 - val_accuracy: 0.9255\n",
      "Epoch 93/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0859 - accuracy: 0.9588 - val_loss: 0.1192 - val_accuracy: 0.9255\n",
      "Epoch 94/100\n",
      "213/213 [==============================] - 37s 174ms/step - loss: 0.0707 - accuracy: 0.9658 - val_loss: 0.1208 - val_accuracy: 0.9468\n",
      "Epoch 95/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0548 - accuracy: 0.9776 - val_loss: 0.1125 - val_accuracy: 0.9362\n",
      "Epoch 96/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0598 - accuracy: 0.9788 - val_loss: 0.1251 - val_accuracy: 0.9149\n",
      "Epoch 97/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0539 - accuracy: 0.9776 - val_loss: 0.1255 - val_accuracy: 0.9255\n",
      "Epoch 98/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0498 - accuracy: 0.9812 - val_loss: 0.1289 - val_accuracy: 0.9255\n",
      "Epoch 99/100\n",
      "213/213 [==============================] - 37s 174ms/step - loss: 0.0517 - accuracy: 0.9800 - val_loss: 0.1258 - val_accuracy: 0.8936\n",
      "Epoch 100/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0669 - accuracy: 0.9729 - val_loss: 0.1270 - val_accuracy: 0.9149\n",
      "Score for fold 6: loss of 0.12696503102779388; accuracy of 91.4893627166748%\n",
      "(849,) (94,)\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 7 ...\n",
      "Epoch 1/100\n",
      "213/213 [==============================] - 45s 177ms/step - loss: 0.6085 - accuracy: 0.4923 - val_loss: 1.0149 - val_accuracy: 0.3404\n",
      "Epoch 2/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.5062 - accuracy: 0.5866 - val_loss: 1.7864 - val_accuracy: 0.3404\n",
      "Epoch 3/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.4614 - accuracy: 0.6349 - val_loss: 2.4872 - val_accuracy: 0.3404\n",
      "Epoch 4/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.4067 - accuracy: 0.6996 - val_loss: 2.6918 - val_accuracy: 0.5319\n",
      "Epoch 5/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.4072 - accuracy: 0.7138 - val_loss: 2.1828 - val_accuracy: 0.4468\n",
      "Epoch 6/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.3831 - accuracy: 0.7232 - val_loss: 0.6266 - val_accuracy: 0.6383\n",
      "Epoch 7/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.3704 - accuracy: 0.7397 - val_loss: 12.5988 - val_accuracy: 0.5000\n",
      "Epoch 8/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.3280 - accuracy: 0.7903 - val_loss: 0.5665 - val_accuracy: 0.5745\n",
      "Epoch 9/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.3430 - accuracy: 0.7939 - val_loss: 1.1549 - val_accuracy: 0.3298\n",
      "Epoch 10/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.3088 - accuracy: 0.8269 - val_loss: 1.2598 - val_accuracy: 0.6277\n",
      "Epoch 11/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.3170 - accuracy: 0.8092 - val_loss: 1.4140 - val_accuracy: 0.6170\n",
      "Epoch 12/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.2717 - accuracy: 0.8327 - val_loss: 0.4254 - val_accuracy: 0.6915\n",
      "Epoch 13/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.2448 - accuracy: 0.8587 - val_loss: 28.4182 - val_accuracy: 0.3191\n",
      "Epoch 14/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.2664 - accuracy: 0.8457 - val_loss: 0.4210 - val_accuracy: 0.7128\n",
      "Epoch 15/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.2459 - accuracy: 0.8445 - val_loss: 12.2924 - val_accuracy: 0.4787\n",
      "Epoch 16/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.2518 - accuracy: 0.8481 - val_loss: 0.4967 - val_accuracy: 0.7021\n",
      "Epoch 17/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.2170 - accuracy: 0.8763 - val_loss: 3.5666 - val_accuracy: 0.3617\n",
      "Epoch 18/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.2053 - accuracy: 0.8893 - val_loss: 2.2476 - val_accuracy: 0.3085\n",
      "Epoch 19/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.2133 - accuracy: 0.8799 - val_loss: 0.6220 - val_accuracy: 0.7128\n",
      "Epoch 20/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.1773 - accuracy: 0.9105 - val_loss: 0.4381 - val_accuracy: 0.7553\n",
      "Epoch 21/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.1733 - accuracy: 0.9187 - val_loss: 0.5329 - val_accuracy: 0.7766\n",
      "Epoch 22/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.1593 - accuracy: 0.9058 - val_loss: 1.0752 - val_accuracy: 0.6277\n",
      "Epoch 23/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.1716 - accuracy: 0.9187 - val_loss: 1.3944 - val_accuracy: 0.6596\n",
      "Epoch 24/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.1449 - accuracy: 0.9258 - val_loss: 0.3621 - val_accuracy: 0.8298\n",
      "Epoch 25/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.1479 - accuracy: 0.9270 - val_loss: 0.2794 - val_accuracy: 0.8936\n",
      "Epoch 26/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.1308 - accuracy: 0.9458 - val_loss: 0.3516 - val_accuracy: 0.8298\n",
      "Epoch 27/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.1515 - accuracy: 0.9164 - val_loss: 0.2944 - val_accuracy: 0.8617\n",
      "Epoch 28/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.1268 - accuracy: 0.9340 - val_loss: 0.4882 - val_accuracy: 0.7340\n",
      "Epoch 29/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.1209 - accuracy: 0.9576 - val_loss: 0.4088 - val_accuracy: 0.7872\n",
      "Epoch 30/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.1129 - accuracy: 0.9541 - val_loss: 1.3083 - val_accuracy: 0.6702\n",
      "Epoch 31/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.1151 - accuracy: 0.9541 - val_loss: 1.0786 - val_accuracy: 0.6489\n",
      "Epoch 32/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.1402 - accuracy: 0.9329 - val_loss: 0.9808 - val_accuracy: 0.6915\n",
      "Epoch 33/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.1261 - accuracy: 0.9446 - val_loss: 0.3163 - val_accuracy: 0.8298\n",
      "Epoch 34/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0990 - accuracy: 0.9647 - val_loss: 1.0572 - val_accuracy: 0.6383\n",
      "Epoch 35/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.1063 - accuracy: 0.9529 - val_loss: 0.9349 - val_accuracy: 0.7766\n",
      "Epoch 36/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.1013 - accuracy: 0.9541 - val_loss: 0.3738 - val_accuracy: 0.8298\n",
      "Epoch 37/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.1073 - accuracy: 0.9541 - val_loss: 2.3260 - val_accuracy: 0.5745\n",
      "Epoch 38/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0892 - accuracy: 0.9682 - val_loss: 1.1515 - val_accuracy: 0.7340\n",
      "Epoch 39/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0739 - accuracy: 0.9741 - val_loss: 0.4061 - val_accuracy: 0.8085\n",
      "Epoch 40/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0880 - accuracy: 0.9694 - val_loss: 0.4629 - val_accuracy: 0.8404\n",
      "Epoch 41/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0756 - accuracy: 0.9682 - val_loss: 1.2806 - val_accuracy: 0.6702\n",
      "Epoch 42/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0739 - accuracy: 0.9682 - val_loss: 0.6136 - val_accuracy: 0.7234\n",
      "Epoch 43/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0595 - accuracy: 0.9812 - val_loss: 0.3561 - val_accuracy: 0.8511\n",
      "Epoch 44/100\n",
      "213/213 [==============================] - 37s 171ms/step - loss: 0.1084 - accuracy: 0.9482 - val_loss: 1.8770 - val_accuracy: 0.6596\n",
      "Epoch 45/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0748 - accuracy: 0.9658 - val_loss: 0.6732 - val_accuracy: 0.7447\n",
      "Epoch 46/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0765 - accuracy: 0.9682 - val_loss: 0.4052 - val_accuracy: 0.8404\n",
      "Epoch 47/100\n",
      "213/213 [==============================] - 37s 171ms/step - loss: 0.0730 - accuracy: 0.9717 - val_loss: 0.6199 - val_accuracy: 0.7128\n",
      "Epoch 48/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0457 - accuracy: 0.9859 - val_loss: 0.4029 - val_accuracy: 0.8085\n",
      "Epoch 49/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0876 - accuracy: 0.9600 - val_loss: 0.4016 - val_accuracy: 0.8191\n",
      "Epoch 50/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0947 - accuracy: 0.9552 - val_loss: 0.7310 - val_accuracy: 0.7340\n",
      "Epoch 51/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0909 - accuracy: 0.9588 - val_loss: 0.7363 - val_accuracy: 0.7234\n",
      "Epoch 52/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0803 - accuracy: 0.9670 - val_loss: 0.3957 - val_accuracy: 0.8404\n",
      "Epoch 53/100\n",
      "213/213 [==============================] - 37s 171ms/step - loss: 0.0753 - accuracy: 0.9682 - val_loss: 0.3357 - val_accuracy: 0.8511\n",
      "Epoch 54/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0698 - accuracy: 0.9717 - val_loss: 0.4549 - val_accuracy: 0.8404\n",
      "Epoch 55/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0613 - accuracy: 0.9764 - val_loss: 0.3852 - val_accuracy: 0.8298\n",
      "Epoch 56/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0845 - accuracy: 0.9647 - val_loss: 0.3712 - val_accuracy: 0.8191\n",
      "Epoch 57/100\n",
      "213/213 [==============================] - 37s 171ms/step - loss: 0.0535 - accuracy: 0.9835 - val_loss: 0.4877 - val_accuracy: 0.7979\n",
      "Epoch 58/100\n",
      "213/213 [==============================] - 37s 171ms/step - loss: 0.0798 - accuracy: 0.9706 - val_loss: 0.5650 - val_accuracy: 0.7766\n",
      "Epoch 59/100\n",
      "213/213 [==============================] - 37s 171ms/step - loss: 0.0958 - accuracy: 0.9576 - val_loss: 1.3843 - val_accuracy: 0.6702\n",
      "Epoch 60/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0692 - accuracy: 0.9741 - val_loss: 0.4642 - val_accuracy: 0.8191\n",
      "Epoch 61/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0711 - accuracy: 0.9717 - val_loss: 0.4820 - val_accuracy: 0.8191\n",
      "Epoch 62/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0910 - accuracy: 0.9541 - val_loss: 0.3328 - val_accuracy: 0.8617\n",
      "Epoch 63/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0633 - accuracy: 0.9706 - val_loss: 0.6120 - val_accuracy: 0.7766\n",
      "Epoch 64/100\n",
      "213/213 [==============================] - 37s 171ms/step - loss: 0.1005 - accuracy: 0.9470 - val_loss: 0.4806 - val_accuracy: 0.7766\n",
      "Epoch 65/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0636 - accuracy: 0.9764 - val_loss: 0.4000 - val_accuracy: 0.8617\n",
      "Epoch 66/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0484 - accuracy: 0.9859 - val_loss: 0.3785 - val_accuracy: 0.8511\n",
      "Epoch 67/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0869 - accuracy: 0.9611 - val_loss: 0.3407 - val_accuracy: 0.8617\n",
      "Epoch 68/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0795 - accuracy: 0.9623 - val_loss: 0.3997 - val_accuracy: 0.8723\n",
      "Epoch 69/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0396 - accuracy: 0.9870 - val_loss: 0.7198 - val_accuracy: 0.7553\n",
      "Epoch 70/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0796 - accuracy: 0.9647 - val_loss: 0.9565 - val_accuracy: 0.7128\n",
      "Epoch 71/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0512 - accuracy: 0.9812 - val_loss: 0.4325 - val_accuracy: 0.8298\n",
      "Epoch 72/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0975 - accuracy: 0.9541 - val_loss: 0.5102 - val_accuracy: 0.7766\n",
      "Epoch 73/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0482 - accuracy: 0.9823 - val_loss: 0.4872 - val_accuracy: 0.8298\n",
      "Epoch 74/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0562 - accuracy: 0.9764 - val_loss: 0.3775 - val_accuracy: 0.8617\n",
      "Epoch 75/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0552 - accuracy: 0.9788 - val_loss: 0.4296 - val_accuracy: 0.8511\n",
      "Epoch 76/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0549 - accuracy: 0.9788 - val_loss: 0.3222 - val_accuracy: 0.8617\n",
      "Epoch 77/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0345 - accuracy: 0.9929 - val_loss: 0.4805 - val_accuracy: 0.8298\n",
      "Epoch 78/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0510 - accuracy: 0.9753 - val_loss: 0.7146 - val_accuracy: 0.7553\n",
      "Epoch 79/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0628 - accuracy: 0.9706 - val_loss: 0.4950 - val_accuracy: 0.8298\n",
      "Epoch 80/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0680 - accuracy: 0.9729 - val_loss: 0.6375 - val_accuracy: 0.7660\n",
      "Epoch 81/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0670 - accuracy: 0.9706 - val_loss: 0.3834 - val_accuracy: 0.8404\n",
      "Epoch 82/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0678 - accuracy: 0.9753 - val_loss: 0.3599 - val_accuracy: 0.8404\n",
      "Epoch 83/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0537 - accuracy: 0.9764 - val_loss: 0.4962 - val_accuracy: 0.8191\n",
      "Epoch 84/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0724 - accuracy: 0.9670 - val_loss: 0.3906 - val_accuracy: 0.8404\n",
      "Epoch 85/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0394 - accuracy: 0.9894 - val_loss: 0.4201 - val_accuracy: 0.8298\n",
      "Epoch 86/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0445 - accuracy: 0.9800 - val_loss: 0.4389 - val_accuracy: 0.8298\n",
      "Epoch 87/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0675 - accuracy: 0.9706 - val_loss: 0.3796 - val_accuracy: 0.8511\n",
      "Epoch 88/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0392 - accuracy: 0.9882 - val_loss: 0.3995 - val_accuracy: 0.7660\n",
      "Epoch 89/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0594 - accuracy: 0.9764 - val_loss: 0.4319 - val_accuracy: 0.8511\n",
      "Epoch 90/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0551 - accuracy: 0.9753 - val_loss: 0.4395 - val_accuracy: 0.8511\n",
      "Epoch 91/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0575 - accuracy: 0.9788 - val_loss: 0.5637 - val_accuracy: 0.8085\n",
      "Epoch 92/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0764 - accuracy: 0.9623 - val_loss: 0.4705 - val_accuracy: 0.8298\n",
      "Epoch 93/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0647 - accuracy: 0.9682 - val_loss: 0.3794 - val_accuracy: 0.8617\n",
      "Epoch 94/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0661 - accuracy: 0.9741 - val_loss: 0.5173 - val_accuracy: 0.8298\n",
      "Epoch 95/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0652 - accuracy: 0.9729 - val_loss: 0.4983 - val_accuracy: 0.8404\n",
      "Epoch 96/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0758 - accuracy: 0.9635 - val_loss: 0.4701 - val_accuracy: 0.8617\n",
      "Epoch 97/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0609 - accuracy: 0.9753 - val_loss: 0.5157 - val_accuracy: 0.8298\n",
      "Epoch 98/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0791 - accuracy: 0.9611 - val_loss: 0.5115 - val_accuracy: 0.8298\n",
      "Epoch 99/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0362 - accuracy: 0.9918 - val_loss: 0.4903 - val_accuracy: 0.8298\n",
      "Epoch 100/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0622 - accuracy: 0.9741 - val_loss: 0.5064 - val_accuracy: 0.8511\n",
      "Score for fold 7: loss of 0.506381630897522; accuracy of 85.10638475418091%\n",
      "(849,) (94,)\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 8 ...\n",
      "Epoch 1/100\n",
      "213/213 [==============================] - 45s 179ms/step - loss: 0.6089 - accuracy: 0.4653 - val_loss: 1.1985 - val_accuracy: 0.3404\n",
      "Epoch 2/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.5008 - accuracy: 0.5925 - val_loss: 1.6349 - val_accuracy: 0.3404\n",
      "Epoch 3/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.4808 - accuracy: 0.6184 - val_loss: 0.8206 - val_accuracy: 0.3298\n",
      "Epoch 4/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.4357 - accuracy: 0.6690 - val_loss: 23.9507 - val_accuracy: 0.2872\n",
      "Epoch 5/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.4242 - accuracy: 0.6784 - val_loss: 1.8599 - val_accuracy: 0.3511\n",
      "Epoch 6/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.4167 - accuracy: 0.6796 - val_loss: 2.9197 - val_accuracy: 0.2447\n",
      "Epoch 7/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.4163 - accuracy: 0.6949 - val_loss: 0.9742 - val_accuracy: 0.4468\n",
      "Epoch 8/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.3857 - accuracy: 0.7350 - val_loss: 0.4078 - val_accuracy: 0.7660\n",
      "Epoch 9/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.3755 - accuracy: 0.7373 - val_loss: 0.3470 - val_accuracy: 0.7447\n",
      "Epoch 10/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.3566 - accuracy: 0.7456 - val_loss: 0.8560 - val_accuracy: 0.6596\n",
      "Epoch 11/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.3490 - accuracy: 0.7503 - val_loss: 0.4993 - val_accuracy: 0.7553\n",
      "Epoch 12/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.3152 - accuracy: 0.7962 - val_loss: 0.3383 - val_accuracy: 0.8085\n",
      "Epoch 13/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.3077 - accuracy: 0.8057 - val_loss: 0.7625 - val_accuracy: 0.3617\n",
      "Epoch 14/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.3085 - accuracy: 0.8104 - val_loss: 4.8712 - val_accuracy: 0.3617\n",
      "Epoch 15/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.2927 - accuracy: 0.8292 - val_loss: 0.1874 - val_accuracy: 0.9043\n",
      "Epoch 16/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.2828 - accuracy: 0.8351 - val_loss: 0.5397 - val_accuracy: 0.7021\n",
      "Epoch 17/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.2501 - accuracy: 0.8375 - val_loss: 0.7940 - val_accuracy: 0.5638\n",
      "Epoch 18/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.2458 - accuracy: 0.8575 - val_loss: 0.6012 - val_accuracy: 0.5106\n",
      "Epoch 19/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.2453 - accuracy: 0.8563 - val_loss: 0.3271 - val_accuracy: 0.7979\n",
      "Epoch 20/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.2297 - accuracy: 0.8610 - val_loss: 0.7327 - val_accuracy: 0.7340\n",
      "Epoch 21/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.2138 - accuracy: 0.8716 - val_loss: 0.3102 - val_accuracy: 0.8085\n",
      "Epoch 22/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.2099 - accuracy: 0.8857 - val_loss: 0.6563 - val_accuracy: 0.6596\n",
      "Epoch 23/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.2412 - accuracy: 0.8516 - val_loss: 0.7421 - val_accuracy: 0.6915\n",
      "Epoch 24/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.2097 - accuracy: 0.8881 - val_loss: 0.4062 - val_accuracy: 0.8085\n",
      "Epoch 25/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.1712 - accuracy: 0.9140 - val_loss: 0.2607 - val_accuracy: 0.8511\n",
      "Epoch 26/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.1747 - accuracy: 0.8987 - val_loss: 0.2593 - val_accuracy: 0.8085\n",
      "Epoch 27/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.1661 - accuracy: 0.9105 - val_loss: 0.2552 - val_accuracy: 0.8830\n",
      "Epoch 28/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.1846 - accuracy: 0.9058 - val_loss: 0.2365 - val_accuracy: 0.8511\n",
      "Epoch 29/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.1881 - accuracy: 0.9022 - val_loss: 0.2565 - val_accuracy: 0.8723\n",
      "Epoch 30/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.1497 - accuracy: 0.9293 - val_loss: 0.2389 - val_accuracy: 0.9043\n",
      "Epoch 31/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.1650 - accuracy: 0.9270 - val_loss: 0.4606 - val_accuracy: 0.7660\n",
      "Epoch 32/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.1768 - accuracy: 0.9128 - val_loss: 0.2025 - val_accuracy: 0.8830\n",
      "Epoch 33/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.1098 - accuracy: 0.9494 - val_loss: 0.2112 - val_accuracy: 0.8830\n",
      "Epoch 34/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.1349 - accuracy: 0.9340 - val_loss: 0.4489 - val_accuracy: 0.7553\n",
      "Epoch 35/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.1261 - accuracy: 0.9411 - val_loss: 0.3448 - val_accuracy: 0.7553\n",
      "Epoch 36/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0940 - accuracy: 0.9694 - val_loss: 1.1832 - val_accuracy: 0.6596\n",
      "Epoch 37/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0986 - accuracy: 0.9600 - val_loss: 0.3011 - val_accuracy: 0.8830\n",
      "Epoch 38/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.1099 - accuracy: 0.9505 - val_loss: 0.2648 - val_accuracy: 0.8723\n",
      "Epoch 39/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.1144 - accuracy: 0.9517 - val_loss: 0.2060 - val_accuracy: 0.8723\n",
      "Epoch 40/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.1281 - accuracy: 0.9482 - val_loss: 0.2747 - val_accuracy: 0.8723\n",
      "Epoch 41/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.1198 - accuracy: 0.9529 - val_loss: 0.2231 - val_accuracy: 0.8723\n",
      "Epoch 42/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.1007 - accuracy: 0.9623 - val_loss: 0.2287 - val_accuracy: 0.8936\n",
      "Epoch 43/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.1007 - accuracy: 0.9647 - val_loss: 0.2348 - val_accuracy: 0.8830\n",
      "Epoch 44/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.1068 - accuracy: 0.9529 - val_loss: 0.2612 - val_accuracy: 0.8830\n",
      "Epoch 45/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0840 - accuracy: 0.9588 - val_loss: 0.7361 - val_accuracy: 0.6915\n",
      "Epoch 46/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.1043 - accuracy: 0.9600 - val_loss: 0.2645 - val_accuracy: 0.8830\n",
      "Epoch 47/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0836 - accuracy: 0.9776 - val_loss: 0.2837 - val_accuracy: 0.8723\n",
      "Epoch 48/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0742 - accuracy: 0.9764 - val_loss: 0.2593 - val_accuracy: 0.8723\n",
      "Epoch 49/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0731 - accuracy: 0.9706 - val_loss: 0.2677 - val_accuracy: 0.8617\n",
      "Epoch 50/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.1040 - accuracy: 0.9517 - val_loss: 0.2279 - val_accuracy: 0.8830\n",
      "Epoch 51/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0630 - accuracy: 0.9812 - val_loss: 0.2230 - val_accuracy: 0.8617\n",
      "Epoch 52/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0985 - accuracy: 0.9552 - val_loss: 0.3666 - val_accuracy: 0.8085\n",
      "Epoch 53/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0665 - accuracy: 0.9776 - val_loss: 0.2277 - val_accuracy: 0.8830\n",
      "Epoch 54/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0756 - accuracy: 0.9717 - val_loss: 0.3372 - val_accuracy: 0.8511\n",
      "Epoch 55/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0875 - accuracy: 0.9635 - val_loss: 0.2692 - val_accuracy: 0.8617\n",
      "Epoch 56/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0608 - accuracy: 0.9776 - val_loss: 0.2481 - val_accuracy: 0.8511\n",
      "Epoch 57/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0759 - accuracy: 0.9706 - val_loss: 0.5077 - val_accuracy: 0.8191\n",
      "Epoch 58/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0632 - accuracy: 0.9800 - val_loss: 0.2654 - val_accuracy: 0.8617\n",
      "Epoch 59/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0830 - accuracy: 0.9682 - val_loss: 0.2263 - val_accuracy: 0.8936\n",
      "Epoch 60/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0803 - accuracy: 0.9682 - val_loss: 0.2488 - val_accuracy: 0.8830\n",
      "Epoch 61/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0949 - accuracy: 0.9588 - val_loss: 0.2793 - val_accuracy: 0.8511\n",
      "Epoch 62/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0689 - accuracy: 0.9729 - val_loss: 0.2190 - val_accuracy: 0.9043\n",
      "Epoch 63/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0646 - accuracy: 0.9753 - val_loss: 0.2245 - val_accuracy: 0.8830\n",
      "Epoch 64/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0603 - accuracy: 0.9788 - val_loss: 0.2729 - val_accuracy: 0.8830\n",
      "Epoch 65/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0841 - accuracy: 0.9647 - val_loss: 0.2651 - val_accuracy: 0.8936\n",
      "Epoch 66/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0866 - accuracy: 0.9623 - val_loss: 0.2681 - val_accuracy: 0.8617\n",
      "Epoch 67/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0742 - accuracy: 0.9682 - val_loss: 0.2377 - val_accuracy: 0.8830\n",
      "Epoch 68/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0626 - accuracy: 0.9741 - val_loss: 0.2833 - val_accuracy: 0.8511\n",
      "Epoch 69/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0793 - accuracy: 0.9694 - val_loss: 0.2107 - val_accuracy: 0.8830\n",
      "Epoch 70/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0778 - accuracy: 0.9741 - val_loss: 0.2252 - val_accuracy: 0.9043\n",
      "Epoch 71/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0737 - accuracy: 0.9706 - val_loss: 0.2335 - val_accuracy: 0.9043\n",
      "Epoch 72/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0704 - accuracy: 0.9682 - val_loss: 0.2698 - val_accuracy: 0.8511\n",
      "Epoch 73/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0722 - accuracy: 0.9694 - val_loss: 0.3196 - val_accuracy: 0.8511\n",
      "Epoch 74/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0486 - accuracy: 0.9847 - val_loss: 0.2512 - val_accuracy: 0.8511\n",
      "Epoch 75/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0443 - accuracy: 0.9823 - val_loss: 0.2314 - val_accuracy: 0.8830\n",
      "Epoch 76/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0622 - accuracy: 0.9753 - val_loss: 0.2454 - val_accuracy: 0.8617\n",
      "Epoch 77/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0675 - accuracy: 0.9753 - val_loss: 0.2129 - val_accuracy: 0.8936\n",
      "Epoch 78/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0882 - accuracy: 0.9658 - val_loss: 0.2296 - val_accuracy: 0.8936\n",
      "Epoch 79/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0559 - accuracy: 0.9882 - val_loss: 0.2251 - val_accuracy: 0.8830\n",
      "Epoch 80/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.1133 - accuracy: 0.9411 - val_loss: 0.2015 - val_accuracy: 0.9043\n",
      "Epoch 81/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0694 - accuracy: 0.9729 - val_loss: 0.2276 - val_accuracy: 0.9043\n",
      "Epoch 82/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0804 - accuracy: 0.9670 - val_loss: 0.2172 - val_accuracy: 0.8936\n",
      "Epoch 83/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0816 - accuracy: 0.9670 - val_loss: 0.2384 - val_accuracy: 0.8830\n",
      "Epoch 84/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0408 - accuracy: 0.9882 - val_loss: 0.2177 - val_accuracy: 0.9149\n",
      "Epoch 85/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.1056 - accuracy: 0.9494 - val_loss: 0.2673 - val_accuracy: 0.8404\n",
      "Epoch 86/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0475 - accuracy: 0.9835 - val_loss: 0.2155 - val_accuracy: 0.9255\n",
      "Epoch 87/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0628 - accuracy: 0.9741 - val_loss: 0.2197 - val_accuracy: 0.9149\n",
      "Epoch 88/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0593 - accuracy: 0.9706 - val_loss: 0.2178 - val_accuracy: 0.8936\n",
      "Epoch 89/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0523 - accuracy: 0.9812 - val_loss: 0.2301 - val_accuracy: 0.8936\n",
      "Epoch 90/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0543 - accuracy: 0.9812 - val_loss: 0.2427 - val_accuracy: 0.8404\n",
      "Epoch 91/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0754 - accuracy: 0.9670 - val_loss: 0.2245 - val_accuracy: 0.9043\n",
      "Epoch 92/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0885 - accuracy: 0.9623 - val_loss: 0.2378 - val_accuracy: 0.8830\n",
      "Epoch 93/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0681 - accuracy: 0.9682 - val_loss: 0.2159 - val_accuracy: 0.9043\n",
      "Epoch 94/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0634 - accuracy: 0.9682 - val_loss: 0.2205 - val_accuracy: 0.8936\n",
      "Epoch 95/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0603 - accuracy: 0.9694 - val_loss: 0.2279 - val_accuracy: 0.8830\n",
      "Epoch 96/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0639 - accuracy: 0.9706 - val_loss: 0.2504 - val_accuracy: 0.8404\n",
      "Epoch 97/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0640 - accuracy: 0.9682 - val_loss: 0.2212 - val_accuracy: 0.9043\n",
      "Epoch 98/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0684 - accuracy: 0.9670 - val_loss: 0.2148 - val_accuracy: 0.9043\n",
      "Epoch 99/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0569 - accuracy: 0.9753 - val_loss: 0.2355 - val_accuracy: 0.8936\n",
      "Epoch 100/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0710 - accuracy: 0.9729 - val_loss: 0.2254 - val_accuracy: 0.9043\n",
      "Score for fold 8: loss of 0.22537493705749512; accuracy of 90.42553305625916%\n",
      "(849,) (94,)\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 9 ...\n",
      "Epoch 1/100\n",
      "213/213 [==============================] - 45s 181ms/step - loss: 0.6076 - accuracy: 0.4994 - val_loss: 0.9138 - val_accuracy: 0.3404\n",
      "Epoch 2/100\n",
      "213/213 [==============================] - 37s 171ms/step - loss: 0.4987 - accuracy: 0.6184 - val_loss: 2.7960 - val_accuracy: 0.3404\n",
      "Epoch 3/100\n",
      "213/213 [==============================] - 37s 171ms/step - loss: 0.4437 - accuracy: 0.6714 - val_loss: 2.5474 - val_accuracy: 0.3404\n",
      "Epoch 4/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.4331 - accuracy: 0.6902 - val_loss: 0.5279 - val_accuracy: 0.7234\n",
      "Epoch 5/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.4090 - accuracy: 0.7197 - val_loss: 73.0865 - val_accuracy: 0.3511\n",
      "Epoch 6/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.4273 - accuracy: 0.6938 - val_loss: 0.6112 - val_accuracy: 0.5106\n",
      "Epoch 7/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.3931 - accuracy: 0.7067 - val_loss: 0.3088 - val_accuracy: 0.7766\n",
      "Epoch 8/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.3437 - accuracy: 0.7621 - val_loss: 1.1631 - val_accuracy: 0.4574\n",
      "Epoch 9/100\n",
      "213/213 [==============================] - 37s 171ms/step - loss: 0.3320 - accuracy: 0.8045 - val_loss: 0.3249 - val_accuracy: 0.7979\n",
      "Epoch 10/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.3221 - accuracy: 0.7903 - val_loss: 0.6968 - val_accuracy: 0.5851\n",
      "Epoch 11/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.3345 - accuracy: 0.7927 - val_loss: 0.4352 - val_accuracy: 0.6915\n",
      "Epoch 12/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.3191 - accuracy: 0.8104 - val_loss: 0.3529 - val_accuracy: 0.7660\n",
      "Epoch 13/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.2986 - accuracy: 0.8104 - val_loss: 0.3861 - val_accuracy: 0.7021\n",
      "Epoch 14/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.2523 - accuracy: 0.8469 - val_loss: 0.5536 - val_accuracy: 0.6809\n",
      "Epoch 15/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.2696 - accuracy: 0.8469 - val_loss: 0.7478 - val_accuracy: 0.6383\n",
      "Epoch 16/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.2361 - accuracy: 0.8622 - val_loss: 0.9000 - val_accuracy: 0.6489\n",
      "Epoch 17/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.2422 - accuracy: 0.8528 - val_loss: 0.3589 - val_accuracy: 0.7872\n",
      "Epoch 18/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.2386 - accuracy: 0.8669 - val_loss: 0.7141 - val_accuracy: 0.5851\n",
      "Epoch 19/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.2172 - accuracy: 0.8822 - val_loss: 0.6017 - val_accuracy: 0.7234\n",
      "Epoch 20/100\n",
      "213/213 [==============================] - 37s 171ms/step - loss: 0.2287 - accuracy: 0.8634 - val_loss: 0.3176 - val_accuracy: 0.8085\n",
      "Epoch 21/100\n",
      "213/213 [==============================] - 37s 171ms/step - loss: 0.1920 - accuracy: 0.8905 - val_loss: 0.5059 - val_accuracy: 0.6489\n",
      "Epoch 22/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.1989 - accuracy: 0.8905 - val_loss: 0.3476 - val_accuracy: 0.7660\n",
      "Epoch 23/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.1768 - accuracy: 0.9117 - val_loss: 0.6645 - val_accuracy: 0.5319\n",
      "Epoch 24/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.1997 - accuracy: 0.8952 - val_loss: 0.4322 - val_accuracy: 0.7660\n",
      "Epoch 25/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.1829 - accuracy: 0.9034 - val_loss: 0.2918 - val_accuracy: 0.8191\n",
      "Epoch 26/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.1649 - accuracy: 0.9223 - val_loss: 0.5135 - val_accuracy: 0.7340\n",
      "Epoch 27/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.1689 - accuracy: 0.9199 - val_loss: 0.2856 - val_accuracy: 0.8511\n",
      "Epoch 28/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.1535 - accuracy: 0.9282 - val_loss: 0.2596 - val_accuracy: 0.8511\n",
      "Epoch 29/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.1576 - accuracy: 0.9199 - val_loss: 0.7335 - val_accuracy: 0.6170\n",
      "Epoch 30/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.1348 - accuracy: 0.9364 - val_loss: 0.5272 - val_accuracy: 0.7447\n",
      "Epoch 31/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.1456 - accuracy: 0.9352 - val_loss: 0.5382 - val_accuracy: 0.6915\n",
      "Epoch 32/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.1146 - accuracy: 0.9482 - val_loss: 0.5661 - val_accuracy: 0.7021\n",
      "Epoch 33/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.1415 - accuracy: 0.9423 - val_loss: 0.3785 - val_accuracy: 0.7553\n",
      "Epoch 34/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.1142 - accuracy: 0.9505 - val_loss: 0.2718 - val_accuracy: 0.8617\n",
      "Epoch 35/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.1497 - accuracy: 0.9258 - val_loss: 0.3711 - val_accuracy: 0.7766\n",
      "Epoch 36/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.1248 - accuracy: 0.9576 - val_loss: 0.3672 - val_accuracy: 0.7766\n",
      "Epoch 37/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.1063 - accuracy: 0.9576 - val_loss: 0.4377 - val_accuracy: 0.8085\n",
      "Epoch 38/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.1258 - accuracy: 0.9423 - val_loss: 0.7213 - val_accuracy: 0.7660\n",
      "Epoch 39/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0886 - accuracy: 0.9682 - val_loss: 0.3033 - val_accuracy: 0.8404\n",
      "Epoch 40/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0959 - accuracy: 0.9635 - val_loss: 0.4767 - val_accuracy: 0.7660\n",
      "Epoch 41/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0997 - accuracy: 0.9623 - val_loss: 0.3858 - val_accuracy: 0.7872\n",
      "Epoch 42/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0910 - accuracy: 0.9694 - val_loss: 0.3986 - val_accuracy: 0.8404\n",
      "Epoch 43/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0805 - accuracy: 0.9706 - val_loss: 0.4037 - val_accuracy: 0.8191\n",
      "Epoch 44/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0976 - accuracy: 0.9600 - val_loss: 0.2909 - val_accuracy: 0.8298\n",
      "Epoch 45/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0734 - accuracy: 0.9764 - val_loss: 0.3650 - val_accuracy: 0.7979\n",
      "Epoch 46/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0811 - accuracy: 0.9706 - val_loss: 0.2871 - val_accuracy: 0.8511\n",
      "Epoch 47/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0797 - accuracy: 0.9670 - val_loss: 0.3498 - val_accuracy: 0.8298\n",
      "Epoch 48/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0698 - accuracy: 0.9764 - val_loss: 0.3995 - val_accuracy: 0.8191\n",
      "Epoch 49/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0810 - accuracy: 0.9682 - val_loss: 0.3958 - val_accuracy: 0.8191\n",
      "Epoch 50/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0610 - accuracy: 0.9800 - val_loss: 0.4625 - val_accuracy: 0.8085\n",
      "Epoch 51/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0862 - accuracy: 0.9670 - val_loss: 0.2608 - val_accuracy: 0.8085\n",
      "Epoch 52/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0881 - accuracy: 0.9647 - val_loss: 0.3449 - val_accuracy: 0.8298\n",
      "Epoch 53/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0730 - accuracy: 0.9717 - val_loss: 0.3269 - val_accuracy: 0.8191\n",
      "Epoch 54/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0595 - accuracy: 0.9764 - val_loss: 0.3764 - val_accuracy: 0.8511\n",
      "Epoch 55/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0636 - accuracy: 0.9753 - val_loss: 0.2977 - val_accuracy: 0.8511\n",
      "Epoch 56/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0600 - accuracy: 0.9717 - val_loss: 0.3106 - val_accuracy: 0.8298\n",
      "Epoch 57/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0772 - accuracy: 0.9753 - val_loss: 0.3616 - val_accuracy: 0.8404\n",
      "Epoch 58/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0713 - accuracy: 0.9764 - val_loss: 0.3163 - val_accuracy: 0.8404\n",
      "Epoch 59/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0683 - accuracy: 0.9741 - val_loss: 0.3218 - val_accuracy: 0.8191\n",
      "Epoch 60/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0648 - accuracy: 0.9729 - val_loss: 0.2784 - val_accuracy: 0.8617\n",
      "Epoch 61/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0748 - accuracy: 0.9658 - val_loss: 0.3236 - val_accuracy: 0.8404\n",
      "Epoch 62/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0722 - accuracy: 0.9800 - val_loss: 0.3470 - val_accuracy: 0.8298\n",
      "Epoch 63/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0593 - accuracy: 0.9764 - val_loss: 0.3076 - val_accuracy: 0.8511\n",
      "Epoch 64/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0613 - accuracy: 0.9812 - val_loss: 0.3456 - val_accuracy: 0.8298\n",
      "Epoch 65/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0654 - accuracy: 0.9753 - val_loss: 0.2987 - val_accuracy: 0.8191\n",
      "Epoch 66/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0788 - accuracy: 0.9670 - val_loss: 0.4029 - val_accuracy: 0.8298\n",
      "Epoch 67/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0831 - accuracy: 0.9588 - val_loss: 0.3312 - val_accuracy: 0.8191\n",
      "Epoch 68/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0792 - accuracy: 0.9647 - val_loss: 0.3103 - val_accuracy: 0.8511\n",
      "Epoch 69/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0531 - accuracy: 0.9812 - val_loss: 0.3153 - val_accuracy: 0.8511\n",
      "Epoch 70/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0758 - accuracy: 0.9658 - val_loss: 0.2805 - val_accuracy: 0.8511\n",
      "Epoch 71/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0598 - accuracy: 0.9788 - val_loss: 0.3400 - val_accuracy: 0.7979\n",
      "Epoch 72/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0849 - accuracy: 0.9611 - val_loss: 0.3525 - val_accuracy: 0.8617\n",
      "Epoch 73/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0505 - accuracy: 0.9812 - val_loss: 0.3439 - val_accuracy: 0.8617\n",
      "Epoch 74/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0510 - accuracy: 0.9835 - val_loss: 0.2692 - val_accuracy: 0.8298\n",
      "Epoch 75/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0844 - accuracy: 0.9564 - val_loss: 0.5732 - val_accuracy: 0.7979\n",
      "Epoch 76/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0716 - accuracy: 0.9658 - val_loss: 0.3117 - val_accuracy: 0.8404\n",
      "Epoch 77/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0726 - accuracy: 0.9670 - val_loss: 0.3331 - val_accuracy: 0.8723\n",
      "Epoch 78/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0563 - accuracy: 0.9776 - val_loss: 0.3210 - val_accuracy: 0.8191\n",
      "Epoch 79/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0506 - accuracy: 0.9812 - val_loss: 0.3371 - val_accuracy: 0.8511\n",
      "Epoch 80/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0819 - accuracy: 0.9623 - val_loss: 0.3109 - val_accuracy: 0.8404\n",
      "Epoch 81/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0643 - accuracy: 0.9717 - val_loss: 0.3497 - val_accuracy: 0.8298\n",
      "Epoch 82/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0718 - accuracy: 0.9706 - val_loss: 0.3578 - val_accuracy: 0.8511\n",
      "Epoch 83/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0519 - accuracy: 0.9776 - val_loss: 0.3804 - val_accuracy: 0.8511\n",
      "Epoch 84/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0518 - accuracy: 0.9788 - val_loss: 0.3659 - val_accuracy: 0.8617\n",
      "Epoch 85/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0748 - accuracy: 0.9611 - val_loss: 0.3320 - val_accuracy: 0.8191\n",
      "Epoch 86/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0789 - accuracy: 0.9635 - val_loss: 0.3142 - val_accuracy: 0.8298\n",
      "Epoch 87/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0519 - accuracy: 0.9788 - val_loss: 0.3289 - val_accuracy: 0.8085\n",
      "Epoch 88/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0638 - accuracy: 0.9706 - val_loss: 0.3557 - val_accuracy: 0.8617\n",
      "Epoch 89/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0467 - accuracy: 0.9812 - val_loss: 0.3412 - val_accuracy: 0.8511\n",
      "Epoch 90/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0599 - accuracy: 0.9729 - val_loss: 0.3231 - val_accuracy: 0.8617\n",
      "Epoch 91/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0810 - accuracy: 0.9623 - val_loss: 0.3061 - val_accuracy: 0.8617\n",
      "Epoch 92/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0659 - accuracy: 0.9706 - val_loss: 0.3131 - val_accuracy: 0.8617\n",
      "Epoch 93/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0621 - accuracy: 0.9706 - val_loss: 0.3270 - val_accuracy: 0.8617\n",
      "Epoch 94/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0530 - accuracy: 0.9788 - val_loss: 0.3300 - val_accuracy: 0.8617\n",
      "Epoch 95/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0800 - accuracy: 0.9682 - val_loss: 0.2665 - val_accuracy: 0.8404\n",
      "Epoch 96/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0476 - accuracy: 0.9823 - val_loss: 0.2941 - val_accuracy: 0.8511\n",
      "Epoch 97/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0705 - accuracy: 0.9647 - val_loss: 0.3314 - val_accuracy: 0.8617\n",
      "Epoch 98/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0673 - accuracy: 0.9706 - val_loss: 0.3393 - val_accuracy: 0.8617\n",
      "Epoch 99/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0378 - accuracy: 0.9870 - val_loss: 0.3449 - val_accuracy: 0.8723\n",
      "Epoch 100/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0933 - accuracy: 0.9588 - val_loss: 0.3253 - val_accuracy: 0.8404\n",
      "Score for fold 9: loss of 0.3253408372402191; accuracy of 84.04255509376526%\n",
      "(849,) (94,)\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 10 ...\n",
      "Epoch 1/100\n",
      "213/213 [==============================] - 45s 177ms/step - loss: 0.6060 - accuracy: 0.4853 - val_loss: 0.8923 - val_accuracy: 0.3404\n",
      "Epoch 2/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.5243 - accuracy: 0.5748 - val_loss: 0.7207 - val_accuracy: 0.3298\n",
      "Epoch 3/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.4521 - accuracy: 0.6690 - val_loss: 4.5992 - val_accuracy: 0.3298\n",
      "Epoch 4/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.4387 - accuracy: 0.6561 - val_loss: 0.4862 - val_accuracy: 0.6170\n",
      "Epoch 5/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.4451 - accuracy: 0.6620 - val_loss: 0.6485 - val_accuracy: 0.5319\n",
      "Epoch 6/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.4039 - accuracy: 0.6832 - val_loss: 0.8355 - val_accuracy: 0.5000\n",
      "Epoch 7/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.3983 - accuracy: 0.7161 - val_loss: 5.1998 - val_accuracy: 0.4468\n",
      "Epoch 8/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.3604 - accuracy: 0.7538 - val_loss: 1.0258 - val_accuracy: 0.7872\n",
      "Epoch 9/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.3285 - accuracy: 0.8068 - val_loss: 48.3841 - val_accuracy: 0.0638\n",
      "Epoch 10/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.3398 - accuracy: 0.7892 - val_loss: 1.2490 - val_accuracy: 0.6383\n",
      "Epoch 11/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.3027 - accuracy: 0.8104 - val_loss: 2.3829 - val_accuracy: 0.6383\n",
      "Epoch 12/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.2725 - accuracy: 0.8386 - val_loss: 0.5376 - val_accuracy: 0.7340\n",
      "Epoch 13/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.2729 - accuracy: 0.8375 - val_loss: 0.4029 - val_accuracy: 0.7766\n",
      "Epoch 14/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.2345 - accuracy: 0.8787 - val_loss: 0.3212 - val_accuracy: 0.7872\n",
      "Epoch 15/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.2390 - accuracy: 0.8598 - val_loss: 0.7568 - val_accuracy: 0.6489\n",
      "Epoch 16/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.2433 - accuracy: 0.8634 - val_loss: 1.3027 - val_accuracy: 0.6064\n",
      "Epoch 17/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.2044 - accuracy: 0.8905 - val_loss: 0.7620 - val_accuracy: 0.6702\n",
      "Epoch 18/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.2323 - accuracy: 0.8645 - val_loss: 0.5637 - val_accuracy: 0.6915\n",
      "Epoch 19/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.2012 - accuracy: 0.8787 - val_loss: 0.2503 - val_accuracy: 0.8404\n",
      "Epoch 20/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.1816 - accuracy: 0.9058 - val_loss: 0.3560 - val_accuracy: 0.7979\n",
      "Epoch 21/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.1894 - accuracy: 0.8905 - val_loss: 0.7047 - val_accuracy: 0.6702\n",
      "Epoch 22/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.1895 - accuracy: 0.8928 - val_loss: 1.8599 - val_accuracy: 0.5957\n",
      "Epoch 23/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.1715 - accuracy: 0.9093 - val_loss: 0.9826 - val_accuracy: 0.6915\n",
      "Epoch 24/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.1617 - accuracy: 0.9258 - val_loss: 0.4007 - val_accuracy: 0.8191\n",
      "Epoch 25/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.1378 - accuracy: 0.9329 - val_loss: 0.5325 - val_accuracy: 0.7553\n",
      "Epoch 26/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.1435 - accuracy: 0.9399 - val_loss: 0.5026 - val_accuracy: 0.7553\n",
      "Epoch 27/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.1395 - accuracy: 0.9258 - val_loss: 1.7523 - val_accuracy: 0.6489\n",
      "Epoch 28/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.1258 - accuracy: 0.9446 - val_loss: 0.6941 - val_accuracy: 0.7340\n",
      "Epoch 29/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.1540 - accuracy: 0.9293 - val_loss: 0.5528 - val_accuracy: 0.7234\n",
      "Epoch 30/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.1118 - accuracy: 0.9552 - val_loss: 0.4717 - val_accuracy: 0.7872\n",
      "Epoch 31/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.1253 - accuracy: 0.9482 - val_loss: 0.7506 - val_accuracy: 0.6702\n",
      "Epoch 32/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.1244 - accuracy: 0.9529 - val_loss: 0.5765 - val_accuracy: 0.6596\n",
      "Epoch 33/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.1140 - accuracy: 0.9623 - val_loss: 0.2253 - val_accuracy: 0.8511\n",
      "Epoch 34/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.1157 - accuracy: 0.9446 - val_loss: 0.3814 - val_accuracy: 0.8085\n",
      "Epoch 35/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.1087 - accuracy: 0.9517 - val_loss: 0.2859 - val_accuracy: 0.8511\n",
      "Epoch 36/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0750 - accuracy: 0.9776 - val_loss: 0.7953 - val_accuracy: 0.7553\n",
      "Epoch 37/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0843 - accuracy: 0.9729 - val_loss: 0.3272 - val_accuracy: 0.7979\n",
      "Epoch 38/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.1126 - accuracy: 0.9541 - val_loss: 0.3989 - val_accuracy: 0.7766\n",
      "Epoch 39/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.1030 - accuracy: 0.9517 - val_loss: 0.2106 - val_accuracy: 0.8936\n",
      "Epoch 40/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.1077 - accuracy: 0.9564 - val_loss: 0.2647 - val_accuracy: 0.8191\n",
      "Epoch 41/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0891 - accuracy: 0.9647 - val_loss: 0.2004 - val_accuracy: 0.8830\n",
      "Epoch 42/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0726 - accuracy: 0.9741 - val_loss: 0.4174 - val_accuracy: 0.7766\n",
      "Epoch 43/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0826 - accuracy: 0.9729 - val_loss: 0.2466 - val_accuracy: 0.8191\n",
      "Epoch 44/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0938 - accuracy: 0.9623 - val_loss: 0.2067 - val_accuracy: 0.8723\n",
      "Epoch 45/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.1025 - accuracy: 0.9552 - val_loss: 0.3896 - val_accuracy: 0.7872\n",
      "Epoch 46/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0761 - accuracy: 0.9717 - val_loss: 0.2654 - val_accuracy: 0.8298\n",
      "Epoch 47/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0643 - accuracy: 0.9741 - val_loss: 0.2874 - val_accuracy: 0.8298\n",
      "Epoch 48/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0575 - accuracy: 0.9788 - val_loss: 0.1943 - val_accuracy: 0.8936\n",
      "Epoch 49/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.1111 - accuracy: 0.9446 - val_loss: 0.8295 - val_accuracy: 0.6809\n",
      "Epoch 50/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0626 - accuracy: 0.9800 - val_loss: 0.3157 - val_accuracy: 0.8085\n",
      "Epoch 51/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0717 - accuracy: 0.9741 - val_loss: 0.1825 - val_accuracy: 0.8936\n",
      "Epoch 52/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0908 - accuracy: 0.9611 - val_loss: 0.2875 - val_accuracy: 0.8191\n",
      "Epoch 53/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0883 - accuracy: 0.9576 - val_loss: 0.1975 - val_accuracy: 0.8936\n",
      "Epoch 54/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0568 - accuracy: 0.9741 - val_loss: 0.2664 - val_accuracy: 0.8404\n",
      "Epoch 55/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0636 - accuracy: 0.9800 - val_loss: 0.2278 - val_accuracy: 0.8617\n",
      "Epoch 56/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0751 - accuracy: 0.9717 - val_loss: 0.2274 - val_accuracy: 0.8511\n",
      "Epoch 57/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0700 - accuracy: 0.9741 - val_loss: 0.2068 - val_accuracy: 0.8617\n",
      "Epoch 58/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0781 - accuracy: 0.9658 - val_loss: 0.2046 - val_accuracy: 0.9043\n",
      "Epoch 59/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0579 - accuracy: 0.9753 - val_loss: 0.2034 - val_accuracy: 0.8830\n",
      "Epoch 60/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0478 - accuracy: 0.9835 - val_loss: 0.2284 - val_accuracy: 0.8617\n",
      "Epoch 61/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0773 - accuracy: 0.9706 - val_loss: 0.2015 - val_accuracy: 0.8830\n",
      "Epoch 62/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0803 - accuracy: 0.9564 - val_loss: 0.2222 - val_accuracy: 0.8404\n",
      "Epoch 63/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0717 - accuracy: 0.9682 - val_loss: 0.2164 - val_accuracy: 0.8723\n",
      "Epoch 64/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0704 - accuracy: 0.9670 - val_loss: 0.3097 - val_accuracy: 0.8298\n",
      "Epoch 65/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0635 - accuracy: 0.9741 - val_loss: 0.3277 - val_accuracy: 0.8085\n",
      "Epoch 66/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0818 - accuracy: 0.9670 - val_loss: 0.1985 - val_accuracy: 0.8830\n",
      "Epoch 67/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0516 - accuracy: 0.9812 - val_loss: 0.1899 - val_accuracy: 0.8830\n",
      "Epoch 68/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0414 - accuracy: 0.9823 - val_loss: 0.2109 - val_accuracy: 0.8830\n",
      "Epoch 69/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0696 - accuracy: 0.9706 - val_loss: 0.2373 - val_accuracy: 0.8723\n",
      "Epoch 70/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0821 - accuracy: 0.9623 - val_loss: 0.2638 - val_accuracy: 0.8298\n",
      "Epoch 71/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0664 - accuracy: 0.9706 - val_loss: 0.2446 - val_accuracy: 0.8511\n",
      "Epoch 72/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0754 - accuracy: 0.9647 - val_loss: 0.1860 - val_accuracy: 0.9043\n",
      "Epoch 73/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0825 - accuracy: 0.9552 - val_loss: 0.1837 - val_accuracy: 0.8830\n",
      "Epoch 74/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0689 - accuracy: 0.9706 - val_loss: 0.2092 - val_accuracy: 0.8404\n",
      "Epoch 75/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0636 - accuracy: 0.9717 - val_loss: 0.1899 - val_accuracy: 0.8511\n",
      "Epoch 76/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0705 - accuracy: 0.9706 - val_loss: 0.1907 - val_accuracy: 0.8723\n",
      "Epoch 77/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.1057 - accuracy: 0.9517 - val_loss: 0.3469 - val_accuracy: 0.8085\n",
      "Epoch 78/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0734 - accuracy: 0.9670 - val_loss: 0.1940 - val_accuracy: 0.8723\n",
      "Epoch 79/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0562 - accuracy: 0.9753 - val_loss: 0.1761 - val_accuracy: 0.8830\n",
      "Epoch 80/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0938 - accuracy: 0.9482 - val_loss: 0.1844 - val_accuracy: 0.8830\n",
      "Epoch 81/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0627 - accuracy: 0.9729 - val_loss: 0.1759 - val_accuracy: 0.8830\n",
      "Epoch 82/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0577 - accuracy: 0.9753 - val_loss: 0.1765 - val_accuracy: 0.8830\n",
      "Epoch 83/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0516 - accuracy: 0.9823 - val_loss: 0.1682 - val_accuracy: 0.9149\n",
      "Epoch 84/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0495 - accuracy: 0.9812 - val_loss: 0.2417 - val_accuracy: 0.8511\n",
      "Epoch 85/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0774 - accuracy: 0.9611 - val_loss: 0.3210 - val_accuracy: 0.8191\n",
      "Epoch 86/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0577 - accuracy: 0.9776 - val_loss: 0.1861 - val_accuracy: 0.8830\n",
      "Epoch 87/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0643 - accuracy: 0.9717 - val_loss: 0.2056 - val_accuracy: 0.8617\n",
      "Epoch 88/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0762 - accuracy: 0.9600 - val_loss: 0.1851 - val_accuracy: 0.8830\n",
      "Epoch 89/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0583 - accuracy: 0.9717 - val_loss: 0.1987 - val_accuracy: 0.8617\n",
      "Epoch 90/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0637 - accuracy: 0.9764 - val_loss: 0.2010 - val_accuracy: 0.8511\n",
      "Epoch 91/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0916 - accuracy: 0.9505 - val_loss: 0.1743 - val_accuracy: 0.8830\n",
      "Epoch 92/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0701 - accuracy: 0.9741 - val_loss: 0.1881 - val_accuracy: 0.8617\n",
      "Epoch 93/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0762 - accuracy: 0.9647 - val_loss: 0.1773 - val_accuracy: 0.8830\n",
      "Epoch 94/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0465 - accuracy: 0.9812 - val_loss: 0.1829 - val_accuracy: 0.8936\n",
      "Epoch 95/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0542 - accuracy: 0.9800 - val_loss: 0.2698 - val_accuracy: 0.8298\n",
      "Epoch 96/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0459 - accuracy: 0.9823 - val_loss: 0.1651 - val_accuracy: 0.9149\n",
      "Epoch 97/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0586 - accuracy: 0.9717 - val_loss: 0.1794 - val_accuracy: 0.9149\n",
      "Epoch 98/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0877 - accuracy: 0.9635 - val_loss: 0.2132 - val_accuracy: 0.8723\n",
      "Epoch 99/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0557 - accuracy: 0.9741 - val_loss: 0.2654 - val_accuracy: 0.8617\n",
      "Epoch 100/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0341 - accuracy: 0.9859 - val_loss: 0.2172 - val_accuracy: 0.8617\n",
      "Score for fold 10: loss of 0.21718080341815948; accuracy of 86.17021441459656%\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 0.3027554452419281 - Accuracy: 83.15789699554443%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 0.1716012805700302 - Accuracy: 90.52631855010986%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 0.2256220430135727 - Accuracy: 86.3157868385315%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 0.2628939747810364 - Accuracy: 86.17021441459656%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 0.32961198687553406 - Accuracy: 81.91489577293396%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 6 - Loss: 0.12696503102779388 - Accuracy: 91.4893627166748%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 7 - Loss: 0.506381630897522 - Accuracy: 85.10638475418091%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 8 - Loss: 0.22537493705749512 - Accuracy: 90.42553305625916%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 9 - Loss: 0.3253408372402191 - Accuracy: 84.04255509376526%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 10 - Loss: 0.21718080341815948 - Accuracy: 86.17021441459656%\n",
      "------------------------------------------------------------------------\n",
      "Average scores for all folds:\n",
      "> Accuracy: 86.5319162607193 (+- 3.113359735460231)\n",
      "> Loss: 0.2693727970123291\n",
      "------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "acc_per_fold = []\n",
    "loss_per_fold = []\n",
    "\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True)\n",
    "\n",
    "fold_no = 1\n",
    "\n",
    "with tf.device('/gpu:0'):\n",
    "    for train, test in kfold.split(X_train, y_train):\n",
    "        print(train.shape, test.shape)\n",
    "    \n",
    "        input = Input(shape=(300, 300, 2))\n",
    "        model = EfficientNetB3(input_tensor=input, include_top=False, weights=None, pooling='avg')\n",
    "        \n",
    "        x = model.output\n",
    "\n",
    "        x = Dense(3, activation='softmax', name='softmax', kernel_initializer='he_normal')(x)\n",
    "        model = Model(model.input, x)\n",
    "\n",
    "        \n",
    "        optimizer = tf.keras.optimizers.SGD(lr=0.01, decay=1e-3, momentum=0.9, nesterov=True)\n",
    "        #optimizer = optimizers.Adam(lr=0.001)\n",
    "        \n",
    "        #callbacks_list = [LearningRateSchedule([20,40])]\n",
    "        \n",
    "        model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "        \n",
    "        print('------------------------------------------------------------------------')\n",
    "        print(f'Training for fold {fold_no} ...')\n",
    "        \n",
    "        history = model.fit(inputs[train], targets[train], \n",
    "                            batch_size=4, \n",
    "                            epochs=100, \n",
    "                            verbose=1,\n",
    "                            validation_data=(inputs[test], targets[test]),\n",
    "                            callbacks = tensorboard_callback) # 여기에 Validation set을 넣어야되는거 아닌가?\n",
    "        \n",
    "        scores = model.evaluate(inputs[test], targets[test], verbose=0)\n",
    "        print(f'Score for fold {fold_no}: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}%')\n",
    "        acc_per_fold.append(scores[1] * 100)\n",
    "        loss_per_fold.append(scores[0])\n",
    "     \n",
    "\n",
    "        #np.savetxt('E:/Result/ver.3.25/EfficientNetB1/acc/'+ f'acc_{i}.csv', acc_per_fold, delimiter=\",\")\n",
    "        #np.savetxt('E:/Result/ver.3.25/EfficientNetB1/loss/'+ f'loss_{i}.csv', loss_per_fold, delimiter=\",\")\n",
    "        \n",
    "        model.save('E:/Result/ver.3.26/EfficientNetB3/' + f'Eff_{fold_no}.h5',fold_no)\n",
    "        \n",
    "        fold_no = fold_no + 1\n",
    "\n",
    "# == Provide average scores ==\n",
    "print('------------------------------------------------------------------------')\n",
    "print('Score per fold')\n",
    "for i in range(0, len(acc_per_fold)):\n",
    "  print('------------------------------------------------------------------------')\n",
    "  print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]}%')\n",
    "print('------------------------------------------------------------------------')\n",
    "print('Average scores for all folds:')\n",
    "print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n",
    "print(f'> Loss: {np.mean(loss_per_fold)}')\n",
    "print('------------------------------------------------------------------------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pylab as plt\n",
    "\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.legend(['training', 'validation'], loc= 'upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.legend(['training', 'validaion'], loc= 'upper right')\n",
    "plt.show()\\\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "351c676f2e382adcf1b705bc0333b8a2b296b5ec0e03cae74b6f4b5ae2fa5d28"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('tf2.0-gpu')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
