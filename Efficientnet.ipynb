{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "tf.test.is_gpu_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 14851708696869686822\n",
      "xla_global_id: -1\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 6300696576\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 8234851641239509871\n",
      "physical_device_desc: \"device: 0, name: NVIDIA GeForce RTX 2060 SUPER, pci bus id: 0000:0a:00.0, compute capability: 7.5\"\n",
      "xla_global_id: 416903419\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from math import ceil\n",
    "from numba import njit, prange\n",
    "from sklearn.utils.validation import check_array\n",
    "from pyts.preprocessing import MinMaxScaler\n",
    "from pyts.approximation import PiecewiseAggregateApproximation\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.utils import np_utils\n",
    "import os \n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "Welding_data = np.load('E:/Result/ver.3.22/MTF/MTF.npz')\n",
    "\n",
    "X_data = Welding_data['X_data']\n",
    "y_data = Welding_data['y_data']\n",
    "i_data = Welding_data['i_data']\n",
    "\n",
    "Welding_data.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 7\n",
    "np.random.seed()\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test,i_train, i_test = train_test_split(X_data, y_data, i_data, test_size= 0.2, shuffle= True, random_state= seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = np.concatenate((X_train, X_test))\n",
    "targets = np.concatenate((y_train, y_test))\n",
    "index = np.concatenate((i_train, i_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = np_utils.to_categorical(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.efficientnet import EfficientNetB0\n",
    "from keras.layers import Dense, Input, Activation, Flatten, Dropout\n",
    "from keras.models import Model\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras import optimizers\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers.normalization.batch_normalization import BatchNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_3 (InputLayer)           [(None, 300, 300, 2  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " rescaling_2 (Rescaling)        (None, 300, 300, 2)  0           ['input_3[0][0]']                \n",
      "                                                                                                  \n",
      " normalization_2 (Normalization  (None, 300, 300, 2)  5          ['rescaling_2[0][0]']            \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " stem_conv_pad (ZeroPadding2D)  (None, 301, 301, 2)  0           ['normalization_2[0][0]']        \n",
      "                                                                                                  \n",
      " stem_conv (Conv2D)             (None, 150, 150, 32  576         ['stem_conv_pad[0][0]']          \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " stem_bn (BatchNormalization)   (None, 150, 150, 32  128         ['stem_conv[0][0]']              \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " stem_activation (Activation)   (None, 150, 150, 32  0           ['stem_bn[0][0]']                \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " block1a_dwconv (DepthwiseConv2  (None, 150, 150, 32  288        ['stem_activation[0][0]']        \n",
      " D)                             )                                                                 \n",
      "                                                                                                  \n",
      " block1a_bn (BatchNormalization  (None, 150, 150, 32  128        ['block1a_dwconv[0][0]']         \n",
      " )                              )                                                                 \n",
      "                                                                                                  \n",
      " block1a_activation (Activation  (None, 150, 150, 32  0          ['block1a_bn[0][0]']             \n",
      " )                              )                                                                 \n",
      "                                                                                                  \n",
      " block1a_se_squeeze (GlobalAver  (None, 32)          0           ['block1a_activation[0][0]']     \n",
      " agePooling2D)                                                                                    \n",
      "                                                                                                  \n",
      " block1a_se_reshape (Reshape)   (None, 1, 1, 32)     0           ['block1a_se_squeeze[0][0]']     \n",
      "                                                                                                  \n",
      " block1a_se_reduce (Conv2D)     (None, 1, 1, 8)      264         ['block1a_se_reshape[0][0]']     \n",
      "                                                                                                  \n",
      " block1a_se_expand (Conv2D)     (None, 1, 1, 32)     288         ['block1a_se_reduce[0][0]']      \n",
      "                                                                                                  \n",
      " block1a_se_excite (Multiply)   (None, 150, 150, 32  0           ['block1a_activation[0][0]',     \n",
      "                                )                                 'block1a_se_expand[0][0]']      \n",
      "                                                                                                  \n",
      " block1a_project_conv (Conv2D)  (None, 150, 150, 16  512         ['block1a_se_excite[0][0]']      \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " block1a_project_bn (BatchNorma  (None, 150, 150, 16  64         ['block1a_project_conv[0][0]']   \n",
      " lization)                      )                                                                 \n",
      "                                                                                                  \n",
      " block2a_expand_conv (Conv2D)   (None, 150, 150, 96  1536        ['block1a_project_bn[0][0]']     \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " block2a_expand_bn (BatchNormal  (None, 150, 150, 96  384        ['block2a_expand_conv[0][0]']    \n",
      " ization)                       )                                                                 \n",
      "                                                                                                  \n",
      " block2a_expand_activation (Act  (None, 150, 150, 96  0          ['block2a_expand_bn[0][0]']      \n",
      " ivation)                       )                                                                 \n",
      "                                                                                                  \n",
      " block2a_dwconv_pad (ZeroPaddin  (None, 151, 151, 96  0          ['block2a_expand_activation[0][0]\n",
      " g2D)                           )                                ']                               \n",
      "                                                                                                  \n",
      " block2a_dwconv (DepthwiseConv2  (None, 75, 75, 96)  864         ['block2a_dwconv_pad[0][0]']     \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " block2a_bn (BatchNormalization  (None, 75, 75, 96)  384         ['block2a_dwconv[0][0]']         \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block2a_activation (Activation  (None, 75, 75, 96)  0           ['block2a_bn[0][0]']             \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block2a_se_squeeze (GlobalAver  (None, 96)          0           ['block2a_activation[0][0]']     \n",
      " agePooling2D)                                                                                    \n",
      "                                                                                                  \n",
      " block2a_se_reshape (Reshape)   (None, 1, 1, 96)     0           ['block2a_se_squeeze[0][0]']     \n",
      "                                                                                                  \n",
      " block2a_se_reduce (Conv2D)     (None, 1, 1, 4)      388         ['block2a_se_reshape[0][0]']     \n",
      "                                                                                                  \n",
      " block2a_se_expand (Conv2D)     (None, 1, 1, 96)     480         ['block2a_se_reduce[0][0]']      \n",
      "                                                                                                  \n",
      " block2a_se_excite (Multiply)   (None, 75, 75, 96)   0           ['block2a_activation[0][0]',     \n",
      "                                                                  'block2a_se_expand[0][0]']      \n",
      "                                                                                                  \n",
      " block2a_project_conv (Conv2D)  (None, 75, 75, 24)   2304        ['block2a_se_excite[0][0]']      \n",
      "                                                                                                  \n",
      " block2a_project_bn (BatchNorma  (None, 75, 75, 24)  96          ['block2a_project_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block2b_expand_conv (Conv2D)   (None, 75, 75, 144)  3456        ['block2a_project_bn[0][0]']     \n",
      "                                                                                                  \n",
      " block2b_expand_bn (BatchNormal  (None, 75, 75, 144)  576        ['block2b_expand_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block2b_expand_activation (Act  (None, 75, 75, 144)  0          ['block2b_expand_bn[0][0]']      \n",
      " ivation)                                                                                         \n",
      "                                                                                                  \n",
      " block2b_dwconv (DepthwiseConv2  (None, 75, 75, 144)  1296       ['block2b_expand_activation[0][0]\n",
      " D)                                                              ']                               \n",
      "                                                                                                  \n",
      " block2b_bn (BatchNormalization  (None, 75, 75, 144)  576        ['block2b_dwconv[0][0]']         \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block2b_activation (Activation  (None, 75, 75, 144)  0          ['block2b_bn[0][0]']             \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block2b_se_squeeze (GlobalAver  (None, 144)         0           ['block2b_activation[0][0]']     \n",
      " agePooling2D)                                                                                    \n",
      "                                                                                                  \n",
      " block2b_se_reshape (Reshape)   (None, 1, 1, 144)    0           ['block2b_se_squeeze[0][0]']     \n",
      "                                                                                                  \n",
      " block2b_se_reduce (Conv2D)     (None, 1, 1, 6)      870         ['block2b_se_reshape[0][0]']     \n",
      "                                                                                                  \n",
      " block2b_se_expand (Conv2D)     (None, 1, 1, 144)    1008        ['block2b_se_reduce[0][0]']      \n",
      "                                                                                                  \n",
      " block2b_se_excite (Multiply)   (None, 75, 75, 144)  0           ['block2b_activation[0][0]',     \n",
      "                                                                  'block2b_se_expand[0][0]']      \n",
      "                                                                                                  \n",
      " block2b_project_conv (Conv2D)  (None, 75, 75, 24)   3456        ['block2b_se_excite[0][0]']      \n",
      "                                                                                                  \n",
      " block2b_project_bn (BatchNorma  (None, 75, 75, 24)  96          ['block2b_project_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block2b_drop (Dropout)         (None, 75, 75, 24)   0           ['block2b_project_bn[0][0]']     \n",
      "                                                                                                  \n",
      " block2b_add (Add)              (None, 75, 75, 24)   0           ['block2b_drop[0][0]',           \n",
      "                                                                  'block2a_project_bn[0][0]']     \n",
      "                                                                                                  \n",
      " block3a_expand_conv (Conv2D)   (None, 75, 75, 144)  3456        ['block2b_add[0][0]']            \n",
      "                                                                                                  \n",
      " block3a_expand_bn (BatchNormal  (None, 75, 75, 144)  576        ['block3a_expand_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block3a_expand_activation (Act  (None, 75, 75, 144)  0          ['block3a_expand_bn[0][0]']      \n",
      " ivation)                                                                                         \n",
      "                                                                                                  \n",
      " block3a_dwconv_pad (ZeroPaddin  (None, 79, 79, 144)  0          ['block3a_expand_activation[0][0]\n",
      " g2D)                                                            ']                               \n",
      "                                                                                                  \n",
      " block3a_dwconv (DepthwiseConv2  (None, 38, 38, 144)  3600       ['block3a_dwconv_pad[0][0]']     \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " block3a_bn (BatchNormalization  (None, 38, 38, 144)  576        ['block3a_dwconv[0][0]']         \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block3a_activation (Activation  (None, 38, 38, 144)  0          ['block3a_bn[0][0]']             \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block3a_se_squeeze (GlobalAver  (None, 144)         0           ['block3a_activation[0][0]']     \n",
      " agePooling2D)                                                                                    \n",
      "                                                                                                  \n",
      " block3a_se_reshape (Reshape)   (None, 1, 1, 144)    0           ['block3a_se_squeeze[0][0]']     \n",
      "                                                                                                  \n",
      " block3a_se_reduce (Conv2D)     (None, 1, 1, 6)      870         ['block3a_se_reshape[0][0]']     \n",
      "                                                                                                  \n",
      " block3a_se_expand (Conv2D)     (None, 1, 1, 144)    1008        ['block3a_se_reduce[0][0]']      \n",
      "                                                                                                  \n",
      " block3a_se_excite (Multiply)   (None, 38, 38, 144)  0           ['block3a_activation[0][0]',     \n",
      "                                                                  'block3a_se_expand[0][0]']      \n",
      "                                                                                                  \n",
      " block3a_project_conv (Conv2D)  (None, 38, 38, 40)   5760        ['block3a_se_excite[0][0]']      \n",
      "                                                                                                  \n",
      " block3a_project_bn (BatchNorma  (None, 38, 38, 40)  160         ['block3a_project_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block3b_expand_conv (Conv2D)   (None, 38, 38, 240)  9600        ['block3a_project_bn[0][0]']     \n",
      "                                                                                                  \n",
      " block3b_expand_bn (BatchNormal  (None, 38, 38, 240)  960        ['block3b_expand_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block3b_expand_activation (Act  (None, 38, 38, 240)  0          ['block3b_expand_bn[0][0]']      \n",
      " ivation)                                                                                         \n",
      "                                                                                                  \n",
      " block3b_dwconv (DepthwiseConv2  (None, 38, 38, 240)  6000       ['block3b_expand_activation[0][0]\n",
      " D)                                                              ']                               \n",
      "                                                                                                  \n",
      " block3b_bn (BatchNormalization  (None, 38, 38, 240)  960        ['block3b_dwconv[0][0]']         \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block3b_activation (Activation  (None, 38, 38, 240)  0          ['block3b_bn[0][0]']             \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block3b_se_squeeze (GlobalAver  (None, 240)         0           ['block3b_activation[0][0]']     \n",
      " agePooling2D)                                                                                    \n",
      "                                                                                                  \n",
      " block3b_se_reshape (Reshape)   (None, 1, 1, 240)    0           ['block3b_se_squeeze[0][0]']     \n",
      "                                                                                                  \n",
      " block3b_se_reduce (Conv2D)     (None, 1, 1, 10)     2410        ['block3b_se_reshape[0][0]']     \n",
      "                                                                                                  \n",
      " block3b_se_expand (Conv2D)     (None, 1, 1, 240)    2640        ['block3b_se_reduce[0][0]']      \n",
      "                                                                                                  \n",
      " block3b_se_excite (Multiply)   (None, 38, 38, 240)  0           ['block3b_activation[0][0]',     \n",
      "                                                                  'block3b_se_expand[0][0]']      \n",
      "                                                                                                  \n",
      " block3b_project_conv (Conv2D)  (None, 38, 38, 40)   9600        ['block3b_se_excite[0][0]']      \n",
      "                                                                                                  \n",
      " block3b_project_bn (BatchNorma  (None, 38, 38, 40)  160         ['block3b_project_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block3b_drop (Dropout)         (None, 38, 38, 40)   0           ['block3b_project_bn[0][0]']     \n",
      "                                                                                                  \n",
      " block3b_add (Add)              (None, 38, 38, 40)   0           ['block3b_drop[0][0]',           \n",
      "                                                                  'block3a_project_bn[0][0]']     \n",
      "                                                                                                  \n",
      " block4a_expand_conv (Conv2D)   (None, 38, 38, 240)  9600        ['block3b_add[0][0]']            \n",
      "                                                                                                  \n",
      " block4a_expand_bn (BatchNormal  (None, 38, 38, 240)  960        ['block4a_expand_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block4a_expand_activation (Act  (None, 38, 38, 240)  0          ['block4a_expand_bn[0][0]']      \n",
      " ivation)                                                                                         \n",
      "                                                                                                  \n",
      " block4a_dwconv_pad (ZeroPaddin  (None, 39, 39, 240)  0          ['block4a_expand_activation[0][0]\n",
      " g2D)                                                            ']                               \n",
      "                                                                                                  \n",
      " block4a_dwconv (DepthwiseConv2  (None, 19, 19, 240)  2160       ['block4a_dwconv_pad[0][0]']     \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " block4a_bn (BatchNormalization  (None, 19, 19, 240)  960        ['block4a_dwconv[0][0]']         \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block4a_activation (Activation  (None, 19, 19, 240)  0          ['block4a_bn[0][0]']             \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block4a_se_squeeze (GlobalAver  (None, 240)         0           ['block4a_activation[0][0]']     \n",
      " agePooling2D)                                                                                    \n",
      "                                                                                                  \n",
      " block4a_se_reshape (Reshape)   (None, 1, 1, 240)    0           ['block4a_se_squeeze[0][0]']     \n",
      "                                                                                                  \n",
      " block4a_se_reduce (Conv2D)     (None, 1, 1, 10)     2410        ['block4a_se_reshape[0][0]']     \n",
      "                                                                                                  \n",
      " block4a_se_expand (Conv2D)     (None, 1, 1, 240)    2640        ['block4a_se_reduce[0][0]']      \n",
      "                                                                                                  \n",
      " block4a_se_excite (Multiply)   (None, 19, 19, 240)  0           ['block4a_activation[0][0]',     \n",
      "                                                                  'block4a_se_expand[0][0]']      \n",
      "                                                                                                  \n",
      " block4a_project_conv (Conv2D)  (None, 19, 19, 80)   19200       ['block4a_se_excite[0][0]']      \n",
      "                                                                                                  \n",
      " block4a_project_bn (BatchNorma  (None, 19, 19, 80)  320         ['block4a_project_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block4b_expand_conv (Conv2D)   (None, 19, 19, 480)  38400       ['block4a_project_bn[0][0]']     \n",
      "                                                                                                  \n",
      " block4b_expand_bn (BatchNormal  (None, 19, 19, 480)  1920       ['block4b_expand_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block4b_expand_activation (Act  (None, 19, 19, 480)  0          ['block4b_expand_bn[0][0]']      \n",
      " ivation)                                                                                         \n",
      "                                                                                                  \n",
      " block4b_dwconv (DepthwiseConv2  (None, 19, 19, 480)  4320       ['block4b_expand_activation[0][0]\n",
      " D)                                                              ']                               \n",
      "                                                                                                  \n",
      " block4b_bn (BatchNormalization  (None, 19, 19, 480)  1920       ['block4b_dwconv[0][0]']         \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block4b_activation (Activation  (None, 19, 19, 480)  0          ['block4b_bn[0][0]']             \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block4b_se_squeeze (GlobalAver  (None, 480)         0           ['block4b_activation[0][0]']     \n",
      " agePooling2D)                                                                                    \n",
      "                                                                                                  \n",
      " block4b_se_reshape (Reshape)   (None, 1, 1, 480)    0           ['block4b_se_squeeze[0][0]']     \n",
      "                                                                                                  \n",
      " block4b_se_reduce (Conv2D)     (None, 1, 1, 20)     9620        ['block4b_se_reshape[0][0]']     \n",
      "                                                                                                  \n",
      " block4b_se_expand (Conv2D)     (None, 1, 1, 480)    10080       ['block4b_se_reduce[0][0]']      \n",
      "                                                                                                  \n",
      " block4b_se_excite (Multiply)   (None, 19, 19, 480)  0           ['block4b_activation[0][0]',     \n",
      "                                                                  'block4b_se_expand[0][0]']      \n",
      "                                                                                                  \n",
      " block4b_project_conv (Conv2D)  (None, 19, 19, 80)   38400       ['block4b_se_excite[0][0]']      \n",
      "                                                                                                  \n",
      " block4b_project_bn (BatchNorma  (None, 19, 19, 80)  320         ['block4b_project_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block4b_drop (Dropout)         (None, 19, 19, 80)   0           ['block4b_project_bn[0][0]']     \n",
      "                                                                                                  \n",
      " block4b_add (Add)              (None, 19, 19, 80)   0           ['block4b_drop[0][0]',           \n",
      "                                                                  'block4a_project_bn[0][0]']     \n",
      "                                                                                                  \n",
      " block4c_expand_conv (Conv2D)   (None, 19, 19, 480)  38400       ['block4b_add[0][0]']            \n",
      "                                                                                                  \n",
      " block4c_expand_bn (BatchNormal  (None, 19, 19, 480)  1920       ['block4c_expand_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block4c_expand_activation (Act  (None, 19, 19, 480)  0          ['block4c_expand_bn[0][0]']      \n",
      " ivation)                                                                                         \n",
      "                                                                                                  \n",
      " block4c_dwconv (DepthwiseConv2  (None, 19, 19, 480)  4320       ['block4c_expand_activation[0][0]\n",
      " D)                                                              ']                               \n",
      "                                                                                                  \n",
      " block4c_bn (BatchNormalization  (None, 19, 19, 480)  1920       ['block4c_dwconv[0][0]']         \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block4c_activation (Activation  (None, 19, 19, 480)  0          ['block4c_bn[0][0]']             \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block4c_se_squeeze (GlobalAver  (None, 480)         0           ['block4c_activation[0][0]']     \n",
      " agePooling2D)                                                                                    \n",
      "                                                                                                  \n",
      " block4c_se_reshape (Reshape)   (None, 1, 1, 480)    0           ['block4c_se_squeeze[0][0]']     \n",
      "                                                                                                  \n",
      " block4c_se_reduce (Conv2D)     (None, 1, 1, 20)     9620        ['block4c_se_reshape[0][0]']     \n",
      "                                                                                                  \n",
      " block4c_se_expand (Conv2D)     (None, 1, 1, 480)    10080       ['block4c_se_reduce[0][0]']      \n",
      "                                                                                                  \n",
      " block4c_se_excite (Multiply)   (None, 19, 19, 480)  0           ['block4c_activation[0][0]',     \n",
      "                                                                  'block4c_se_expand[0][0]']      \n",
      "                                                                                                  \n",
      " block4c_project_conv (Conv2D)  (None, 19, 19, 80)   38400       ['block4c_se_excite[0][0]']      \n",
      "                                                                                                  \n",
      " block4c_project_bn (BatchNorma  (None, 19, 19, 80)  320         ['block4c_project_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block4c_drop (Dropout)         (None, 19, 19, 80)   0           ['block4c_project_bn[0][0]']     \n",
      "                                                                                                  \n",
      " block4c_add (Add)              (None, 19, 19, 80)   0           ['block4c_drop[0][0]',           \n",
      "                                                                  'block4b_add[0][0]']            \n",
      "                                                                                                  \n",
      " block5a_expand_conv (Conv2D)   (None, 19, 19, 480)  38400       ['block4c_add[0][0]']            \n",
      "                                                                                                  \n",
      " block5a_expand_bn (BatchNormal  (None, 19, 19, 480)  1920       ['block5a_expand_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block5a_expand_activation (Act  (None, 19, 19, 480)  0          ['block5a_expand_bn[0][0]']      \n",
      " ivation)                                                                                         \n",
      "                                                                                                  \n",
      " block5a_dwconv (DepthwiseConv2  (None, 19, 19, 480)  12000      ['block5a_expand_activation[0][0]\n",
      " D)                                                              ']                               \n",
      "                                                                                                  \n",
      " block5a_bn (BatchNormalization  (None, 19, 19, 480)  1920       ['block5a_dwconv[0][0]']         \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block5a_activation (Activation  (None, 19, 19, 480)  0          ['block5a_bn[0][0]']             \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block5a_se_squeeze (GlobalAver  (None, 480)         0           ['block5a_activation[0][0]']     \n",
      " agePooling2D)                                                                                    \n",
      "                                                                                                  \n",
      " block5a_se_reshape (Reshape)   (None, 1, 1, 480)    0           ['block5a_se_squeeze[0][0]']     \n",
      "                                                                                                  \n",
      " block5a_se_reduce (Conv2D)     (None, 1, 1, 20)     9620        ['block5a_se_reshape[0][0]']     \n",
      "                                                                                                  \n",
      " block5a_se_expand (Conv2D)     (None, 1, 1, 480)    10080       ['block5a_se_reduce[0][0]']      \n",
      "                                                                                                  \n",
      " block5a_se_excite (Multiply)   (None, 19, 19, 480)  0           ['block5a_activation[0][0]',     \n",
      "                                                                  'block5a_se_expand[0][0]']      \n",
      "                                                                                                  \n",
      " block5a_project_conv (Conv2D)  (None, 19, 19, 112)  53760       ['block5a_se_excite[0][0]']      \n",
      "                                                                                                  \n",
      " block5a_project_bn (BatchNorma  (None, 19, 19, 112)  448        ['block5a_project_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block5b_expand_conv (Conv2D)   (None, 19, 19, 672)  75264       ['block5a_project_bn[0][0]']     \n",
      "                                                                                                  \n",
      " block5b_expand_bn (BatchNormal  (None, 19, 19, 672)  2688       ['block5b_expand_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block5b_expand_activation (Act  (None, 19, 19, 672)  0          ['block5b_expand_bn[0][0]']      \n",
      " ivation)                                                                                         \n",
      "                                                                                                  \n",
      " block5b_dwconv (DepthwiseConv2  (None, 19, 19, 672)  16800      ['block5b_expand_activation[0][0]\n",
      " D)                                                              ']                               \n",
      "                                                                                                  \n",
      " block5b_bn (BatchNormalization  (None, 19, 19, 672)  2688       ['block5b_dwconv[0][0]']         \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block5b_activation (Activation  (None, 19, 19, 672)  0          ['block5b_bn[0][0]']             \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block5b_se_squeeze (GlobalAver  (None, 672)         0           ['block5b_activation[0][0]']     \n",
      " agePooling2D)                                                                                    \n",
      "                                                                                                  \n",
      " block5b_se_reshape (Reshape)   (None, 1, 1, 672)    0           ['block5b_se_squeeze[0][0]']     \n",
      "                                                                                                  \n",
      " block5b_se_reduce (Conv2D)     (None, 1, 1, 28)     18844       ['block5b_se_reshape[0][0]']     \n",
      "                                                                                                  \n",
      " block5b_se_expand (Conv2D)     (None, 1, 1, 672)    19488       ['block5b_se_reduce[0][0]']      \n",
      "                                                                                                  \n",
      " block5b_se_excite (Multiply)   (None, 19, 19, 672)  0           ['block5b_activation[0][0]',     \n",
      "                                                                  'block5b_se_expand[0][0]']      \n",
      "                                                                                                  \n",
      " block5b_project_conv (Conv2D)  (None, 19, 19, 112)  75264       ['block5b_se_excite[0][0]']      \n",
      "                                                                                                  \n",
      " block5b_project_bn (BatchNorma  (None, 19, 19, 112)  448        ['block5b_project_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block5b_drop (Dropout)         (None, 19, 19, 112)  0           ['block5b_project_bn[0][0]']     \n",
      "                                                                                                  \n",
      " block5b_add (Add)              (None, 19, 19, 112)  0           ['block5b_drop[0][0]',           \n",
      "                                                                  'block5a_project_bn[0][0]']     \n",
      "                                                                                                  \n",
      " block5c_expand_conv (Conv2D)   (None, 19, 19, 672)  75264       ['block5b_add[0][0]']            \n",
      "                                                                                                  \n",
      " block5c_expand_bn (BatchNormal  (None, 19, 19, 672)  2688       ['block5c_expand_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block5c_expand_activation (Act  (None, 19, 19, 672)  0          ['block5c_expand_bn[0][0]']      \n",
      " ivation)                                                                                         \n",
      "                                                                                                  \n",
      " block5c_dwconv (DepthwiseConv2  (None, 19, 19, 672)  16800      ['block5c_expand_activation[0][0]\n",
      " D)                                                              ']                               \n",
      "                                                                                                  \n",
      " block5c_bn (BatchNormalization  (None, 19, 19, 672)  2688       ['block5c_dwconv[0][0]']         \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block5c_activation (Activation  (None, 19, 19, 672)  0          ['block5c_bn[0][0]']             \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block5c_se_squeeze (GlobalAver  (None, 672)         0           ['block5c_activation[0][0]']     \n",
      " agePooling2D)                                                                                    \n",
      "                                                                                                  \n",
      " block5c_se_reshape (Reshape)   (None, 1, 1, 672)    0           ['block5c_se_squeeze[0][0]']     \n",
      "                                                                                                  \n",
      " block5c_se_reduce (Conv2D)     (None, 1, 1, 28)     18844       ['block5c_se_reshape[0][0]']     \n",
      "                                                                                                  \n",
      " block5c_se_expand (Conv2D)     (None, 1, 1, 672)    19488       ['block5c_se_reduce[0][0]']      \n",
      "                                                                                                  \n",
      " block5c_se_excite (Multiply)   (None, 19, 19, 672)  0           ['block5c_activation[0][0]',     \n",
      "                                                                  'block5c_se_expand[0][0]']      \n",
      "                                                                                                  \n",
      " block5c_project_conv (Conv2D)  (None, 19, 19, 112)  75264       ['block5c_se_excite[0][0]']      \n",
      "                                                                                                  \n",
      " block5c_project_bn (BatchNorma  (None, 19, 19, 112)  448        ['block5c_project_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block5c_drop (Dropout)         (None, 19, 19, 112)  0           ['block5c_project_bn[0][0]']     \n",
      "                                                                                                  \n",
      " block5c_add (Add)              (None, 19, 19, 112)  0           ['block5c_drop[0][0]',           \n",
      "                                                                  'block5b_add[0][0]']            \n",
      "                                                                                                  \n",
      " block6a_expand_conv (Conv2D)   (None, 19, 19, 672)  75264       ['block5c_add[0][0]']            \n",
      "                                                                                                  \n",
      " block6a_expand_bn (BatchNormal  (None, 19, 19, 672)  2688       ['block6a_expand_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block6a_expand_activation (Act  (None, 19, 19, 672)  0          ['block6a_expand_bn[0][0]']      \n",
      " ivation)                                                                                         \n",
      "                                                                                                  \n",
      " block6a_dwconv_pad (ZeroPaddin  (None, 23, 23, 672)  0          ['block6a_expand_activation[0][0]\n",
      " g2D)                                                            ']                               \n",
      "                                                                                                  \n",
      " block6a_dwconv (DepthwiseConv2  (None, 10, 10, 672)  16800      ['block6a_dwconv_pad[0][0]']     \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " block6a_bn (BatchNormalization  (None, 10, 10, 672)  2688       ['block6a_dwconv[0][0]']         \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block6a_activation (Activation  (None, 10, 10, 672)  0          ['block6a_bn[0][0]']             \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block6a_se_squeeze (GlobalAver  (None, 672)         0           ['block6a_activation[0][0]']     \n",
      " agePooling2D)                                                                                    \n",
      "                                                                                                  \n",
      " block6a_se_reshape (Reshape)   (None, 1, 1, 672)    0           ['block6a_se_squeeze[0][0]']     \n",
      "                                                                                                  \n",
      " block6a_se_reduce (Conv2D)     (None, 1, 1, 28)     18844       ['block6a_se_reshape[0][0]']     \n",
      "                                                                                                  \n",
      " block6a_se_expand (Conv2D)     (None, 1, 1, 672)    19488       ['block6a_se_reduce[0][0]']      \n",
      "                                                                                                  \n",
      " block6a_se_excite (Multiply)   (None, 10, 10, 672)  0           ['block6a_activation[0][0]',     \n",
      "                                                                  'block6a_se_expand[0][0]']      \n",
      "                                                                                                  \n",
      " block6a_project_conv (Conv2D)  (None, 10, 10, 192)  129024      ['block6a_se_excite[0][0]']      \n",
      "                                                                                                  \n",
      " block6a_project_bn (BatchNorma  (None, 10, 10, 192)  768        ['block6a_project_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block6b_expand_conv (Conv2D)   (None, 10, 10, 1152  221184      ['block6a_project_bn[0][0]']     \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " block6b_expand_bn (BatchNormal  (None, 10, 10, 1152  4608       ['block6b_expand_conv[0][0]']    \n",
      " ization)                       )                                                                 \n",
      "                                                                                                  \n",
      " block6b_expand_activation (Act  (None, 10, 10, 1152  0          ['block6b_expand_bn[0][0]']      \n",
      " ivation)                       )                                                                 \n",
      "                                                                                                  \n",
      " block6b_dwconv (DepthwiseConv2  (None, 10, 10, 1152  28800      ['block6b_expand_activation[0][0]\n",
      " D)                             )                                ']                               \n",
      "                                                                                                  \n",
      " block6b_bn (BatchNormalization  (None, 10, 10, 1152  4608       ['block6b_dwconv[0][0]']         \n",
      " )                              )                                                                 \n",
      "                                                                                                  \n",
      " block6b_activation (Activation  (None, 10, 10, 1152  0          ['block6b_bn[0][0]']             \n",
      " )                              )                                                                 \n",
      "                                                                                                  \n",
      " block6b_se_squeeze (GlobalAver  (None, 1152)        0           ['block6b_activation[0][0]']     \n",
      " agePooling2D)                                                                                    \n",
      "                                                                                                  \n",
      " block6b_se_reshape (Reshape)   (None, 1, 1, 1152)   0           ['block6b_se_squeeze[0][0]']     \n",
      "                                                                                                  \n",
      " block6b_se_reduce (Conv2D)     (None, 1, 1, 48)     55344       ['block6b_se_reshape[0][0]']     \n",
      "                                                                                                  \n",
      " block6b_se_expand (Conv2D)     (None, 1, 1, 1152)   56448       ['block6b_se_reduce[0][0]']      \n",
      "                                                                                                  \n",
      " block6b_se_excite (Multiply)   (None, 10, 10, 1152  0           ['block6b_activation[0][0]',     \n",
      "                                )                                 'block6b_se_expand[0][0]']      \n",
      "                                                                                                  \n",
      " block6b_project_conv (Conv2D)  (None, 10, 10, 192)  221184      ['block6b_se_excite[0][0]']      \n",
      "                                                                                                  \n",
      " block6b_project_bn (BatchNorma  (None, 10, 10, 192)  768        ['block6b_project_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block6b_drop (Dropout)         (None, 10, 10, 192)  0           ['block6b_project_bn[0][0]']     \n",
      "                                                                                                  \n",
      " block6b_add (Add)              (None, 10, 10, 192)  0           ['block6b_drop[0][0]',           \n",
      "                                                                  'block6a_project_bn[0][0]']     \n",
      "                                                                                                  \n",
      " block6c_expand_conv (Conv2D)   (None, 10, 10, 1152  221184      ['block6b_add[0][0]']            \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " block6c_expand_bn (BatchNormal  (None, 10, 10, 1152  4608       ['block6c_expand_conv[0][0]']    \n",
      " ization)                       )                                                                 \n",
      "                                                                                                  \n",
      " block6c_expand_activation (Act  (None, 10, 10, 1152  0          ['block6c_expand_bn[0][0]']      \n",
      " ivation)                       )                                                                 \n",
      "                                                                                                  \n",
      " block6c_dwconv (DepthwiseConv2  (None, 10, 10, 1152  28800      ['block6c_expand_activation[0][0]\n",
      " D)                             )                                ']                               \n",
      "                                                                                                  \n",
      " block6c_bn (BatchNormalization  (None, 10, 10, 1152  4608       ['block6c_dwconv[0][0]']         \n",
      " )                              )                                                                 \n",
      "                                                                                                  \n",
      " block6c_activation (Activation  (None, 10, 10, 1152  0          ['block6c_bn[0][0]']             \n",
      " )                              )                                                                 \n",
      "                                                                                                  \n",
      " block6c_se_squeeze (GlobalAver  (None, 1152)        0           ['block6c_activation[0][0]']     \n",
      " agePooling2D)                                                                                    \n",
      "                                                                                                  \n",
      " block6c_se_reshape (Reshape)   (None, 1, 1, 1152)   0           ['block6c_se_squeeze[0][0]']     \n",
      "                                                                                                  \n",
      " block6c_se_reduce (Conv2D)     (None, 1, 1, 48)     55344       ['block6c_se_reshape[0][0]']     \n",
      "                                                                                                  \n",
      " block6c_se_expand (Conv2D)     (None, 1, 1, 1152)   56448       ['block6c_se_reduce[0][0]']      \n",
      "                                                                                                  \n",
      " block6c_se_excite (Multiply)   (None, 10, 10, 1152  0           ['block6c_activation[0][0]',     \n",
      "                                )                                 'block6c_se_expand[0][0]']      \n",
      "                                                                                                  \n",
      " block6c_project_conv (Conv2D)  (None, 10, 10, 192)  221184      ['block6c_se_excite[0][0]']      \n",
      "                                                                                                  \n",
      " block6c_project_bn (BatchNorma  (None, 10, 10, 192)  768        ['block6c_project_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block6c_drop (Dropout)         (None, 10, 10, 192)  0           ['block6c_project_bn[0][0]']     \n",
      "                                                                                                  \n",
      " block6c_add (Add)              (None, 10, 10, 192)  0           ['block6c_drop[0][0]',           \n",
      "                                                                  'block6b_add[0][0]']            \n",
      "                                                                                                  \n",
      " block6d_expand_conv (Conv2D)   (None, 10, 10, 1152  221184      ['block6c_add[0][0]']            \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " block6d_expand_bn (BatchNormal  (None, 10, 10, 1152  4608       ['block6d_expand_conv[0][0]']    \n",
      " ization)                       )                                                                 \n",
      "                                                                                                  \n",
      " block6d_expand_activation (Act  (None, 10, 10, 1152  0          ['block6d_expand_bn[0][0]']      \n",
      " ivation)                       )                                                                 \n",
      "                                                                                                  \n",
      " block6d_dwconv (DepthwiseConv2  (None, 10, 10, 1152  28800      ['block6d_expand_activation[0][0]\n",
      " D)                             )                                ']                               \n",
      "                                                                                                  \n",
      " block6d_bn (BatchNormalization  (None, 10, 10, 1152  4608       ['block6d_dwconv[0][0]']         \n",
      " )                              )                                                                 \n",
      "                                                                                                  \n",
      " block6d_activation (Activation  (None, 10, 10, 1152  0          ['block6d_bn[0][0]']             \n",
      " )                              )                                                                 \n",
      "                                                                                                  \n",
      " block6d_se_squeeze (GlobalAver  (None, 1152)        0           ['block6d_activation[0][0]']     \n",
      " agePooling2D)                                                                                    \n",
      "                                                                                                  \n",
      " block6d_se_reshape (Reshape)   (None, 1, 1, 1152)   0           ['block6d_se_squeeze[0][0]']     \n",
      "                                                                                                  \n",
      " block6d_se_reduce (Conv2D)     (None, 1, 1, 48)     55344       ['block6d_se_reshape[0][0]']     \n",
      "                                                                                                  \n",
      " block6d_se_expand (Conv2D)     (None, 1, 1, 1152)   56448       ['block6d_se_reduce[0][0]']      \n",
      "                                                                                                  \n",
      " block6d_se_excite (Multiply)   (None, 10, 10, 1152  0           ['block6d_activation[0][0]',     \n",
      "                                )                                 'block6d_se_expand[0][0]']      \n",
      "                                                                                                  \n",
      " block6d_project_conv (Conv2D)  (None, 10, 10, 192)  221184      ['block6d_se_excite[0][0]']      \n",
      "                                                                                                  \n",
      " block6d_project_bn (BatchNorma  (None, 10, 10, 192)  768        ['block6d_project_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block6d_drop (Dropout)         (None, 10, 10, 192)  0           ['block6d_project_bn[0][0]']     \n",
      "                                                                                                  \n",
      " block6d_add (Add)              (None, 10, 10, 192)  0           ['block6d_drop[0][0]',           \n",
      "                                                                  'block6c_add[0][0]']            \n",
      "                                                                                                  \n",
      " block7a_expand_conv (Conv2D)   (None, 10, 10, 1152  221184      ['block6d_add[0][0]']            \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " block7a_expand_bn (BatchNormal  (None, 10, 10, 1152  4608       ['block7a_expand_conv[0][0]']    \n",
      " ization)                       )                                                                 \n",
      "                                                                                                  \n",
      " block7a_expand_activation (Act  (None, 10, 10, 1152  0          ['block7a_expand_bn[0][0]']      \n",
      " ivation)                       )                                                                 \n",
      "                                                                                                  \n",
      " block7a_dwconv (DepthwiseConv2  (None, 10, 10, 1152  10368      ['block7a_expand_activation[0][0]\n",
      " D)                             )                                ']                               \n",
      "                                                                                                  \n",
      " block7a_bn (BatchNormalization  (None, 10, 10, 1152  4608       ['block7a_dwconv[0][0]']         \n",
      " )                              )                                                                 \n",
      "                                                                                                  \n",
      " block7a_activation (Activation  (None, 10, 10, 1152  0          ['block7a_bn[0][0]']             \n",
      " )                              )                                                                 \n",
      "                                                                                                  \n",
      " block7a_se_squeeze (GlobalAver  (None, 1152)        0           ['block7a_activation[0][0]']     \n",
      " agePooling2D)                                                                                    \n",
      "                                                                                                  \n",
      " block7a_se_reshape (Reshape)   (None, 1, 1, 1152)   0           ['block7a_se_squeeze[0][0]']     \n",
      "                                                                                                  \n",
      " block7a_se_reduce (Conv2D)     (None, 1, 1, 48)     55344       ['block7a_se_reshape[0][0]']     \n",
      "                                                                                                  \n",
      " block7a_se_expand (Conv2D)     (None, 1, 1, 1152)   56448       ['block7a_se_reduce[0][0]']      \n",
      "                                                                                                  \n",
      " block7a_se_excite (Multiply)   (None, 10, 10, 1152  0           ['block7a_activation[0][0]',     \n",
      "                                )                                 'block7a_se_expand[0][0]']      \n",
      "                                                                                                  \n",
      " block7a_project_conv (Conv2D)  (None, 10, 10, 320)  368640      ['block7a_se_excite[0][0]']      \n",
      "                                                                                                  \n",
      " block7a_project_bn (BatchNorma  (None, 10, 10, 320)  1280       ['block7a_project_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " top_conv (Conv2D)              (None, 10, 10, 1280  409600      ['block7a_project_bn[0][0]']     \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " top_bn (BatchNormalization)    (None, 10, 10, 1280  5120        ['top_conv[0][0]']               \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " top_activation (Activation)    (None, 10, 10, 1280  0           ['top_bn[0][0]']                 \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " avg_pool (GlobalAveragePooling  (None, 1280)        0           ['top_activation[0][0]']         \n",
      " 2D)                                                                                              \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 1280)         0           ['avg_pool[0][0]']               \n",
      "                                                                                                  \n",
      " softmax (Dense)                (None, 3)            3843        ['dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 4,053,124\n",
      "Trainable params: 4,011,103\n",
      "Non-trainable params: 42,021\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input = Input(shape= (300,300,2))\n",
    "model = EfficientNetB0(\n",
    "    input_tensor= input,\n",
    "    include_top= False,\n",
    "    weights= None,\n",
    "    pooling= 'avg'\n",
    ")\n",
    "\n",
    "x = model.output\n",
    "x = Dropout(0,5)(x)\n",
    "x = Dense(3, activation= 'softmax', name= 'softmax')(x)\n",
    "\n",
    "model = Model(model.input, x)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf.keras.utils.plot_model(model, show_shapes= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "import datetime\n",
    "\n",
    "log_dir = \"logs/my_board/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir= log_dir, histogram_freq= 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118\n",
      "1061\n",
      "118\n",
      "1061\n",
      "118\n",
      "1061\n",
      "118\n",
      "1061\n",
      "118\n",
      "1061\n",
      "118\n",
      "1061\n",
      "118\n",
      "1061\n",
      "118\n",
      "1061\n",
      "118\n",
      "1061\n",
      "117\n",
      "1062\n"
     ]
    }
   ],
   "source": [
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "kfold = KFold(n_splits=10, shuffle= True, random_state= seed)\n",
    "\n",
    "test= []\n",
    "train= []\n",
    "test_= []\n",
    "train_= []\n",
    "\n",
    "for train, test in kfold.split(inputs, targets):\n",
    "    \n",
    "    print(len(test))\n",
    "    print(len(train))\n",
    "    \n",
    "    for i in zip(test):\n",
    "        test_.append(i)\n",
    "    for i in zip(train):\n",
    "        train_.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train_[0:1061]\n",
    "train = np.reshape(train, 1061)\n",
    "test = test_[0:117]\n",
    "test = np.reshape(test, 117)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1061,)\n",
      "(117,)\n"
     ]
    }
   ],
   "source": [
    "print(train.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(848,) (95,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jwhyu\\anaconda3\\envs\\venv\\lib\\site-packages\\keras\\optimizer_v2\\gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------\n",
      "Training for fold 1 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jwhyu\\anaconda3\\envs\\venv\\lib\\site-packages\\keras\\engine\\functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  layer_config = serialize_layer_fn(layer)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "212/212 [==============================] - 27s 95ms/step - loss: 0.5696 - accuracy: 0.4976 - val_loss: 1.5558 - val_accuracy: 0.3474\n",
      "Epoch 2/100\n",
      "212/212 [==============================] - 19s 90ms/step - loss: 0.4708 - accuracy: 0.6545 - val_loss: 1.2840 - val_accuracy: 0.3474\n",
      "Epoch 3/100\n",
      "212/212 [==============================] - 19s 91ms/step - loss: 0.4634 - accuracy: 0.6568 - val_loss: 0.7427 - val_accuracy: 0.3263\n",
      "Epoch 4/100\n",
      "212/212 [==============================] - 20s 92ms/step - loss: 0.4169 - accuracy: 0.6910 - val_loss: 3.1707 - val_accuracy: 0.3474\n",
      "Epoch 5/100\n",
      "212/212 [==============================] - 20s 92ms/step - loss: 0.4033 - accuracy: 0.7182 - val_loss: 0.6337 - val_accuracy: 0.5158\n",
      "Epoch 6/100\n",
      "212/212 [==============================] - 19s 91ms/step - loss: 0.3715 - accuracy: 0.7417 - val_loss: 2.4415 - val_accuracy: 0.6000\n",
      "Epoch 7/100\n",
      "212/212 [==============================] - 19s 92ms/step - loss: 0.3583 - accuracy: 0.7677 - val_loss: 1.5196 - val_accuracy: 0.3789\n",
      "Epoch 8/100\n",
      "212/212 [==============================] - 19s 92ms/step - loss: 0.3208 - accuracy: 0.8066 - val_loss: 1.3972 - val_accuracy: 0.6105\n",
      "Epoch 9/100\n",
      "212/212 [==============================] - 19s 92ms/step - loss: 0.3227 - accuracy: 0.7983 - val_loss: 0.5984 - val_accuracy: 0.6421\n",
      "Epoch 10/100\n",
      "212/212 [==============================] - 19s 91ms/step - loss: 0.2579 - accuracy: 0.8491 - val_loss: 1.2322 - val_accuracy: 0.6211\n",
      "Epoch 11/100\n",
      "212/212 [==============================] - 19s 91ms/step - loss: 0.2588 - accuracy: 0.8443 - val_loss: 10.3864 - val_accuracy: 0.3263\n",
      "Epoch 12/100\n",
      "212/212 [==============================] - 19s 91ms/step - loss: 0.2565 - accuracy: 0.8550 - val_loss: 0.8614 - val_accuracy: 0.5789\n",
      "Epoch 13/100\n",
      "212/212 [==============================] - 19s 91ms/step - loss: 0.2012 - accuracy: 0.8880 - val_loss: 0.4662 - val_accuracy: 0.6737\n",
      "Epoch 14/100\n",
      "212/212 [==============================] - 20s 92ms/step - loss: 0.2171 - accuracy: 0.8774 - val_loss: 0.7610 - val_accuracy: 0.6211\n",
      "Epoch 15/100\n",
      "212/212 [==============================] - 19s 92ms/step - loss: 0.1770 - accuracy: 0.9175 - val_loss: 1.7687 - val_accuracy: 0.6105\n",
      "Epoch 16/100\n",
      "212/212 [==============================] - 19s 91ms/step - loss: 0.2047 - accuracy: 0.8915 - val_loss: 1.9965 - val_accuracy: 0.3579\n",
      "Epoch 17/100\n",
      "212/212 [==============================] - 19s 90ms/step - loss: 0.1713 - accuracy: 0.9092 - val_loss: 0.6942 - val_accuracy: 0.7053\n",
      "Epoch 18/100\n",
      "212/212 [==============================] - 19s 90ms/step - loss: 0.1291 - accuracy: 0.9422 - val_loss: 3.8203 - val_accuracy: 0.6105\n",
      "Epoch 19/100\n",
      "212/212 [==============================] - 19s 91ms/step - loss: 0.1458 - accuracy: 0.9363 - val_loss: 3.0501 - val_accuracy: 0.4421\n",
      "Epoch 20/100\n",
      "212/212 [==============================] - 19s 90ms/step - loss: 0.1154 - accuracy: 0.9469 - val_loss: 1.3535 - val_accuracy: 0.3789\n",
      "Epoch 21/100\n",
      "212/212 [==============================] - 19s 91ms/step - loss: 0.1570 - accuracy: 0.9233 - val_loss: 0.8897 - val_accuracy: 0.5158\n",
      "Epoch 22/100\n",
      "212/212 [==============================] - 19s 90ms/step - loss: 0.1240 - accuracy: 0.9410 - val_loss: 1.0084 - val_accuracy: 0.6632\n",
      "Epoch 23/100\n",
      "212/212 [==============================] - 19s 90ms/step - loss: 0.1217 - accuracy: 0.9493 - val_loss: 2.0610 - val_accuracy: 0.4947\n",
      "Epoch 24/100\n",
      "212/212 [==============================] - 19s 91ms/step - loss: 0.1234 - accuracy: 0.9493 - val_loss: 0.6367 - val_accuracy: 0.7368\n",
      "Epoch 25/100\n",
      "212/212 [==============================] - 19s 91ms/step - loss: 0.1317 - accuracy: 0.9340 - val_loss: 1.6363 - val_accuracy: 0.6000\n",
      "Epoch 26/100\n",
      "212/212 [==============================] - 19s 91ms/step - loss: 0.0833 - accuracy: 0.9705 - val_loss: 0.7301 - val_accuracy: 0.5895\n",
      "Epoch 27/100\n",
      "212/212 [==============================] - 19s 91ms/step - loss: 0.1363 - accuracy: 0.9387 - val_loss: 0.3748 - val_accuracy: 0.8105\n",
      "Epoch 28/100\n",
      "212/212 [==============================] - 19s 91ms/step - loss: 0.1120 - accuracy: 0.9587 - val_loss: 0.8136 - val_accuracy: 0.6947\n",
      "Epoch 29/100\n",
      "212/212 [==============================] - 19s 91ms/step - loss: 0.0845 - accuracy: 0.9752 - val_loss: 0.3834 - val_accuracy: 0.7789\n",
      "Epoch 30/100\n",
      "212/212 [==============================] - 19s 91ms/step - loss: 0.1076 - accuracy: 0.9528 - val_loss: 0.4381 - val_accuracy: 0.7895\n",
      "Epoch 31/100\n",
      "212/212 [==============================] - 19s 90ms/step - loss: 0.0850 - accuracy: 0.9611 - val_loss: 0.8127 - val_accuracy: 0.7158\n",
      "Epoch 32/100\n",
      "212/212 [==============================] - 19s 90ms/step - loss: 0.0950 - accuracy: 0.9646 - val_loss: 0.5868 - val_accuracy: 0.7474\n",
      "Epoch 33/100\n",
      "212/212 [==============================] - 19s 90ms/step - loss: 0.0579 - accuracy: 0.9811 - val_loss: 0.5329 - val_accuracy: 0.7368\n",
      "Epoch 34/100\n",
      "212/212 [==============================] - 19s 90ms/step - loss: 0.0855 - accuracy: 0.9693 - val_loss: 0.9405 - val_accuracy: 0.7053\n",
      "Epoch 35/100\n",
      "212/212 [==============================] - 19s 90ms/step - loss: 0.1037 - accuracy: 0.9587 - val_loss: 1.2653 - val_accuracy: 0.6105\n",
      "Epoch 36/100\n",
      "212/212 [==============================] - 19s 90ms/step - loss: 0.0872 - accuracy: 0.9646 - val_loss: 0.6603 - val_accuracy: 0.6316\n",
      "Epoch 37/100\n",
      "212/212 [==============================] - 19s 90ms/step - loss: 0.0817 - accuracy: 0.9670 - val_loss: 0.6637 - val_accuracy: 0.7474\n",
      "Epoch 38/100\n",
      "212/212 [==============================] - 19s 90ms/step - loss: 0.0848 - accuracy: 0.9658 - val_loss: 0.5093 - val_accuracy: 0.7895\n",
      "Epoch 39/100\n",
      "212/212 [==============================] - 19s 90ms/step - loss: 0.0939 - accuracy: 0.9599 - val_loss: 0.5197 - val_accuracy: 0.7579\n",
      "Epoch 40/100\n",
      "212/212 [==============================] - 19s 90ms/step - loss: 0.0577 - accuracy: 0.9788 - val_loss: 0.3920 - val_accuracy: 0.8105\n",
      "Epoch 41/100\n",
      "212/212 [==============================] - 19s 90ms/step - loss: 0.0924 - accuracy: 0.9564 - val_loss: 0.7753 - val_accuracy: 0.7368\n",
      "Epoch 42/100\n",
      "212/212 [==============================] - 19s 90ms/step - loss: 0.1093 - accuracy: 0.9493 - val_loss: 0.4916 - val_accuracy: 0.7789\n",
      "Epoch 43/100\n",
      "212/212 [==============================] - 19s 91ms/step - loss: 0.0693 - accuracy: 0.9729 - val_loss: 0.4208 - val_accuracy: 0.7895\n",
      "Epoch 44/100\n",
      "212/212 [==============================] - 19s 90ms/step - loss: 0.0655 - accuracy: 0.9776 - val_loss: 0.8130 - val_accuracy: 0.6947\n",
      "Epoch 45/100\n",
      "212/212 [==============================] - 19s 91ms/step - loss: 0.0757 - accuracy: 0.9729 - val_loss: 0.4022 - val_accuracy: 0.8105\n",
      "Epoch 46/100\n",
      "212/212 [==============================] - 19s 90ms/step - loss: 0.0716 - accuracy: 0.9741 - val_loss: 0.4732 - val_accuracy: 0.8105\n",
      "Epoch 47/100\n",
      "212/212 [==============================] - 19s 89ms/step - loss: 0.0435 - accuracy: 0.9870 - val_loss: 0.4118 - val_accuracy: 0.8421\n",
      "Epoch 48/100\n",
      "212/212 [==============================] - 19s 90ms/step - loss: 0.0489 - accuracy: 0.9870 - val_loss: 0.5211 - val_accuracy: 0.7474\n",
      "Epoch 49/100\n",
      "212/212 [==============================] - 19s 89ms/step - loss: 0.0400 - accuracy: 0.9882 - val_loss: 0.4717 - val_accuracy: 0.7895\n",
      "Epoch 50/100\n",
      "212/212 [==============================] - 19s 89ms/step - loss: 0.0823 - accuracy: 0.9670 - val_loss: 0.6057 - val_accuracy: 0.7789\n",
      "Epoch 51/100\n",
      "212/212 [==============================] - 19s 90ms/step - loss: 0.0754 - accuracy: 0.9670 - val_loss: 0.5551 - val_accuracy: 0.7684\n",
      "Epoch 52/100\n",
      "212/212 [==============================] - 19s 90ms/step - loss: 0.1012 - accuracy: 0.9634 - val_loss: 0.4240 - val_accuracy: 0.8211\n",
      "Epoch 53/100\n",
      "212/212 [==============================] - 19s 90ms/step - loss: 0.0820 - accuracy: 0.9658 - val_loss: 0.5693 - val_accuracy: 0.7684\n",
      "Epoch 54/100\n",
      "212/212 [==============================] - 19s 90ms/step - loss: 0.0587 - accuracy: 0.9752 - val_loss: 0.4845 - val_accuracy: 0.8105\n",
      "Epoch 55/100\n",
      "212/212 [==============================] - 19s 90ms/step - loss: 0.1086 - accuracy: 0.9528 - val_loss: 0.4505 - val_accuracy: 0.7789\n",
      "Epoch 56/100\n",
      "212/212 [==============================] - 19s 90ms/step - loss: 0.0579 - accuracy: 0.9764 - val_loss: 0.6164 - val_accuracy: 0.7579\n",
      "Epoch 57/100\n",
      "212/212 [==============================] - 19s 89ms/step - loss: 0.0663 - accuracy: 0.9764 - val_loss: 0.4815 - val_accuracy: 0.7789\n",
      "Epoch 58/100\n",
      "212/212 [==============================] - 19s 90ms/step - loss: 0.1012 - accuracy: 0.9481 - val_loss: 0.4054 - val_accuracy: 0.8421\n",
      "Epoch 59/100\n",
      "212/212 [==============================] - 19s 90ms/step - loss: 0.0765 - accuracy: 0.9705 - val_loss: 0.5149 - val_accuracy: 0.7684\n",
      "Epoch 60/100\n",
      "212/212 [==============================] - 19s 90ms/step - loss: 0.0407 - accuracy: 0.9870 - val_loss: 0.5145 - val_accuracy: 0.7684\n",
      "Epoch 61/100\n",
      "212/212 [==============================] - 19s 90ms/step - loss: 0.0739 - accuracy: 0.9693 - val_loss: 0.4251 - val_accuracy: 0.8105\n",
      "Epoch 62/100\n",
      "212/212 [==============================] - 19s 90ms/step - loss: 0.0618 - accuracy: 0.9811 - val_loss: 0.4196 - val_accuracy: 0.7895\n",
      "Epoch 63/100\n",
      "212/212 [==============================] - 19s 90ms/step - loss: 0.0460 - accuracy: 0.9870 - val_loss: 0.4901 - val_accuracy: 0.7789\n",
      "Epoch 64/100\n",
      "212/212 [==============================] - 19s 90ms/step - loss: 0.1036 - accuracy: 0.9493 - val_loss: 0.4309 - val_accuracy: 0.8211\n",
      "Epoch 65/100\n",
      "212/212 [==============================] - 19s 90ms/step - loss: 0.0735 - accuracy: 0.9682 - val_loss: 0.4645 - val_accuracy: 0.7895\n",
      "Epoch 66/100\n",
      "212/212 [==============================] - 19s 90ms/step - loss: 0.0745 - accuracy: 0.9752 - val_loss: 0.4384 - val_accuracy: 0.7895\n",
      "Epoch 67/100\n",
      "212/212 [==============================] - 19s 89ms/step - loss: 0.0410 - accuracy: 0.9870 - val_loss: 0.4408 - val_accuracy: 0.8211\n",
      "Epoch 68/100\n",
      "212/212 [==============================] - 19s 90ms/step - loss: 0.0567 - accuracy: 0.9776 - val_loss: 0.4406 - val_accuracy: 0.8421\n",
      "Epoch 69/100\n",
      "212/212 [==============================] - 19s 90ms/step - loss: 0.0668 - accuracy: 0.9705 - val_loss: 0.5164 - val_accuracy: 0.7579\n",
      "Epoch 70/100\n",
      "212/212 [==============================] - 19s 90ms/step - loss: 0.0700 - accuracy: 0.9670 - val_loss: 0.4132 - val_accuracy: 0.7684\n",
      "Epoch 71/100\n",
      "212/212 [==============================] - 19s 90ms/step - loss: 0.0820 - accuracy: 0.9658 - val_loss: 0.4597 - val_accuracy: 0.7789\n",
      "Epoch 72/100\n",
      "212/212 [==============================] - 19s 90ms/step - loss: 0.0597 - accuracy: 0.9788 - val_loss: 0.4295 - val_accuracy: 0.8105\n",
      "Epoch 73/100\n",
      "212/212 [==============================] - 19s 90ms/step - loss: 0.0586 - accuracy: 0.9764 - val_loss: 0.4266 - val_accuracy: 0.7684\n",
      "Epoch 74/100\n",
      "212/212 [==============================] - 19s 90ms/step - loss: 0.0726 - accuracy: 0.9634 - val_loss: 0.4593 - val_accuracy: 0.8316\n",
      "Epoch 75/100\n",
      "212/212 [==============================] - 19s 90ms/step - loss: 0.0504 - accuracy: 0.9847 - val_loss: 0.5371 - val_accuracy: 0.7789\n",
      "Epoch 76/100\n",
      "212/212 [==============================] - 19s 90ms/step - loss: 0.0580 - accuracy: 0.9776 - val_loss: 0.4102 - val_accuracy: 0.8421\n",
      "Epoch 77/100\n",
      "212/212 [==============================] - 19s 90ms/step - loss: 0.0600 - accuracy: 0.9741 - val_loss: 0.4642 - val_accuracy: 0.8000\n",
      "Epoch 78/100\n",
      "212/212 [==============================] - 19s 89ms/step - loss: 0.0460 - accuracy: 0.9823 - val_loss: 0.4097 - val_accuracy: 0.8000\n",
      "Epoch 79/100\n",
      "212/212 [==============================] - 19s 90ms/step - loss: 0.0579 - accuracy: 0.9776 - val_loss: 0.5092 - val_accuracy: 0.7895\n",
      "Epoch 80/100\n",
      "212/212 [==============================] - 19s 90ms/step - loss: 0.0496 - accuracy: 0.9800 - val_loss: 0.4245 - val_accuracy: 0.8316\n",
      "Epoch 81/100\n",
      "212/212 [==============================] - 19s 90ms/step - loss: 0.0776 - accuracy: 0.9658 - val_loss: 0.4079 - val_accuracy: 0.8316\n",
      "Epoch 82/100\n",
      "212/212 [==============================] - 19s 89ms/step - loss: 0.0557 - accuracy: 0.9764 - val_loss: 0.4062 - val_accuracy: 0.8316\n",
      "Epoch 83/100\n",
      "212/212 [==============================] - 19s 90ms/step - loss: 0.0600 - accuracy: 0.9752 - val_loss: 0.4276 - val_accuracy: 0.7684\n",
      "Epoch 84/100\n",
      "212/212 [==============================] - 19s 89ms/step - loss: 0.0663 - accuracy: 0.9741 - val_loss: 0.4314 - val_accuracy: 0.8105\n",
      "Epoch 85/100\n",
      "212/212 [==============================] - 19s 90ms/step - loss: 0.0544 - accuracy: 0.9847 - val_loss: 0.4483 - val_accuracy: 0.8211\n",
      "Epoch 86/100\n",
      "212/212 [==============================] - 19s 90ms/step - loss: 0.0503 - accuracy: 0.9858 - val_loss: 0.4215 - val_accuracy: 0.8000\n",
      "Epoch 87/100\n",
      "212/212 [==============================] - 19s 90ms/step - loss: 0.0397 - accuracy: 0.9858 - val_loss: 0.5514 - val_accuracy: 0.7684\n",
      "Epoch 88/100\n",
      "212/212 [==============================] - 19s 90ms/step - loss: 0.0608 - accuracy: 0.9741 - val_loss: 0.4951 - val_accuracy: 0.7684\n",
      "Epoch 89/100\n",
      "212/212 [==============================] - 19s 90ms/step - loss: 0.0688 - accuracy: 0.9741 - val_loss: 0.4339 - val_accuracy: 0.8211\n",
      "Epoch 90/100\n",
      "212/212 [==============================] - 19s 90ms/step - loss: 0.0464 - accuracy: 0.9858 - val_loss: 0.4663 - val_accuracy: 0.8211\n",
      "Epoch 91/100\n",
      "212/212 [==============================] - 19s 90ms/step - loss: 0.0684 - accuracy: 0.9705 - val_loss: 0.4363 - val_accuracy: 0.8316\n",
      "Epoch 92/100\n",
      "212/212 [==============================] - 19s 90ms/step - loss: 0.0636 - accuracy: 0.9705 - val_loss: 0.4961 - val_accuracy: 0.7789\n",
      "Epoch 93/100\n",
      "212/212 [==============================] - 20s 93ms/step - loss: 0.0512 - accuracy: 0.9823 - val_loss: 0.4389 - val_accuracy: 0.7895\n",
      "Epoch 94/100\n",
      "212/212 [==============================] - 20s 93ms/step - loss: 0.0652 - accuracy: 0.9741 - val_loss: 0.4374 - val_accuracy: 0.8000\n",
      "Epoch 95/100\n",
      "212/212 [==============================] - 20s 94ms/step - loss: 0.0714 - accuracy: 0.9705 - val_loss: 0.4383 - val_accuracy: 0.7684\n",
      "Epoch 96/100\n",
      "212/212 [==============================] - 20s 93ms/step - loss: 0.0349 - accuracy: 0.9882 - val_loss: 0.4273 - val_accuracy: 0.8211\n",
      "Epoch 97/100\n",
      "212/212 [==============================] - 20s 93ms/step - loss: 0.0627 - accuracy: 0.9741 - val_loss: 0.4173 - val_accuracy: 0.8316\n",
      "Epoch 98/100\n",
      "212/212 [==============================] - 20s 92ms/step - loss: 0.0380 - accuracy: 0.9882 - val_loss: 0.4588 - val_accuracy: 0.7789\n",
      "Epoch 99/100\n",
      "212/212 [==============================] - 19s 92ms/step - loss: 0.0456 - accuracy: 0.9823 - val_loss: 0.4222 - val_accuracy: 0.8316\n",
      "Epoch 100/100\n",
      "212/212 [==============================] - 19s 92ms/step - loss: 0.0652 - accuracy: 0.9705 - val_loss: 0.4486 - val_accuracy: 0.7789\n",
      "Score for fold 1: loss of 0.4486108124256134; accuracy of 77.89473533630371%\n",
      "(848,) (95,)\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 2 ...\n",
      "Epoch 1/100\n",
      "212/212 [==============================] - 24s 93ms/step - loss: 0.5928 - accuracy: 0.5000 - val_loss: 0.8587 - val_accuracy: 0.3474\n",
      "Epoch 2/100\n",
      "212/212 [==============================] - 19s 89ms/step - loss: 0.4834 - accuracy: 0.6403 - val_loss: 1.0087 - val_accuracy: 0.3684\n",
      "Epoch 3/100\n",
      "212/212 [==============================] - 19s 90ms/step - loss: 0.4482 - accuracy: 0.6604 - val_loss: 70.9743 - val_accuracy: 0.2737\n",
      "Epoch 4/100\n",
      "212/212 [==============================] - 19s 90ms/step - loss: 0.4072 - accuracy: 0.7217 - val_loss: 1.7752 - val_accuracy: 0.3895\n",
      "Epoch 5/100\n",
      "212/212 [==============================] - 19s 90ms/step - loss: 0.4035 - accuracy: 0.7241 - val_loss: 0.4049 - val_accuracy: 0.7263\n",
      "Epoch 6/100\n",
      "212/212 [==============================] - 19s 90ms/step - loss: 0.3564 - accuracy: 0.7736 - val_loss: 4.3042 - val_accuracy: 0.3368\n",
      "Epoch 7/100\n",
      "212/212 [==============================] - 19s 90ms/step - loss: 0.3358 - accuracy: 0.7842 - val_loss: 1.3818 - val_accuracy: 0.3789\n",
      "Epoch 8/100\n",
      "212/212 [==============================] - 19s 90ms/step - loss: 0.2949 - accuracy: 0.8090 - val_loss: 3.7374 - val_accuracy: 0.3474\n",
      "Epoch 9/100\n",
      "212/212 [==============================] - 19s 90ms/step - loss: 0.2785 - accuracy: 0.8432 - val_loss: 1.2071 - val_accuracy: 0.4842\n",
      "Epoch 10/100\n",
      "212/212 [==============================] - 19s 90ms/step - loss: 0.2303 - accuracy: 0.8726 - val_loss: 1.4065 - val_accuracy: 0.4421\n",
      "Epoch 11/100\n",
      "212/212 [==============================] - 19s 90ms/step - loss: 0.2391 - accuracy: 0.8679 - val_loss: 2.0858 - val_accuracy: 0.3263\n",
      "Epoch 12/100\n",
      "212/212 [==============================] - 19s 90ms/step - loss: 0.2100 - accuracy: 0.8821 - val_loss: 1.2253 - val_accuracy: 0.5895\n",
      "Epoch 13/100\n",
      "212/212 [==============================] - 19s 90ms/step - loss: 0.2053 - accuracy: 0.8962 - val_loss: 1.3100 - val_accuracy: 0.4000\n",
      "Epoch 14/100\n",
      "212/212 [==============================] - 19s 90ms/step - loss: 0.1875 - accuracy: 0.9021 - val_loss: 0.6423 - val_accuracy: 0.6105\n",
      "Epoch 15/100\n",
      "212/212 [==============================] - 19s 90ms/step - loss: 0.1732 - accuracy: 0.9104 - val_loss: 0.2717 - val_accuracy: 0.7789\n",
      "Epoch 16/100\n",
      "212/212 [==============================] - 19s 90ms/step - loss: 0.1645 - accuracy: 0.9210 - val_loss: 0.5723 - val_accuracy: 0.6842\n",
      "Epoch 17/100\n",
      "212/212 [==============================] - 19s 90ms/step - loss: 0.1529 - accuracy: 0.9281 - val_loss: 2.4313 - val_accuracy: 0.3579\n",
      "Epoch 18/100\n",
      "212/212 [==============================] - 19s 90ms/step - loss: 0.1459 - accuracy: 0.9410 - val_loss: 1.2098 - val_accuracy: 0.3789\n",
      "Epoch 19/100\n",
      "212/212 [==============================] - 19s 90ms/step - loss: 0.1531 - accuracy: 0.9269 - val_loss: 0.3839 - val_accuracy: 0.7474\n",
      "Epoch 20/100\n",
      "212/212 [==============================] - 19s 90ms/step - loss: 0.1699 - accuracy: 0.9222 - val_loss: 1.0793 - val_accuracy: 0.4737\n",
      "Epoch 21/100\n",
      "212/212 [==============================] - 19s 90ms/step - loss: 0.1470 - accuracy: 0.9269 - val_loss: 0.9231 - val_accuracy: 0.5474\n",
      "Epoch 22/100\n",
      "212/212 [==============================] - 19s 90ms/step - loss: 0.1195 - accuracy: 0.9517 - val_loss: 1.2396 - val_accuracy: 0.8105\n",
      "Epoch 23/100\n",
      "212/212 [==============================] - 19s 90ms/step - loss: 0.1357 - accuracy: 0.9340 - val_loss: 2.1611 - val_accuracy: 0.3474\n",
      "Epoch 24/100\n",
      "212/212 [==============================] - 19s 90ms/step - loss: 0.1009 - accuracy: 0.9623 - val_loss: 0.2212 - val_accuracy: 0.8421\n",
      "Epoch 25/100\n",
      "212/212 [==============================] - 19s 90ms/step - loss: 0.1016 - accuracy: 0.9634 - val_loss: 0.8915 - val_accuracy: 0.5684\n",
      "Epoch 26/100\n",
      "212/212 [==============================] - 19s 90ms/step - loss: 0.1032 - accuracy: 0.9587 - val_loss: 0.5351 - val_accuracy: 0.7368\n",
      "Epoch 27/100\n",
      "212/212 [==============================] - 19s 90ms/step - loss: 0.1175 - accuracy: 0.9540 - val_loss: 0.2738 - val_accuracy: 0.8632\n",
      "Epoch 28/100\n",
      "212/212 [==============================] - 19s 90ms/step - loss: 0.0881 - accuracy: 0.9658 - val_loss: 0.2746 - val_accuracy: 0.8526\n",
      "Epoch 29/100\n",
      "212/212 [==============================] - 19s 90ms/step - loss: 0.0839 - accuracy: 0.9776 - val_loss: 2.2085 - val_accuracy: 0.6737\n",
      "Epoch 30/100\n",
      "212/212 [==============================] - 19s 90ms/step - loss: 0.0743 - accuracy: 0.9764 - val_loss: 0.5616 - val_accuracy: 0.7474\n",
      "Epoch 31/100\n",
      "212/212 [==============================] - 19s 90ms/step - loss: 0.0743 - accuracy: 0.9693 - val_loss: 0.3952 - val_accuracy: 0.7789\n",
      "Epoch 32/100\n",
      "212/212 [==============================] - 19s 90ms/step - loss: 0.0889 - accuracy: 0.9646 - val_loss: 0.2494 - val_accuracy: 0.8632\n",
      "Epoch 33/100\n",
      "212/212 [==============================] - 19s 90ms/step - loss: 0.0735 - accuracy: 0.9752 - val_loss: 0.5798 - val_accuracy: 0.7158\n",
      "Epoch 34/100\n",
      "212/212 [==============================] - 19s 89ms/step - loss: 0.0727 - accuracy: 0.9729 - val_loss: 1.4680 - val_accuracy: 0.5368\n",
      "Epoch 35/100\n",
      "212/212 [==============================] - 19s 89ms/step - loss: 0.0940 - accuracy: 0.9611 - val_loss: 1.1284 - val_accuracy: 0.6526\n",
      "Epoch 36/100\n",
      "212/212 [==============================] - 19s 90ms/step - loss: 0.0775 - accuracy: 0.9717 - val_loss: 0.3051 - val_accuracy: 0.8842\n",
      "Epoch 37/100\n",
      "212/212 [==============================] - 19s 90ms/step - loss: 0.1019 - accuracy: 0.9587 - val_loss: 0.2354 - val_accuracy: 0.8632\n",
      "Epoch 38/100\n",
      "212/212 [==============================] - 19s 89ms/step - loss: 0.0727 - accuracy: 0.9764 - val_loss: 0.4887 - val_accuracy: 0.7579\n",
      "Epoch 39/100\n",
      "212/212 [==============================] - 19s 90ms/step - loss: 0.0688 - accuracy: 0.9752 - val_loss: 0.2640 - val_accuracy: 0.8842\n",
      "Epoch 40/100\n",
      "212/212 [==============================] - 19s 90ms/step - loss: 0.0672 - accuracy: 0.9752 - val_loss: 0.5906 - val_accuracy: 0.7368\n",
      "Epoch 41/100\n",
      "212/212 [==============================] - 19s 90ms/step - loss: 0.0638 - accuracy: 0.9752 - val_loss: 0.2857 - val_accuracy: 0.8526\n",
      "Epoch 42/100\n",
      "212/212 [==============================] - 19s 90ms/step - loss: 0.0589 - accuracy: 0.9847 - val_loss: 0.2999 - val_accuracy: 0.8316\n",
      "Epoch 43/100\n",
      "212/212 [==============================] - 19s 89ms/step - loss: 0.0805 - accuracy: 0.9729 - val_loss: 0.2597 - val_accuracy: 0.8842\n",
      "Epoch 44/100\n",
      "212/212 [==============================] - 19s 90ms/step - loss: 0.0334 - accuracy: 0.9917 - val_loss: 0.3935 - val_accuracy: 0.7895\n",
      "Epoch 45/100\n",
      "212/212 [==============================] - 19s 89ms/step - loss: 0.0502 - accuracy: 0.9823 - val_loss: 0.3799 - val_accuracy: 0.8316\n",
      "Epoch 46/100\n",
      "212/212 [==============================] - 19s 89ms/step - loss: 0.1023 - accuracy: 0.9575 - val_loss: 0.4498 - val_accuracy: 0.7789\n",
      "Epoch 47/100\n",
      "212/212 [==============================] - 19s 90ms/step - loss: 0.0564 - accuracy: 0.9776 - val_loss: 0.2714 - val_accuracy: 0.8737\n",
      "Epoch 48/100\n",
      "212/212 [==============================] - 19s 90ms/step - loss: 0.0599 - accuracy: 0.9788 - val_loss: 0.3088 - val_accuracy: 0.8421\n",
      "Epoch 49/100\n",
      "212/212 [==============================] - 19s 90ms/step - loss: 0.0623 - accuracy: 0.9788 - val_loss: 0.2329 - val_accuracy: 0.8842\n",
      "Epoch 50/100\n",
      "212/212 [==============================] - 19s 90ms/step - loss: 0.0596 - accuracy: 0.9776 - val_loss: 0.2713 - val_accuracy: 0.8316\n",
      "Epoch 51/100\n",
      "212/212 [==============================] - 19s 90ms/step - loss: 0.0935 - accuracy: 0.9564 - val_loss: 0.4648 - val_accuracy: 0.7474\n",
      "Epoch 52/100\n",
      "212/212 [==============================] - 19s 90ms/step - loss: 0.0522 - accuracy: 0.9835 - val_loss: 0.2275 - val_accuracy: 0.8842\n",
      "Epoch 53/100\n",
      "212/212 [==============================] - 19s 90ms/step - loss: 0.0411 - accuracy: 0.9929 - val_loss: 0.3176 - val_accuracy: 0.8316\n",
      "Epoch 54/100\n",
      "212/212 [==============================] - 19s 90ms/step - loss: 0.0763 - accuracy: 0.9741 - val_loss: 0.2224 - val_accuracy: 0.8632\n",
      "Epoch 55/100\n",
      "212/212 [==============================] - 19s 90ms/step - loss: 0.0810 - accuracy: 0.9705 - val_loss: 0.2724 - val_accuracy: 0.8632\n",
      "Epoch 56/100\n",
      "212/212 [==============================] - 19s 90ms/step - loss: 0.0573 - accuracy: 0.9788 - val_loss: 0.3109 - val_accuracy: 0.8737\n",
      "Epoch 57/100\n",
      "212/212 [==============================] - 19s 90ms/step - loss: 0.0603 - accuracy: 0.9764 - val_loss: 0.2481 - val_accuracy: 0.8632\n",
      "Epoch 58/100\n",
      "212/212 [==============================] - 19s 90ms/step - loss: 0.0532 - accuracy: 0.9811 - val_loss: 0.2387 - val_accuracy: 0.8842\n",
      "Epoch 59/100\n",
      "212/212 [==============================] - 19s 90ms/step - loss: 0.0707 - accuracy: 0.9764 - val_loss: 0.2654 - val_accuracy: 0.8737\n",
      "Epoch 60/100\n",
      "212/212 [==============================] - 19s 90ms/step - loss: 0.0366 - accuracy: 0.9929 - val_loss: 0.2342 - val_accuracy: 0.8632\n",
      "Epoch 61/100\n",
      "212/212 [==============================] - 19s 90ms/step - loss: 0.0732 - accuracy: 0.9646 - val_loss: 0.2458 - val_accuracy: 0.8737\n",
      "Epoch 62/100\n",
      "212/212 [==============================] - 19s 90ms/step - loss: 0.0486 - accuracy: 0.9835 - val_loss: 0.2405 - val_accuracy: 0.8737\n",
      "Epoch 63/100\n",
      "212/212 [==============================] - 19s 90ms/step - loss: 0.0850 - accuracy: 0.9587 - val_loss: 0.2610 - val_accuracy: 0.8526\n",
      "Epoch 64/100\n",
      "212/212 [==============================] - 19s 90ms/step - loss: 0.0826 - accuracy: 0.9682 - val_loss: 0.2317 - val_accuracy: 0.8947\n",
      "Epoch 65/100\n",
      "212/212 [==============================] - 19s 90ms/step - loss: 0.0697 - accuracy: 0.9741 - val_loss: 0.2794 - val_accuracy: 0.8526\n",
      "Epoch 66/100\n",
      "212/212 [==============================] - 19s 90ms/step - loss: 0.0632 - accuracy: 0.9729 - val_loss: 0.2847 - val_accuracy: 0.8421\n",
      "Epoch 67/100\n",
      "212/212 [==============================] - 19s 90ms/step - loss: 0.0702 - accuracy: 0.9717 - val_loss: 0.2244 - val_accuracy: 0.8947\n",
      "Epoch 68/100\n",
      "212/212 [==============================] - 19s 90ms/step - loss: 0.0599 - accuracy: 0.9741 - val_loss: 0.2282 - val_accuracy: 0.8737\n",
      "Epoch 69/100\n",
      "212/212 [==============================] - 19s 90ms/step - loss: 0.0634 - accuracy: 0.9776 - val_loss: 0.7763 - val_accuracy: 0.7158\n",
      "Epoch 70/100\n",
      "212/212 [==============================] - 19s 90ms/step - loss: 0.0431 - accuracy: 0.9870 - val_loss: 0.5057 - val_accuracy: 0.7895\n",
      "Epoch 71/100\n",
      "212/212 [==============================] - 19s 90ms/step - loss: 0.0664 - accuracy: 0.9741 - val_loss: 0.2920 - val_accuracy: 0.8526\n",
      "Epoch 72/100\n",
      "212/212 [==============================] - 19s 90ms/step - loss: 0.0529 - accuracy: 0.9800 - val_loss: 0.2461 - val_accuracy: 0.8737\n",
      "Epoch 73/100\n",
      "212/212 [==============================] - 19s 90ms/step - loss: 0.0568 - accuracy: 0.9764 - val_loss: 0.2542 - val_accuracy: 0.8737\n",
      "Epoch 74/100\n",
      "212/212 [==============================] - 19s 90ms/step - loss: 0.0399 - accuracy: 0.9882 - val_loss: 0.2941 - val_accuracy: 0.8526\n",
      "Epoch 75/100\n",
      "212/212 [==============================] - 19s 90ms/step - loss: 0.0595 - accuracy: 0.9741 - val_loss: 0.2549 - val_accuracy: 0.8632\n",
      "Epoch 76/100\n",
      "212/212 [==============================] - 19s 90ms/step - loss: 0.0583 - accuracy: 0.9788 - val_loss: 0.2327 - val_accuracy: 0.8526\n",
      "Epoch 77/100\n",
      "212/212 [==============================] - 19s 90ms/step - loss: 0.0527 - accuracy: 0.9811 - val_loss: 0.2298 - val_accuracy: 0.8632\n",
      "Epoch 78/100\n",
      "212/212 [==============================] - 19s 90ms/step - loss: 0.0609 - accuracy: 0.9764 - val_loss: 0.2513 - val_accuracy: 0.8632\n",
      "Epoch 79/100\n",
      "212/212 [==============================] - 19s 90ms/step - loss: 0.0637 - accuracy: 0.9741 - val_loss: 0.3148 - val_accuracy: 0.8211\n",
      "Epoch 80/100\n",
      "212/212 [==============================] - 19s 90ms/step - loss: 0.0442 - accuracy: 0.9847 - val_loss: 0.2774 - val_accuracy: 0.8211\n",
      "Epoch 81/100\n",
      "212/212 [==============================] - 19s 90ms/step - loss: 0.0443 - accuracy: 0.9847 - val_loss: 0.2128 - val_accuracy: 0.8737\n",
      "Epoch 82/100\n",
      "212/212 [==============================] - 19s 90ms/step - loss: 0.0498 - accuracy: 0.9823 - val_loss: 0.2247 - val_accuracy: 0.8737\n",
      "Epoch 83/100\n",
      "212/212 [==============================] - 19s 90ms/step - loss: 0.0344 - accuracy: 0.9870 - val_loss: 0.2074 - val_accuracy: 0.8842\n",
      "Epoch 84/100\n",
      "212/212 [==============================] - 19s 90ms/step - loss: 0.0548 - accuracy: 0.9800 - val_loss: 0.4143 - val_accuracy: 0.8211\n",
      "Epoch 85/100\n",
      "212/212 [==============================] - 19s 90ms/step - loss: 0.0629 - accuracy: 0.9705 - val_loss: 0.2377 - val_accuracy: 0.8737\n",
      "Epoch 86/100\n",
      "212/212 [==============================] - 19s 90ms/step - loss: 0.0665 - accuracy: 0.9788 - val_loss: 0.2124 - val_accuracy: 0.8632\n",
      "Epoch 87/100\n",
      "212/212 [==============================] - 19s 90ms/step - loss: 0.0441 - accuracy: 0.9847 - val_loss: 0.2096 - val_accuracy: 0.8947\n",
      "Epoch 88/100\n",
      "212/212 [==============================] - 19s 90ms/step - loss: 0.0424 - accuracy: 0.9823 - val_loss: 0.2220 - val_accuracy: 0.8842\n",
      "Epoch 89/100\n",
      "212/212 [==============================] - 19s 90ms/step - loss: 0.0772 - accuracy: 0.9634 - val_loss: 0.2603 - val_accuracy: 0.8632\n",
      "Epoch 90/100\n",
      "212/212 [==============================] - 19s 90ms/step - loss: 0.0698 - accuracy: 0.9682 - val_loss: 0.2822 - val_accuracy: 0.8421\n",
      "Epoch 91/100\n",
      "212/212 [==============================] - 19s 90ms/step - loss: 0.0514 - accuracy: 0.9788 - val_loss: 0.2254 - val_accuracy: 0.8947\n",
      "Epoch 92/100\n",
      "212/212 [==============================] - 19s 90ms/step - loss: 0.0808 - accuracy: 0.9670 - val_loss: 0.4137 - val_accuracy: 0.8316\n",
      "Epoch 93/100\n",
      "212/212 [==============================] - 19s 90ms/step - loss: 0.0804 - accuracy: 0.9611 - val_loss: 0.4566 - val_accuracy: 0.8211\n",
      "Epoch 94/100\n",
      "212/212 [==============================] - 19s 90ms/step - loss: 0.0488 - accuracy: 0.9823 - val_loss: 0.2710 - val_accuracy: 0.8737\n",
      "Epoch 95/100\n",
      "212/212 [==============================] - 19s 90ms/step - loss: 0.0562 - accuracy: 0.9764 - val_loss: 0.2838 - val_accuracy: 0.8737\n",
      "Epoch 96/100\n",
      "212/212 [==============================] - 19s 90ms/step - loss: 0.0679 - accuracy: 0.9658 - val_loss: 0.2413 - val_accuracy: 0.8842\n",
      "Epoch 97/100\n",
      "212/212 [==============================] - 19s 90ms/step - loss: 0.0420 - accuracy: 0.9835 - val_loss: 0.2282 - val_accuracy: 0.8632\n",
      "Epoch 98/100\n",
      "212/212 [==============================] - 19s 90ms/step - loss: 0.0498 - accuracy: 0.9788 - val_loss: 0.2991 - val_accuracy: 0.8421\n",
      "Epoch 99/100\n",
      "212/212 [==============================] - 19s 90ms/step - loss: 0.0623 - accuracy: 0.9729 - val_loss: 0.2341 - val_accuracy: 0.8737\n",
      "Epoch 100/100\n",
      "212/212 [==============================] - 19s 90ms/step - loss: 0.0586 - accuracy: 0.9788 - val_loss: 0.2748 - val_accuracy: 0.8421\n",
      "Score for fold 2: loss of 0.2747935950756073; accuracy of 84.21052694320679%\n",
      "(848,) (95,)\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 3 ...\n",
      "Epoch 1/100\n",
      "212/212 [==============================] - 24s 92ms/step - loss: 0.5643 - accuracy: 0.5165 - val_loss: 1.8485 - val_accuracy: 0.3474\n",
      "Epoch 2/100\n",
      "212/212 [==============================] - 19s 89ms/step - loss: 0.4815 - accuracy: 0.6002 - val_loss: 0.7183 - val_accuracy: 0.3263\n",
      "Epoch 3/100\n",
      "212/212 [==============================] - 19s 89ms/step - loss: 0.4436 - accuracy: 0.6604 - val_loss: 0.7908 - val_accuracy: 0.3579\n",
      "Epoch 4/100\n",
      "212/212 [==============================] - 19s 89ms/step - loss: 0.4134 - accuracy: 0.6792 - val_loss: 0.4695 - val_accuracy: 0.6526\n",
      "Epoch 5/100\n",
      "212/212 [==============================] - 19s 89ms/step - loss: 0.4073 - accuracy: 0.6769 - val_loss: 0.4745 - val_accuracy: 0.5684\n",
      "Epoch 6/100\n",
      "212/212 [==============================] - 19s 89ms/step - loss: 0.3663 - accuracy: 0.7559 - val_loss: 1.5206 - val_accuracy: 0.3579\n",
      "Epoch 7/100\n",
      "212/212 [==============================] - 19s 89ms/step - loss: 0.3437 - accuracy: 0.7736 - val_loss: 0.4086 - val_accuracy: 0.7158\n",
      "Epoch 8/100\n",
      "212/212 [==============================] - 19s 89ms/step - loss: 0.2845 - accuracy: 0.8101 - val_loss: 0.8956 - val_accuracy: 0.6947\n",
      "Epoch 9/100\n",
      "212/212 [==============================] - 19s 89ms/step - loss: 0.3127 - accuracy: 0.8078 - val_loss: 2.7414 - val_accuracy: 0.5263\n",
      "Epoch 10/100\n",
      "212/212 [==============================] - 19s 89ms/step - loss: 0.2441 - accuracy: 0.8762 - val_loss: 2.4224 - val_accuracy: 0.4947\n",
      "Epoch 11/100\n",
      "212/212 [==============================] - 19s 89ms/step - loss: 0.2409 - accuracy: 0.8679 - val_loss: 1.1031 - val_accuracy: 0.6632\n",
      "Epoch 12/100\n",
      "212/212 [==============================] - 19s 89ms/step - loss: 0.2301 - accuracy: 0.8715 - val_loss: 0.4080 - val_accuracy: 0.7684\n",
      "Epoch 13/100\n",
      "212/212 [==============================] - 19s 89ms/step - loss: 0.2201 - accuracy: 0.8821 - val_loss: 1.3686 - val_accuracy: 0.6211\n",
      "Epoch 14/100\n",
      "212/212 [==============================] - 19s 89ms/step - loss: 0.1751 - accuracy: 0.9092 - val_loss: 0.7672 - val_accuracy: 0.6947\n",
      "Epoch 15/100\n",
      "212/212 [==============================] - 19s 89ms/step - loss: 0.1830 - accuracy: 0.9021 - val_loss: 0.2635 - val_accuracy: 0.8421\n",
      "Epoch 16/100\n",
      "212/212 [==============================] - 19s 89ms/step - loss: 0.1745 - accuracy: 0.9210 - val_loss: 0.2935 - val_accuracy: 0.8526\n",
      "Epoch 17/100\n",
      "212/212 [==============================] - 19s 89ms/step - loss: 0.1533 - accuracy: 0.9328 - val_loss: 0.4370 - val_accuracy: 0.7789\n",
      "Epoch 18/100\n",
      "212/212 [==============================] - 19s 89ms/step - loss: 0.1332 - accuracy: 0.9422 - val_loss: 1.9940 - val_accuracy: 0.5684\n",
      "Epoch 19/100\n",
      "212/212 [==============================] - 19s 89ms/step - loss: 0.1749 - accuracy: 0.9116 - val_loss: 0.6707 - val_accuracy: 0.6842\n",
      "Epoch 20/100\n",
      "212/212 [==============================] - 19s 89ms/step - loss: 0.1332 - accuracy: 0.9540 - val_loss: 0.3513 - val_accuracy: 0.8316\n",
      "Epoch 21/100\n",
      "212/212 [==============================] - 19s 89ms/step - loss: 0.1082 - accuracy: 0.9540 - val_loss: 0.7583 - val_accuracy: 0.7263\n",
      "Epoch 22/100\n",
      "212/212 [==============================] - 19s 89ms/step - loss: 0.1173 - accuracy: 0.9434 - val_loss: 2.2085 - val_accuracy: 0.7474\n",
      "Epoch 23/100\n",
      "212/212 [==============================] - 19s 89ms/step - loss: 0.1139 - accuracy: 0.9528 - val_loss: 0.2732 - val_accuracy: 0.8526\n",
      "Epoch 24/100\n",
      "212/212 [==============================] - 19s 89ms/step - loss: 0.1205 - accuracy: 0.9493 - val_loss: 0.5052 - val_accuracy: 0.8000\n",
      "Epoch 25/100\n",
      "212/212 [==============================] - 19s 89ms/step - loss: 0.0778 - accuracy: 0.9788 - val_loss: 0.8137 - val_accuracy: 0.7368\n",
      "Epoch 26/100\n",
      "212/212 [==============================] - 19s 89ms/step - loss: 0.0915 - accuracy: 0.9634 - val_loss: 0.3302 - val_accuracy: 0.8632\n",
      "Epoch 27/100\n",
      "212/212 [==============================] - 19s 89ms/step - loss: 0.1146 - accuracy: 0.9481 - val_loss: 0.9454 - val_accuracy: 0.7053\n",
      "Epoch 28/100\n",
      "212/212 [==============================] - 19s 89ms/step - loss: 0.0903 - accuracy: 0.9599 - val_loss: 0.5096 - val_accuracy: 0.7368\n",
      "Epoch 29/100\n",
      "212/212 [==============================] - 19s 89ms/step - loss: 0.0974 - accuracy: 0.9587 - val_loss: 0.3267 - val_accuracy: 0.8421\n",
      "Epoch 30/100\n",
      "212/212 [==============================] - 19s 89ms/step - loss: 0.1252 - accuracy: 0.9493 - val_loss: 0.3442 - val_accuracy: 0.8000\n",
      "Epoch 31/100\n",
      "212/212 [==============================] - 19s 89ms/step - loss: 0.0742 - accuracy: 0.9776 - val_loss: 0.8912 - val_accuracy: 0.7158\n",
      "Epoch 32/100\n",
      "212/212 [==============================] - 19s 89ms/step - loss: 0.0487 - accuracy: 0.9894 - val_loss: 0.5904 - val_accuracy: 0.8105\n",
      "Epoch 33/100\n",
      "212/212 [==============================] - 19s 89ms/step - loss: 0.0899 - accuracy: 0.9705 - val_loss: 0.4947 - val_accuracy: 0.8000\n",
      "Epoch 34/100\n",
      "212/212 [==============================] - 19s 89ms/step - loss: 0.0836 - accuracy: 0.9752 - val_loss: 1.3372 - val_accuracy: 0.6737\n",
      "Epoch 35/100\n",
      "212/212 [==============================] - 19s 89ms/step - loss: 0.1000 - accuracy: 0.9575 - val_loss: 0.4390 - val_accuracy: 0.8000\n",
      "Epoch 36/100\n",
      "212/212 [==============================] - 19s 90ms/step - loss: 0.0610 - accuracy: 0.9776 - val_loss: 0.5279 - val_accuracy: 0.7895\n",
      "Epoch 37/100\n",
      "212/212 [==============================] - 19s 89ms/step - loss: 0.0781 - accuracy: 0.9682 - val_loss: 0.4407 - val_accuracy: 0.7789\n",
      "Epoch 38/100\n",
      "212/212 [==============================] - 19s 89ms/step - loss: 0.0706 - accuracy: 0.9717 - val_loss: 0.4812 - val_accuracy: 0.8105\n",
      "Epoch 39/100\n",
      "212/212 [==============================] - 19s 89ms/step - loss: 0.0605 - accuracy: 0.9835 - val_loss: 0.4569 - val_accuracy: 0.8211\n",
      "Epoch 40/100\n",
      "212/212 [==============================] - 19s 89ms/step - loss: 0.0562 - accuracy: 0.9800 - val_loss: 1.0713 - val_accuracy: 0.7053\n",
      "Epoch 41/100\n",
      "212/212 [==============================] - 19s 89ms/step - loss: 0.0725 - accuracy: 0.9752 - val_loss: 0.4010 - val_accuracy: 0.8211\n",
      "Epoch 42/100\n",
      "212/212 [==============================] - 19s 89ms/step - loss: 0.0766 - accuracy: 0.9729 - val_loss: 0.6878 - val_accuracy: 0.7789\n",
      "Epoch 43/100\n",
      "212/212 [==============================] - 19s 90ms/step - loss: 0.0574 - accuracy: 0.9788 - val_loss: 0.4483 - val_accuracy: 0.8211\n",
      "Epoch 44/100\n",
      "212/212 [==============================] - 19s 89ms/step - loss: 0.0959 - accuracy: 0.9599 - val_loss: 0.3562 - val_accuracy: 0.8000\n",
      "Epoch 45/100\n",
      "212/212 [==============================] - 19s 89ms/step - loss: 0.0766 - accuracy: 0.9705 - val_loss: 0.6716 - val_accuracy: 0.7684\n",
      "Epoch 46/100\n",
      "212/212 [==============================] - 19s 90ms/step - loss: 0.0761 - accuracy: 0.9646 - val_loss: 0.5088 - val_accuracy: 0.7895\n",
      "Epoch 47/100\n",
      "212/212 [==============================] - 19s 90ms/step - loss: 0.0682 - accuracy: 0.9729 - val_loss: 0.5907 - val_accuracy: 0.8000\n",
      "Epoch 48/100\n",
      "212/212 [==============================] - 19s 89ms/step - loss: 0.0618 - accuracy: 0.9788 - val_loss: 0.6393 - val_accuracy: 0.7789\n",
      "Epoch 49/100\n",
      "212/212 [==============================] - 19s 90ms/step - loss: 0.0660 - accuracy: 0.9764 - val_loss: 0.3733 - val_accuracy: 0.8105\n",
      "Epoch 50/100\n",
      "212/212 [==============================] - 19s 90ms/step - loss: 0.0749 - accuracy: 0.9682 - val_loss: 0.5702 - val_accuracy: 0.7895\n",
      "Epoch 51/100\n",
      "212/212 [==============================] - 19s 89ms/step - loss: 0.0855 - accuracy: 0.9693 - val_loss: 0.4423 - val_accuracy: 0.8316\n",
      "Epoch 52/100\n",
      "212/212 [==============================] - 19s 90ms/step - loss: 0.0596 - accuracy: 0.9764 - val_loss: 0.4026 - val_accuracy: 0.8105\n",
      "Epoch 53/100\n",
      "212/212 [==============================] - 19s 90ms/step - loss: 0.0570 - accuracy: 0.9823 - val_loss: 0.3550 - val_accuracy: 0.8211\n",
      "Epoch 54/100\n",
      "212/212 [==============================] - 19s 90ms/step - loss: 0.0417 - accuracy: 0.9858 - val_loss: 0.3946 - val_accuracy: 0.8211\n",
      "Epoch 55/100\n",
      "212/212 [==============================] - 19s 90ms/step - loss: 0.0651 - accuracy: 0.9741 - val_loss: 0.3559 - val_accuracy: 0.8316\n",
      "Epoch 56/100\n",
      "212/212 [==============================] - 19s 90ms/step - loss: 0.0755 - accuracy: 0.9752 - val_loss: 0.3891 - val_accuracy: 0.8000\n",
      "Epoch 57/100\n",
      "212/212 [==============================] - 19s 89ms/step - loss: 0.0565 - accuracy: 0.9788 - val_loss: 0.3648 - val_accuracy: 0.8211\n",
      "Epoch 58/100\n",
      "212/212 [==============================] - 19s 90ms/step - loss: 0.0836 - accuracy: 0.9623 - val_loss: 0.4160 - val_accuracy: 0.7895\n",
      "Epoch 59/100\n",
      "212/212 [==============================] - 19s 90ms/step - loss: 0.0619 - accuracy: 0.9788 - val_loss: 0.4394 - val_accuracy: 0.8000\n",
      "Epoch 60/100\n",
      "212/212 [==============================] - 19s 90ms/step - loss: 0.0651 - accuracy: 0.9705 - val_loss: 0.4658 - val_accuracy: 0.8000\n",
      "Epoch 61/100\n",
      "212/212 [==============================] - 19s 90ms/step - loss: 0.0622 - accuracy: 0.9741 - val_loss: 0.5011 - val_accuracy: 0.8105\n",
      "Epoch 62/100\n",
      "212/212 [==============================] - 19s 90ms/step - loss: 0.0457 - accuracy: 0.9894 - val_loss: 0.4223 - val_accuracy: 0.8105\n",
      "Epoch 63/100\n",
      "212/212 [==============================] - 19s 90ms/step - loss: 0.0448 - accuracy: 0.9811 - val_loss: 0.4313 - val_accuracy: 0.8000\n",
      "Epoch 64/100\n",
      "212/212 [==============================] - 19s 90ms/step - loss: 0.1065 - accuracy: 0.9458 - val_loss: 0.3716 - val_accuracy: 0.8105\n",
      "Epoch 65/100\n",
      "212/212 [==============================] - 19s 90ms/step - loss: 0.0653 - accuracy: 0.9705 - val_loss: 0.4337 - val_accuracy: 0.8000\n",
      "Epoch 66/100\n",
      "212/212 [==============================] - 19s 90ms/step - loss: 0.0588 - accuracy: 0.9776 - val_loss: 0.4525 - val_accuracy: 0.8105\n",
      "Epoch 67/100\n",
      "212/212 [==============================] - 19s 90ms/step - loss: 0.0624 - accuracy: 0.9741 - val_loss: 0.3778 - val_accuracy: 0.8000\n",
      "Epoch 68/100\n",
      "212/212 [==============================] - 19s 90ms/step - loss: 0.0753 - accuracy: 0.9646 - val_loss: 0.4519 - val_accuracy: 0.8105\n",
      "Epoch 69/100\n",
      "212/212 [==============================] - 19s 90ms/step - loss: 0.0711 - accuracy: 0.9682 - val_loss: 0.3831 - val_accuracy: 0.8211\n",
      "Epoch 70/100\n",
      "212/212 [==============================] - 19s 90ms/step - loss: 0.0516 - accuracy: 0.9835 - val_loss: 0.5687 - val_accuracy: 0.8105\n",
      "Epoch 71/100\n",
      "212/212 [==============================] - 19s 90ms/step - loss: 0.0815 - accuracy: 0.9599 - val_loss: 0.3603 - val_accuracy: 0.8211\n",
      "Epoch 72/100\n",
      "212/212 [==============================] - 19s 90ms/step - loss: 0.0825 - accuracy: 0.9682 - val_loss: 0.3644 - val_accuracy: 0.8105\n",
      "Epoch 73/100\n",
      "212/212 [==============================] - 19s 90ms/step - loss: 0.0362 - accuracy: 0.9870 - val_loss: 0.3802 - val_accuracy: 0.8211\n",
      "Epoch 74/100\n",
      "212/212 [==============================] - 19s 90ms/step - loss: 0.0676 - accuracy: 0.9741 - val_loss: 0.6623 - val_accuracy: 0.8000\n",
      "Epoch 75/100\n",
      "212/212 [==============================] - 19s 90ms/step - loss: 0.0485 - accuracy: 0.9776 - val_loss: 0.5082 - val_accuracy: 0.8105\n",
      "Epoch 76/100\n",
      "212/212 [==============================] - 19s 90ms/step - loss: 0.0504 - accuracy: 0.9788 - val_loss: 0.5708 - val_accuracy: 0.8105\n",
      "Epoch 77/100\n",
      "212/212 [==============================] - 19s 90ms/step - loss: 0.0814 - accuracy: 0.9634 - val_loss: 0.5406 - val_accuracy: 0.7789\n",
      "Epoch 78/100\n",
      "212/212 [==============================] - 19s 90ms/step - loss: 0.0420 - accuracy: 0.9847 - val_loss: 0.4258 - val_accuracy: 0.8211\n",
      "Epoch 79/100\n",
      "212/212 [==============================] - 19s 90ms/step - loss: 0.0492 - accuracy: 0.9823 - val_loss: 0.5178 - val_accuracy: 0.8000\n",
      "Epoch 80/100\n",
      "212/212 [==============================] - 19s 90ms/step - loss: 0.0322 - accuracy: 0.9882 - val_loss: 0.4283 - val_accuracy: 0.8211\n",
      "Epoch 81/100\n",
      "212/212 [==============================] - 19s 90ms/step - loss: 0.0679 - accuracy: 0.9670 - val_loss: 0.4132 - val_accuracy: 0.8316\n",
      "Epoch 82/100\n",
      "212/212 [==============================] - 19s 90ms/step - loss: 0.0590 - accuracy: 0.9764 - val_loss: 0.3771 - val_accuracy: 0.8105\n",
      "Epoch 83/100\n",
      "212/212 [==============================] - 19s 90ms/step - loss: 0.0658 - accuracy: 0.9741 - val_loss: 0.4339 - val_accuracy: 0.8211\n",
      "Epoch 84/100\n",
      "212/212 [==============================] - 19s 90ms/step - loss: 0.0323 - accuracy: 0.9917 - val_loss: 0.4640 - val_accuracy: 0.8211\n",
      "Epoch 85/100\n",
      "212/212 [==============================] - 19s 90ms/step - loss: 0.0768 - accuracy: 0.9670 - val_loss: 0.4082 - val_accuracy: 0.8105\n",
      "Epoch 86/100\n",
      "212/212 [==============================] - 19s 90ms/step - loss: 0.0358 - accuracy: 0.9858 - val_loss: 0.4189 - val_accuracy: 0.8211\n",
      "Epoch 87/100\n",
      "212/212 [==============================] - 19s 90ms/step - loss: 0.0752 - accuracy: 0.9717 - val_loss: 0.6389 - val_accuracy: 0.7895\n",
      "Epoch 88/100\n",
      "212/212 [==============================] - 19s 90ms/step - loss: 0.0746 - accuracy: 0.9670 - val_loss: 0.4361 - val_accuracy: 0.8105\n",
      "Epoch 89/100\n",
      "212/212 [==============================] - 19s 90ms/step - loss: 0.0717 - accuracy: 0.9693 - val_loss: 0.4034 - val_accuracy: 0.8000\n",
      "Epoch 90/100\n",
      "212/212 [==============================] - 19s 90ms/step - loss: 0.0386 - accuracy: 0.9882 - val_loss: 0.3902 - val_accuracy: 0.8316\n",
      "Epoch 91/100\n",
      "212/212 [==============================] - 19s 90ms/step - loss: 0.0534 - accuracy: 0.9800 - val_loss: 0.4430 - val_accuracy: 0.8211\n",
      "Epoch 92/100\n",
      "212/212 [==============================] - 19s 90ms/step - loss: 0.0792 - accuracy: 0.9587 - val_loss: 0.4974 - val_accuracy: 0.8000\n",
      "Epoch 93/100\n",
      "212/212 [==============================] - 19s 90ms/step - loss: 0.0393 - accuracy: 0.9894 - val_loss: 0.4840 - val_accuracy: 0.8211\n",
      "Epoch 94/100\n",
      "212/212 [==============================] - 19s 90ms/step - loss: 0.0532 - accuracy: 0.9811 - val_loss: 0.3918 - val_accuracy: 0.8211\n",
      "Epoch 95/100\n",
      "212/212 [==============================] - 19s 90ms/step - loss: 0.0358 - accuracy: 0.9894 - val_loss: 0.3836 - val_accuracy: 0.8211\n",
      "Epoch 96/100\n",
      "212/212 [==============================] - 19s 90ms/step - loss: 0.0496 - accuracy: 0.9764 - val_loss: 0.4266 - val_accuracy: 0.8000\n",
      "Epoch 97/100\n",
      "212/212 [==============================] - 19s 90ms/step - loss: 0.0651 - accuracy: 0.9741 - val_loss: 0.4056 - val_accuracy: 0.8105\n",
      "Epoch 98/100\n",
      "212/212 [==============================] - 19s 90ms/step - loss: 0.0537 - accuracy: 0.9788 - val_loss: 0.3703 - val_accuracy: 0.8211\n",
      "Epoch 99/100\n",
      "212/212 [==============================] - 19s 90ms/step - loss: 0.0621 - accuracy: 0.9776 - val_loss: 0.3894 - val_accuracy: 0.8105\n",
      "Epoch 100/100\n",
      "212/212 [==============================] - 19s 90ms/step - loss: 0.0569 - accuracy: 0.9776 - val_loss: 0.3717 - val_accuracy: 0.8105\n",
      "Score for fold 3: loss of 0.3717457056045532; accuracy of 81.05263113975525%\n",
      "(849,) (94,)\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 4 ...\n",
      "Epoch 1/100\n",
      "213/213 [==============================] - 27s 106ms/step - loss: 0.5554 - accuracy: 0.5265 - val_loss: 1.1734 - val_accuracy: 0.3511\n",
      "Epoch 2/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.4568 - accuracy: 0.6278 - val_loss: 1.7125 - val_accuracy: 0.3191\n",
      "Epoch 3/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.4487 - accuracy: 0.6443 - val_loss: 12.9311 - val_accuracy: 0.3511\n",
      "Epoch 4/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.4221 - accuracy: 0.6855 - val_loss: 0.4665 - val_accuracy: 0.6170\n",
      "Epoch 5/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.3555 - accuracy: 0.7621 - val_loss: 1.4408 - val_accuracy: 0.6702\n",
      "Epoch 6/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.3720 - accuracy: 0.7468 - val_loss: 1.1190 - val_accuracy: 0.6489\n",
      "Epoch 7/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.3340 - accuracy: 0.7762 - val_loss: 1.4117 - val_accuracy: 0.5106\n",
      "Epoch 8/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.2742 - accuracy: 0.8280 - val_loss: 1.2903 - val_accuracy: 0.6170\n",
      "Epoch 9/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.2862 - accuracy: 0.8186 - val_loss: 1.4533 - val_accuracy: 0.6277\n",
      "Epoch 10/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.2678 - accuracy: 0.8398 - val_loss: 1.0700 - val_accuracy: 0.6064\n",
      "Epoch 11/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.2160 - accuracy: 0.8751 - val_loss: 2.7334 - val_accuracy: 0.3936\n",
      "Epoch 12/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.1957 - accuracy: 0.8928 - val_loss: 1.6541 - val_accuracy: 0.6489\n",
      "Epoch 13/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.2036 - accuracy: 0.8940 - val_loss: 2.0709 - val_accuracy: 0.6702\n",
      "Epoch 14/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.2061 - accuracy: 0.8999 - val_loss: 1.7782 - val_accuracy: 0.6383\n",
      "Epoch 15/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.1778 - accuracy: 0.9117 - val_loss: 0.9611 - val_accuracy: 0.6064\n",
      "Epoch 16/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.1721 - accuracy: 0.9140 - val_loss: 1.1479 - val_accuracy: 0.6170\n",
      "Epoch 17/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.1635 - accuracy: 0.9176 - val_loss: 1.1661 - val_accuracy: 0.6277\n",
      "Epoch 18/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.1563 - accuracy: 0.9246 - val_loss: 1.1725 - val_accuracy: 0.6489\n",
      "Epoch 19/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.1294 - accuracy: 0.9470 - val_loss: 1.0034 - val_accuracy: 0.5745\n",
      "Epoch 20/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.1500 - accuracy: 0.9305 - val_loss: 22.6902 - val_accuracy: 0.3191\n",
      "Epoch 21/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.1358 - accuracy: 0.9340 - val_loss: 0.3826 - val_accuracy: 0.8191\n",
      "Epoch 22/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.1197 - accuracy: 0.9576 - val_loss: 1.4328 - val_accuracy: 0.6702\n",
      "Epoch 23/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.1273 - accuracy: 0.9446 - val_loss: 0.9498 - val_accuracy: 0.6383\n",
      "Epoch 24/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0838 - accuracy: 0.9682 - val_loss: 1.4293 - val_accuracy: 0.5745\n",
      "Epoch 25/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.1167 - accuracy: 0.9529 - val_loss: 2.3200 - val_accuracy: 0.5213\n",
      "Epoch 26/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.1202 - accuracy: 0.9505 - val_loss: 0.7007 - val_accuracy: 0.6915\n",
      "Epoch 27/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0808 - accuracy: 0.9741 - val_loss: 0.2686 - val_accuracy: 0.8511\n",
      "Epoch 28/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.1091 - accuracy: 0.9564 - val_loss: 0.4127 - val_accuracy: 0.7872\n",
      "Epoch 29/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0729 - accuracy: 0.9847 - val_loss: 0.7450 - val_accuracy: 0.7234\n",
      "Epoch 30/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0986 - accuracy: 0.9647 - val_loss: 0.6284 - val_accuracy: 0.7660\n",
      "Epoch 31/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0783 - accuracy: 0.9717 - val_loss: 0.8038 - val_accuracy: 0.6809\n",
      "Epoch 32/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0850 - accuracy: 0.9706 - val_loss: 1.0811 - val_accuracy: 0.6702\n",
      "Epoch 33/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0617 - accuracy: 0.9812 - val_loss: 1.0894 - val_accuracy: 0.6702\n",
      "Epoch 34/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0448 - accuracy: 0.9906 - val_loss: 0.7629 - val_accuracy: 0.7128\n",
      "Epoch 35/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0674 - accuracy: 0.9800 - val_loss: 0.3482 - val_accuracy: 0.8191\n",
      "Epoch 36/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.1140 - accuracy: 0.9482 - val_loss: 0.4192 - val_accuracy: 0.8085\n",
      "Epoch 37/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0799 - accuracy: 0.9694 - val_loss: 0.2620 - val_accuracy: 0.8723\n",
      "Epoch 38/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0730 - accuracy: 0.9729 - val_loss: 1.1697 - val_accuracy: 0.5319\n",
      "Epoch 39/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0851 - accuracy: 0.9623 - val_loss: 0.7384 - val_accuracy: 0.7660\n",
      "Epoch 40/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0724 - accuracy: 0.9753 - val_loss: 0.3943 - val_accuracy: 0.7979\n",
      "Epoch 41/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0793 - accuracy: 0.9694 - val_loss: 0.2257 - val_accuracy: 0.8723\n",
      "Epoch 42/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0591 - accuracy: 0.9788 - val_loss: 1.0324 - val_accuracy: 0.6702\n",
      "Epoch 43/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0666 - accuracy: 0.9776 - val_loss: 0.2645 - val_accuracy: 0.8617\n",
      "Epoch 44/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0637 - accuracy: 0.9764 - val_loss: 0.2220 - val_accuracy: 0.8936\n",
      "Epoch 45/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0841 - accuracy: 0.9706 - val_loss: 0.6063 - val_accuracy: 0.7447\n",
      "Epoch 46/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0904 - accuracy: 0.9670 - val_loss: 0.7456 - val_accuracy: 0.7128\n",
      "Epoch 47/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0523 - accuracy: 0.9847 - val_loss: 0.2619 - val_accuracy: 0.8298\n",
      "Epoch 48/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0774 - accuracy: 0.9647 - val_loss: 0.5107 - val_accuracy: 0.7766\n",
      "Epoch 49/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0773 - accuracy: 0.9670 - val_loss: 1.0680 - val_accuracy: 0.6596\n",
      "Epoch 50/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0616 - accuracy: 0.9764 - val_loss: 0.2109 - val_accuracy: 0.9149\n",
      "Epoch 51/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0721 - accuracy: 0.9717 - val_loss: 0.5288 - val_accuracy: 0.7553\n",
      "Epoch 52/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0417 - accuracy: 0.9882 - val_loss: 0.4065 - val_accuracy: 0.8191\n",
      "Epoch 53/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0651 - accuracy: 0.9753 - val_loss: 0.2767 - val_accuracy: 0.8511\n",
      "Epoch 54/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0731 - accuracy: 0.9706 - val_loss: 1.1399 - val_accuracy: 0.6702\n",
      "Epoch 55/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0671 - accuracy: 0.9682 - val_loss: 0.2174 - val_accuracy: 0.8936\n",
      "Epoch 56/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0685 - accuracy: 0.9741 - val_loss: 0.2230 - val_accuracy: 0.8936\n",
      "Epoch 57/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0559 - accuracy: 0.9823 - val_loss: 0.3655 - val_accuracy: 0.8191\n",
      "Epoch 58/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0885 - accuracy: 0.9564 - val_loss: 0.6854 - val_accuracy: 0.7340\n",
      "Epoch 59/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0454 - accuracy: 0.9823 - val_loss: 0.2308 - val_accuracy: 0.9043\n",
      "Epoch 60/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0693 - accuracy: 0.9706 - val_loss: 0.2175 - val_accuracy: 0.8936\n",
      "Epoch 61/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0487 - accuracy: 0.9823 - val_loss: 0.4706 - val_accuracy: 0.7979\n",
      "Epoch 62/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0659 - accuracy: 0.9753 - val_loss: 0.2392 - val_accuracy: 0.8511\n",
      "Epoch 63/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0548 - accuracy: 0.9812 - val_loss: 0.3608 - val_accuracy: 0.8191\n",
      "Epoch 64/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0442 - accuracy: 0.9859 - val_loss: 0.2216 - val_accuracy: 0.8936\n",
      "Epoch 65/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0928 - accuracy: 0.9564 - val_loss: 0.2671 - val_accuracy: 0.8617\n",
      "Epoch 66/100\n",
      "213/213 [==============================] - 21s 96ms/step - loss: 0.0756 - accuracy: 0.9647 - val_loss: 0.2387 - val_accuracy: 0.8723\n",
      "Epoch 67/100\n",
      "213/213 [==============================] - 21s 96ms/step - loss: 0.0412 - accuracy: 0.9894 - val_loss: 0.2567 - val_accuracy: 0.8617\n",
      "Epoch 68/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0549 - accuracy: 0.9788 - val_loss: 0.2540 - val_accuracy: 0.8617\n",
      "Epoch 69/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0645 - accuracy: 0.9753 - val_loss: 0.2370 - val_accuracy: 0.8511\n",
      "Epoch 70/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0800 - accuracy: 0.9647 - val_loss: 0.2419 - val_accuracy: 0.8404\n",
      "Epoch 71/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0386 - accuracy: 0.9847 - val_loss: 0.2390 - val_accuracy: 0.8617\n",
      "Epoch 72/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0577 - accuracy: 0.9788 - val_loss: 0.2874 - val_accuracy: 0.8404\n",
      "Epoch 73/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0686 - accuracy: 0.9694 - val_loss: 0.9658 - val_accuracy: 0.6915\n",
      "Epoch 74/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0550 - accuracy: 0.9729 - val_loss: 0.5035 - val_accuracy: 0.8191\n",
      "Epoch 75/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0630 - accuracy: 0.9776 - val_loss: 0.2255 - val_accuracy: 0.8936\n",
      "Epoch 76/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0728 - accuracy: 0.9623 - val_loss: 0.2273 - val_accuracy: 0.8936\n",
      "Epoch 77/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0783 - accuracy: 0.9670 - val_loss: 0.4121 - val_accuracy: 0.8298\n",
      "Epoch 78/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0810 - accuracy: 0.9611 - val_loss: 0.2177 - val_accuracy: 0.8936\n",
      "Epoch 79/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0661 - accuracy: 0.9729 - val_loss: 0.2142 - val_accuracy: 0.8830\n",
      "Epoch 80/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0864 - accuracy: 0.9611 - val_loss: 0.4776 - val_accuracy: 0.7979\n",
      "Epoch 81/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0775 - accuracy: 0.9670 - val_loss: 0.4161 - val_accuracy: 0.8085\n",
      "Epoch 82/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0630 - accuracy: 0.9753 - val_loss: 0.4090 - val_accuracy: 0.8085\n",
      "Epoch 83/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0430 - accuracy: 0.9882 - val_loss: 0.2432 - val_accuracy: 0.8936\n",
      "Epoch 84/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0582 - accuracy: 0.9729 - val_loss: 0.3949 - val_accuracy: 0.8298\n",
      "Epoch 85/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0483 - accuracy: 0.9812 - val_loss: 0.2235 - val_accuracy: 0.8723\n",
      "Epoch 86/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0800 - accuracy: 0.9623 - val_loss: 0.2650 - val_accuracy: 0.8404\n",
      "Epoch 87/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0834 - accuracy: 0.9611 - val_loss: 0.2205 - val_accuracy: 0.8723\n",
      "Epoch 88/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0525 - accuracy: 0.9764 - val_loss: 0.3491 - val_accuracy: 0.8191\n",
      "Epoch 89/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0804 - accuracy: 0.9611 - val_loss: 0.2200 - val_accuracy: 0.8830\n",
      "Epoch 90/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0313 - accuracy: 0.9894 - val_loss: 0.3619 - val_accuracy: 0.8191\n",
      "Epoch 91/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0546 - accuracy: 0.9788 - val_loss: 0.2383 - val_accuracy: 0.8723\n",
      "Epoch 92/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0586 - accuracy: 0.9812 - val_loss: 0.2253 - val_accuracy: 0.8723\n",
      "Epoch 93/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0519 - accuracy: 0.9800 - val_loss: 0.2239 - val_accuracy: 0.8617\n",
      "Epoch 94/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0563 - accuracy: 0.9788 - val_loss: 0.2210 - val_accuracy: 0.8723\n",
      "Epoch 95/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0756 - accuracy: 0.9647 - val_loss: 0.2184 - val_accuracy: 0.8723\n",
      "Epoch 96/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0521 - accuracy: 0.9776 - val_loss: 0.2490 - val_accuracy: 0.8511\n",
      "Epoch 97/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0635 - accuracy: 0.9753 - val_loss: 0.3422 - val_accuracy: 0.8511\n",
      "Epoch 98/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0424 - accuracy: 0.9847 - val_loss: 0.2317 - val_accuracy: 0.8830\n",
      "Epoch 99/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0546 - accuracy: 0.9764 - val_loss: 0.2850 - val_accuracy: 0.8298\n",
      "Epoch 100/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0545 - accuracy: 0.9753 - val_loss: 0.2370 - val_accuracy: 0.8830\n",
      "Score for fold 4: loss of 0.23704662919044495; accuracy of 88.29787373542786%\n",
      "(849,) (94,)\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 5 ...\n",
      "Epoch 1/100\n",
      "213/213 [==============================] - 25s 100ms/step - loss: 0.5633 - accuracy: 0.5230 - val_loss: 1.1038 - val_accuracy: 0.3298\n",
      "Epoch 2/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.4855 - accuracy: 0.6172 - val_loss: 1.1824 - val_accuracy: 0.3511\n",
      "Epoch 3/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.4574 - accuracy: 0.6266 - val_loss: 8.1399 - val_accuracy: 0.4894\n",
      "Epoch 4/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.4280 - accuracy: 0.6843 - val_loss: 23.8320 - val_accuracy: 0.2872\n",
      "Epoch 5/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.3931 - accuracy: 0.7279 - val_loss: 66.9923 - val_accuracy: 0.3298\n",
      "Epoch 6/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.3853 - accuracy: 0.7256 - val_loss: 0.7519 - val_accuracy: 0.5426\n",
      "Epoch 7/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.3388 - accuracy: 0.7809 - val_loss: 1.0297 - val_accuracy: 0.5106\n",
      "Epoch 8/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.3305 - accuracy: 0.7774 - val_loss: 3.5845 - val_accuracy: 0.4255\n",
      "Epoch 9/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.3042 - accuracy: 0.8163 - val_loss: 53.2572 - val_accuracy: 0.3191\n",
      "Epoch 10/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.2548 - accuracy: 0.8634 - val_loss: 1.0623 - val_accuracy: 0.4574\n",
      "Epoch 11/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.2423 - accuracy: 0.8728 - val_loss: 28.0790 - val_accuracy: 0.3191\n",
      "Epoch 12/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.2385 - accuracy: 0.8681 - val_loss: 104.7255 - val_accuracy: 0.3191\n",
      "Epoch 13/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.2032 - accuracy: 0.8963 - val_loss: 11.4664 - val_accuracy: 0.3511\n",
      "Epoch 14/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.1969 - accuracy: 0.8928 - val_loss: 0.3368 - val_accuracy: 0.8191\n",
      "Epoch 15/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.2012 - accuracy: 0.8952 - val_loss: 1.7034 - val_accuracy: 0.3298\n",
      "Epoch 16/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.1617 - accuracy: 0.9246 - val_loss: 11.1308 - val_accuracy: 0.3191\n",
      "Epoch 17/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.1931 - accuracy: 0.9069 - val_loss: 20.8056 - val_accuracy: 0.3191\n",
      "Epoch 18/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.1562 - accuracy: 0.9329 - val_loss: 16.2888 - val_accuracy: 0.3191\n",
      "Epoch 19/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.1317 - accuracy: 0.9505 - val_loss: 44.1538 - val_accuracy: 0.3191\n",
      "Epoch 20/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.1277 - accuracy: 0.9446 - val_loss: 4.8790 - val_accuracy: 0.5000\n",
      "Epoch 21/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.1066 - accuracy: 0.9517 - val_loss: 0.7269 - val_accuracy: 0.7340\n",
      "Epoch 22/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.1335 - accuracy: 0.9446 - val_loss: 2.1810 - val_accuracy: 0.4574\n",
      "Epoch 23/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.1170 - accuracy: 0.9470 - val_loss: 0.8755 - val_accuracy: 0.6596\n",
      "Epoch 24/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0956 - accuracy: 0.9611 - val_loss: 3.0581 - val_accuracy: 0.4574\n",
      "Epoch 25/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.1132 - accuracy: 0.9552 - val_loss: 1.2694 - val_accuracy: 0.6489\n",
      "Epoch 26/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0931 - accuracy: 0.9635 - val_loss: 0.6002 - val_accuracy: 0.7766\n",
      "Epoch 27/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.1011 - accuracy: 0.9623 - val_loss: 0.5206 - val_accuracy: 0.7766\n",
      "Epoch 28/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.1658 - accuracy: 0.9234 - val_loss: 0.4440 - val_accuracy: 0.7553\n",
      "Epoch 29/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0911 - accuracy: 0.9717 - val_loss: 0.3352 - val_accuracy: 0.8617\n",
      "Epoch 30/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0834 - accuracy: 0.9694 - val_loss: 0.3292 - val_accuracy: 0.7979\n",
      "Epoch 31/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0957 - accuracy: 0.9647 - val_loss: 1.9347 - val_accuracy: 0.3191\n",
      "Epoch 32/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.1047 - accuracy: 0.9529 - val_loss: 0.9842 - val_accuracy: 0.6809\n",
      "Epoch 33/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0715 - accuracy: 0.9776 - val_loss: 0.2642 - val_accuracy: 0.8511\n",
      "Epoch 34/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0846 - accuracy: 0.9682 - val_loss: 0.3216 - val_accuracy: 0.7766\n",
      "Epoch 35/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0840 - accuracy: 0.9706 - val_loss: 0.3469 - val_accuracy: 0.8511\n",
      "Epoch 36/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0921 - accuracy: 0.9600 - val_loss: 0.3146 - val_accuracy: 0.8617\n",
      "Epoch 37/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0527 - accuracy: 0.9847 - val_loss: 0.8276 - val_accuracy: 0.6064\n",
      "Epoch 38/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0637 - accuracy: 0.9776 - val_loss: 0.2706 - val_accuracy: 0.8298\n",
      "Epoch 39/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0691 - accuracy: 0.9776 - val_loss: 0.3648 - val_accuracy: 0.8404\n",
      "Epoch 40/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0743 - accuracy: 0.9670 - val_loss: 0.3823 - val_accuracy: 0.8298\n",
      "Epoch 41/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0608 - accuracy: 0.9812 - val_loss: 0.3950 - val_accuracy: 0.7979\n",
      "Epoch 42/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0543 - accuracy: 0.9788 - val_loss: 0.4622 - val_accuracy: 0.8191\n",
      "Epoch 43/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0688 - accuracy: 0.9682 - val_loss: 0.3910 - val_accuracy: 0.8085\n",
      "Epoch 44/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0651 - accuracy: 0.9776 - val_loss: 0.3184 - val_accuracy: 0.8511\n",
      "Epoch 45/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0613 - accuracy: 0.9788 - val_loss: 0.2825 - val_accuracy: 0.8511\n",
      "Epoch 46/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0973 - accuracy: 0.9564 - val_loss: 0.5850 - val_accuracy: 0.7447\n",
      "Epoch 47/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0795 - accuracy: 0.9635 - val_loss: 0.8825 - val_accuracy: 0.7340\n",
      "Epoch 48/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0725 - accuracy: 0.9741 - val_loss: 0.5148 - val_accuracy: 0.7979\n",
      "Epoch 49/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0461 - accuracy: 0.9859 - val_loss: 0.3098 - val_accuracy: 0.8298\n",
      "Epoch 50/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0786 - accuracy: 0.9635 - val_loss: 0.2771 - val_accuracy: 0.8298\n",
      "Epoch 51/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0514 - accuracy: 0.9812 - val_loss: 0.3794 - val_accuracy: 0.8191\n",
      "Epoch 52/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0464 - accuracy: 0.9823 - val_loss: 0.5580 - val_accuracy: 0.7872\n",
      "Epoch 53/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0602 - accuracy: 0.9741 - val_loss: 0.3258 - val_accuracy: 0.8511\n",
      "Epoch 54/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0487 - accuracy: 0.9835 - val_loss: 0.3403 - val_accuracy: 0.8723\n",
      "Epoch 55/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0586 - accuracy: 0.9800 - val_loss: 0.4634 - val_accuracy: 0.7447\n",
      "Epoch 56/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0648 - accuracy: 0.9764 - val_loss: 0.2533 - val_accuracy: 0.8723\n",
      "Epoch 57/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0680 - accuracy: 0.9694 - val_loss: 0.5088 - val_accuracy: 0.7553\n",
      "Epoch 58/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0981 - accuracy: 0.9564 - val_loss: 0.2613 - val_accuracy: 0.8723\n",
      "Epoch 59/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0636 - accuracy: 0.9764 - val_loss: 0.2950 - val_accuracy: 0.8404\n",
      "Epoch 60/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0722 - accuracy: 0.9682 - val_loss: 0.2860 - val_accuracy: 0.8617\n",
      "Epoch 61/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0787 - accuracy: 0.9623 - val_loss: 0.2391 - val_accuracy: 0.8617\n",
      "Epoch 62/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0680 - accuracy: 0.9706 - val_loss: 0.3757 - val_accuracy: 0.8191\n",
      "Epoch 63/100\n",
      "213/213 [==============================] - 21s 96ms/step - loss: 0.0733 - accuracy: 0.9670 - val_loss: 0.3576 - val_accuracy: 0.8617\n",
      "Epoch 64/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0366 - accuracy: 0.9906 - val_loss: 0.2558 - val_accuracy: 0.8936\n",
      "Epoch 65/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0650 - accuracy: 0.9729 - val_loss: 0.3623 - val_accuracy: 0.8191\n",
      "Epoch 66/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0776 - accuracy: 0.9635 - val_loss: 0.2705 - val_accuracy: 0.8404\n",
      "Epoch 67/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0361 - accuracy: 0.9870 - val_loss: 0.2476 - val_accuracy: 0.8617\n",
      "Epoch 68/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0894 - accuracy: 0.9552 - val_loss: 0.2400 - val_accuracy: 0.8723\n",
      "Epoch 69/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0549 - accuracy: 0.9835 - val_loss: 0.4770 - val_accuracy: 0.7979\n",
      "Epoch 70/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0420 - accuracy: 0.9859 - val_loss: 0.2658 - val_accuracy: 0.8617\n",
      "Epoch 71/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0839 - accuracy: 0.9588 - val_loss: 0.3410 - val_accuracy: 0.8404\n",
      "Epoch 72/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0813 - accuracy: 0.9600 - val_loss: 0.2483 - val_accuracy: 0.8830\n",
      "Epoch 73/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0625 - accuracy: 0.9741 - val_loss: 0.2800 - val_accuracy: 0.8617\n",
      "Epoch 74/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0628 - accuracy: 0.9706 - val_loss: 0.3280 - val_accuracy: 0.8404\n",
      "Epoch 75/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0513 - accuracy: 0.9800 - val_loss: 0.2405 - val_accuracy: 0.8723\n",
      "Epoch 76/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0611 - accuracy: 0.9706 - val_loss: 0.2613 - val_accuracy: 0.8617\n",
      "Epoch 77/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0615 - accuracy: 0.9741 - val_loss: 0.3118 - val_accuracy: 0.8085\n",
      "Epoch 78/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0541 - accuracy: 0.9847 - val_loss: 0.2888 - val_accuracy: 0.8511\n",
      "Epoch 79/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.1009 - accuracy: 0.9552 - val_loss: 0.3485 - val_accuracy: 0.8191\n",
      "Epoch 80/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0715 - accuracy: 0.9658 - val_loss: 0.2854 - val_accuracy: 0.8617\n",
      "Epoch 81/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0605 - accuracy: 0.9741 - val_loss: 0.2496 - val_accuracy: 0.8723\n",
      "Epoch 82/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0816 - accuracy: 0.9552 - val_loss: 0.3365 - val_accuracy: 0.8511\n",
      "Epoch 83/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0638 - accuracy: 0.9729 - val_loss: 0.3263 - val_accuracy: 0.8085\n",
      "Epoch 84/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0380 - accuracy: 0.9882 - val_loss: 0.4531 - val_accuracy: 0.7872\n",
      "Epoch 85/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0597 - accuracy: 0.9764 - val_loss: 0.4796 - val_accuracy: 0.7660\n",
      "Epoch 86/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0581 - accuracy: 0.9753 - val_loss: 0.3151 - val_accuracy: 0.8617\n",
      "Epoch 87/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0550 - accuracy: 0.9812 - val_loss: 0.2559 - val_accuracy: 0.8723\n",
      "Epoch 88/100\n",
      "213/213 [==============================] - 21s 99ms/step - loss: 0.0581 - accuracy: 0.9741 - val_loss: 0.2645 - val_accuracy: 0.8617\n",
      "Epoch 89/100\n",
      "213/213 [==============================] - 21s 100ms/step - loss: 0.0607 - accuracy: 0.9764 - val_loss: 0.3816 - val_accuracy: 0.7979\n",
      "Epoch 90/100\n",
      "213/213 [==============================] - 21s 100ms/step - loss: 0.0303 - accuracy: 0.9918 - val_loss: 0.2969 - val_accuracy: 0.8191\n",
      "Epoch 91/100\n",
      "213/213 [==============================] - 22s 101ms/step - loss: 0.0685 - accuracy: 0.9670 - val_loss: 0.2757 - val_accuracy: 0.8404\n",
      "Epoch 92/100\n",
      "213/213 [==============================] - 21s 99ms/step - loss: 0.0780 - accuracy: 0.9658 - val_loss: 0.3040 - val_accuracy: 0.8511\n",
      "Epoch 93/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0683 - accuracy: 0.9658 - val_loss: 0.2612 - val_accuracy: 0.8511\n",
      "Epoch 94/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0541 - accuracy: 0.9859 - val_loss: 0.2743 - val_accuracy: 0.8404\n",
      "Epoch 95/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0820 - accuracy: 0.9588 - val_loss: 0.2830 - val_accuracy: 0.8511\n",
      "Epoch 96/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0790 - accuracy: 0.9623 - val_loss: 0.2747 - val_accuracy: 0.8723\n",
      "Epoch 97/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0685 - accuracy: 0.9706 - val_loss: 0.4084 - val_accuracy: 0.8085\n",
      "Epoch 98/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0437 - accuracy: 0.9847 - val_loss: 0.3581 - val_accuracy: 0.8191\n",
      "Epoch 99/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0413 - accuracy: 0.9835 - val_loss: 0.3769 - val_accuracy: 0.8191\n",
      "Epoch 100/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0707 - accuracy: 0.9717 - val_loss: 0.2944 - val_accuracy: 0.8191\n",
      "Score for fold 5: loss of 0.2944486737251282; accuracy of 81.91489577293396%\n",
      "(849,) (94,)\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 6 ...\n",
      "Epoch 1/100\n",
      "213/213 [==============================] - 25s 100ms/step - loss: 0.5634 - accuracy: 0.5430 - val_loss: 1.0725 - val_accuracy: 0.3191\n",
      "Epoch 2/100\n",
      "213/213 [==============================] - 21s 96ms/step - loss: 0.4911 - accuracy: 0.6078 - val_loss: 3.3248 - val_accuracy: 0.3191\n",
      "Epoch 3/100\n",
      "213/213 [==============================] - 21s 96ms/step - loss: 0.4280 - accuracy: 0.6773 - val_loss: 3.4878 - val_accuracy: 0.3191\n",
      "Epoch 4/100\n",
      "213/213 [==============================] - 21s 96ms/step - loss: 0.4232 - accuracy: 0.6961 - val_loss: 0.4949 - val_accuracy: 0.5213\n",
      "Epoch 5/100\n",
      "213/213 [==============================] - 21s 96ms/step - loss: 0.4061 - accuracy: 0.7244 - val_loss: 8.6177 - val_accuracy: 0.3511\n",
      "Epoch 6/100\n",
      "213/213 [==============================] - 21s 96ms/step - loss: 0.3750 - accuracy: 0.7373 - val_loss: 0.6410 - val_accuracy: 0.4468\n",
      "Epoch 7/100\n",
      "213/213 [==============================] - 20s 96ms/step - loss: 0.3370 - accuracy: 0.7880 - val_loss: 6.8962 - val_accuracy: 0.3511\n",
      "Epoch 8/100\n",
      "213/213 [==============================] - 20s 96ms/step - loss: 0.3113 - accuracy: 0.8257 - val_loss: 14.8915 - val_accuracy: 0.3511\n",
      "Epoch 9/100\n",
      "213/213 [==============================] - 20s 96ms/step - loss: 0.3033 - accuracy: 0.8233 - val_loss: 16.2552 - val_accuracy: 0.3511\n",
      "Epoch 10/100\n",
      "213/213 [==============================] - 20s 96ms/step - loss: 0.2627 - accuracy: 0.8469 - val_loss: 0.5927 - val_accuracy: 0.7340\n",
      "Epoch 11/100\n",
      "213/213 [==============================] - 21s 96ms/step - loss: 0.2381 - accuracy: 0.8787 - val_loss: 0.4427 - val_accuracy: 0.7340\n",
      "Epoch 12/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.2238 - accuracy: 0.8716 - val_loss: 0.5365 - val_accuracy: 0.5745\n",
      "Epoch 13/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.2242 - accuracy: 0.8740 - val_loss: 6.7915 - val_accuracy: 0.3085\n",
      "Epoch 14/100\n",
      "213/213 [==============================] - 21s 96ms/step - loss: 0.1958 - accuracy: 0.8928 - val_loss: 0.2682 - val_accuracy: 0.8191\n",
      "Epoch 15/100\n",
      "213/213 [==============================] - 21s 96ms/step - loss: 0.1863 - accuracy: 0.9046 - val_loss: 0.3632 - val_accuracy: 0.7766\n",
      "Epoch 16/100\n",
      "213/213 [==============================] - 21s 96ms/step - loss: 0.1428 - accuracy: 0.9352 - val_loss: 2.8541 - val_accuracy: 0.5851\n",
      "Epoch 17/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.1705 - accuracy: 0.9187 - val_loss: 82.2730 - val_accuracy: 0.3404\n",
      "Epoch 18/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.1452 - accuracy: 0.9293 - val_loss: 0.3498 - val_accuracy: 0.7766\n",
      "Epoch 19/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.1269 - accuracy: 0.9435 - val_loss: 1.6225 - val_accuracy: 0.6702\n",
      "Epoch 20/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.1465 - accuracy: 0.9388 - val_loss: 0.4710 - val_accuracy: 0.7872\n",
      "Epoch 21/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.1084 - accuracy: 0.9600 - val_loss: 1.3903 - val_accuracy: 0.6489\n",
      "Epoch 22/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.1184 - accuracy: 0.9470 - val_loss: 2.4755 - val_accuracy: 0.6064\n",
      "Epoch 23/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.1506 - accuracy: 0.9352 - val_loss: 0.6574 - val_accuracy: 0.7447\n",
      "Epoch 24/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0945 - accuracy: 0.9670 - val_loss: 0.3935 - val_accuracy: 0.8085\n",
      "Epoch 25/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.1140 - accuracy: 0.9517 - val_loss: 1.2352 - val_accuracy: 0.6277\n",
      "Epoch 26/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.1011 - accuracy: 0.9576 - val_loss: 1.9675 - val_accuracy: 0.6064\n",
      "Epoch 27/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.1109 - accuracy: 0.9588 - val_loss: 0.3190 - val_accuracy: 0.8404\n",
      "Epoch 28/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0840 - accuracy: 0.9729 - val_loss: 0.3160 - val_accuracy: 0.8404\n",
      "Epoch 29/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0902 - accuracy: 0.9694 - val_loss: 0.2830 - val_accuracy: 0.8511\n",
      "Epoch 30/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0773 - accuracy: 0.9706 - val_loss: 0.4305 - val_accuracy: 0.7553\n",
      "Epoch 31/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0765 - accuracy: 0.9682 - val_loss: 0.2422 - val_accuracy: 0.8830\n",
      "Epoch 32/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0709 - accuracy: 0.9729 - val_loss: 0.4415 - val_accuracy: 0.8191\n",
      "Epoch 33/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0693 - accuracy: 0.9800 - val_loss: 0.3465 - val_accuracy: 0.8191\n",
      "Epoch 34/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0831 - accuracy: 0.9682 - val_loss: 0.2225 - val_accuracy: 0.8830\n",
      "Epoch 35/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0809 - accuracy: 0.9753 - val_loss: 0.3304 - val_accuracy: 0.8404\n",
      "Epoch 36/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0646 - accuracy: 0.9800 - val_loss: 0.2141 - val_accuracy: 0.8936\n",
      "Epoch 37/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.1178 - accuracy: 0.9494 - val_loss: 0.3483 - val_accuracy: 0.8404\n",
      "Epoch 38/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0960 - accuracy: 0.9552 - val_loss: 0.2730 - val_accuracy: 0.8404\n",
      "Epoch 39/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0859 - accuracy: 0.9623 - val_loss: 0.2064 - val_accuracy: 0.8617\n",
      "Epoch 40/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0901 - accuracy: 0.9647 - val_loss: 0.2395 - val_accuracy: 0.8404\n",
      "Epoch 41/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.1084 - accuracy: 0.9482 - val_loss: 0.1975 - val_accuracy: 0.8723\n",
      "Epoch 42/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0831 - accuracy: 0.9635 - val_loss: 0.2183 - val_accuracy: 0.8298\n",
      "Epoch 43/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0828 - accuracy: 0.9717 - val_loss: 0.2332 - val_accuracy: 0.8723\n",
      "Epoch 44/100\n",
      "213/213 [==============================] - 21s 96ms/step - loss: 0.0655 - accuracy: 0.9717 - val_loss: 0.1948 - val_accuracy: 0.9043\n",
      "Epoch 45/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0724 - accuracy: 0.9741 - val_loss: 0.2129 - val_accuracy: 0.8936\n",
      "Epoch 46/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0617 - accuracy: 0.9823 - val_loss: 0.2225 - val_accuracy: 0.8511\n",
      "Epoch 47/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0508 - accuracy: 0.9847 - val_loss: 0.2091 - val_accuracy: 0.8936\n",
      "Epoch 48/100\n",
      "213/213 [==============================] - 21s 96ms/step - loss: 0.0673 - accuracy: 0.9764 - val_loss: 0.2056 - val_accuracy: 0.8723\n",
      "Epoch 49/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0777 - accuracy: 0.9694 - val_loss: 0.2376 - val_accuracy: 0.8617\n",
      "Epoch 50/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0720 - accuracy: 0.9694 - val_loss: 0.2080 - val_accuracy: 0.8936\n",
      "Epoch 51/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0833 - accuracy: 0.9670 - val_loss: 0.2091 - val_accuracy: 0.8830\n",
      "Epoch 52/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0782 - accuracy: 0.9658 - val_loss: 0.2084 - val_accuracy: 0.8723\n",
      "Epoch 53/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0656 - accuracy: 0.9800 - val_loss: 0.2403 - val_accuracy: 0.8511\n",
      "Epoch 54/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0797 - accuracy: 0.9623 - val_loss: 0.2258 - val_accuracy: 0.8723\n",
      "Epoch 55/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0569 - accuracy: 0.9812 - val_loss: 0.2287 - val_accuracy: 0.8830\n",
      "Epoch 56/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0713 - accuracy: 0.9682 - val_loss: 0.2474 - val_accuracy: 0.8617\n",
      "Epoch 57/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0594 - accuracy: 0.9753 - val_loss: 0.2372 - val_accuracy: 0.8617\n",
      "Epoch 58/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0697 - accuracy: 0.9729 - val_loss: 0.3915 - val_accuracy: 0.8617\n",
      "Epoch 59/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0711 - accuracy: 0.9658 - val_loss: 0.2651 - val_accuracy: 0.8404\n",
      "Epoch 60/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0667 - accuracy: 0.9717 - val_loss: 0.2462 - val_accuracy: 0.8723\n",
      "Epoch 61/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0635 - accuracy: 0.9753 - val_loss: 0.2440 - val_accuracy: 0.8723\n",
      "Epoch 62/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0469 - accuracy: 0.9870 - val_loss: 0.2397 - val_accuracy: 0.8723\n",
      "Epoch 63/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0512 - accuracy: 0.9812 - val_loss: 0.2624 - val_accuracy: 0.8511\n",
      "Epoch 64/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0622 - accuracy: 0.9764 - val_loss: 0.2302 - val_accuracy: 0.8936\n",
      "Epoch 65/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0664 - accuracy: 0.9670 - val_loss: 0.2391 - val_accuracy: 0.8830\n",
      "Epoch 66/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0519 - accuracy: 0.9800 - val_loss: 0.2310 - val_accuracy: 0.8617\n",
      "Epoch 67/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0390 - accuracy: 0.9847 - val_loss: 0.2299 - val_accuracy: 0.8830\n",
      "Epoch 68/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0472 - accuracy: 0.9812 - val_loss: 0.3485 - val_accuracy: 0.8404\n",
      "Epoch 69/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0530 - accuracy: 0.9823 - val_loss: 0.2330 - val_accuracy: 0.8617\n",
      "Epoch 70/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0569 - accuracy: 0.9788 - val_loss: 0.2330 - val_accuracy: 0.8936\n",
      "Epoch 71/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0653 - accuracy: 0.9694 - val_loss: 0.2351 - val_accuracy: 0.8830\n",
      "Epoch 72/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0594 - accuracy: 0.9753 - val_loss: 0.2270 - val_accuracy: 0.8830\n",
      "Epoch 73/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0423 - accuracy: 0.9823 - val_loss: 0.2298 - val_accuracy: 0.8617\n",
      "Epoch 74/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0811 - accuracy: 0.9682 - val_loss: 0.2225 - val_accuracy: 0.8617\n",
      "Epoch 75/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0926 - accuracy: 0.9564 - val_loss: 0.2373 - val_accuracy: 0.8404\n",
      "Epoch 76/100\n",
      "213/213 [==============================] - 21s 96ms/step - loss: 0.0816 - accuracy: 0.9635 - val_loss: 0.2386 - val_accuracy: 0.8511\n",
      "Epoch 77/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0683 - accuracy: 0.9658 - val_loss: 0.2626 - val_accuracy: 0.8617\n",
      "Epoch 78/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0744 - accuracy: 0.9658 - val_loss: 0.2339 - val_accuracy: 0.8404\n",
      "Epoch 79/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0694 - accuracy: 0.9706 - val_loss: 0.2353 - val_accuracy: 0.8723\n",
      "Epoch 80/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0489 - accuracy: 0.9823 - val_loss: 0.2362 - val_accuracy: 0.8617\n",
      "Epoch 81/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0698 - accuracy: 0.9670 - val_loss: 0.2351 - val_accuracy: 0.8936\n",
      "Epoch 82/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0678 - accuracy: 0.9706 - val_loss: 0.2377 - val_accuracy: 0.8723\n",
      "Epoch 83/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0532 - accuracy: 0.9800 - val_loss: 0.2383 - val_accuracy: 0.8404\n",
      "Epoch 84/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0351 - accuracy: 0.9906 - val_loss: 0.2320 - val_accuracy: 0.8723\n",
      "Epoch 85/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0609 - accuracy: 0.9694 - val_loss: 0.2654 - val_accuracy: 0.8617\n",
      "Epoch 86/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0319 - accuracy: 0.9906 - val_loss: 0.2390 - val_accuracy: 0.8936\n",
      "Epoch 87/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0692 - accuracy: 0.9706 - val_loss: 0.2399 - val_accuracy: 0.8511\n",
      "Epoch 88/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0620 - accuracy: 0.9741 - val_loss: 0.2449 - val_accuracy: 0.8404\n",
      "Epoch 89/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0716 - accuracy: 0.9694 - val_loss: 0.2487 - val_accuracy: 0.8617\n",
      "Epoch 90/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0611 - accuracy: 0.9753 - val_loss: 0.2465 - val_accuracy: 0.8723\n",
      "Epoch 91/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0362 - accuracy: 0.9882 - val_loss: 0.2414 - val_accuracy: 0.8723\n",
      "Epoch 92/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0377 - accuracy: 0.9823 - val_loss: 0.2742 - val_accuracy: 0.8404\n",
      "Epoch 93/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0545 - accuracy: 0.9776 - val_loss: 0.2392 - val_accuracy: 0.8617\n",
      "Epoch 94/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0676 - accuracy: 0.9694 - val_loss: 0.2471 - val_accuracy: 0.8830\n",
      "Epoch 95/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0586 - accuracy: 0.9776 - val_loss: 0.2349 - val_accuracy: 0.8617\n",
      "Epoch 96/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0431 - accuracy: 0.9812 - val_loss: 0.2383 - val_accuracy: 0.8617\n",
      "Epoch 97/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0547 - accuracy: 0.9812 - val_loss: 0.2962 - val_accuracy: 0.8298\n",
      "Epoch 98/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0534 - accuracy: 0.9800 - val_loss: 0.2276 - val_accuracy: 0.8617\n",
      "Epoch 99/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0451 - accuracy: 0.9847 - val_loss: 0.2447 - val_accuracy: 0.8830\n",
      "Epoch 100/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0381 - accuracy: 0.9894 - val_loss: 0.2368 - val_accuracy: 0.8830\n",
      "Score for fold 6: loss of 0.23681634664535522; accuracy of 88.29787373542786%\n",
      "(849,) (94,)\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 7 ...\n",
      "Epoch 1/100\n",
      "213/213 [==============================] - 26s 101ms/step - loss: 0.5918 - accuracy: 0.4841 - val_loss: 0.6489 - val_accuracy: 0.3404\n",
      "Epoch 2/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.4645 - accuracy: 0.6408 - val_loss: 21.2757 - val_accuracy: 0.3404\n",
      "Epoch 3/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.4479 - accuracy: 0.6655 - val_loss: 0.5496 - val_accuracy: 0.4468\n",
      "Epoch 4/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.3934 - accuracy: 0.7279 - val_loss: 0.8292 - val_accuracy: 0.3830\n",
      "Epoch 5/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.3891 - accuracy: 0.7232 - val_loss: 1.3487 - val_accuracy: 0.3617\n",
      "Epoch 6/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.3897 - accuracy: 0.7279 - val_loss: 2.5505 - val_accuracy: 0.4255\n",
      "Epoch 7/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.3197 - accuracy: 0.7868 - val_loss: 0.6228 - val_accuracy: 0.5638\n",
      "Epoch 8/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.3148 - accuracy: 0.8174 - val_loss: 0.9925 - val_accuracy: 0.3830\n",
      "Epoch 9/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.2767 - accuracy: 0.8386 - val_loss: 0.4658 - val_accuracy: 0.7021\n",
      "Epoch 10/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.2418 - accuracy: 0.8634 - val_loss: 4.7257 - val_accuracy: 0.3830\n",
      "Epoch 11/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.2350 - accuracy: 0.8598 - val_loss: 1.6139 - val_accuracy: 0.3298\n",
      "Epoch 12/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.2173 - accuracy: 0.8940 - val_loss: 3.1983 - val_accuracy: 0.3191\n",
      "Epoch 13/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.1862 - accuracy: 0.9022 - val_loss: 14.9745 - val_accuracy: 0.3191\n",
      "Epoch 14/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.1915 - accuracy: 0.9046 - val_loss: 0.3637 - val_accuracy: 0.7447\n",
      "Epoch 15/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.1699 - accuracy: 0.9164 - val_loss: 8.1842 - val_accuracy: 0.3191\n",
      "Epoch 16/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.1629 - accuracy: 0.9246 - val_loss: 0.6450 - val_accuracy: 0.7021\n",
      "Epoch 17/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.1117 - accuracy: 0.9552 - val_loss: 0.8924 - val_accuracy: 0.6809\n",
      "Epoch 18/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.1379 - accuracy: 0.9340 - val_loss: 0.8660 - val_accuracy: 0.6809\n",
      "Epoch 19/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.1631 - accuracy: 0.9270 - val_loss: 1.3951 - val_accuracy: 0.6702\n",
      "Epoch 20/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.1332 - accuracy: 0.9388 - val_loss: 0.4199 - val_accuracy: 0.7766\n",
      "Epoch 21/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.1308 - accuracy: 0.9470 - val_loss: 0.3990 - val_accuracy: 0.7340\n",
      "Epoch 22/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.1218 - accuracy: 0.9517 - val_loss: 0.8044 - val_accuracy: 0.7340\n",
      "Epoch 23/100\n",
      "213/213 [==============================] - 21s 96ms/step - loss: 0.0973 - accuracy: 0.9611 - val_loss: 0.5490 - val_accuracy: 0.7660\n",
      "Epoch 24/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.1216 - accuracy: 0.9411 - val_loss: 4.2251 - val_accuracy: 0.3191\n",
      "Epoch 25/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.1159 - accuracy: 0.9376 - val_loss: 0.7726 - val_accuracy: 0.6383\n",
      "Epoch 26/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.1222 - accuracy: 0.9494 - val_loss: 0.3124 - val_accuracy: 0.7979\n",
      "Epoch 27/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0792 - accuracy: 0.9741 - val_loss: 3.7528 - val_accuracy: 0.3191\n",
      "Epoch 28/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0739 - accuracy: 0.9741 - val_loss: 1.1660 - val_accuracy: 0.5638\n",
      "Epoch 29/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.1130 - accuracy: 0.9541 - val_loss: 1.6854 - val_accuracy: 0.5957\n",
      "Epoch 30/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.1132 - accuracy: 0.9564 - val_loss: 0.3531 - val_accuracy: 0.7979\n",
      "Epoch 31/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.1165 - accuracy: 0.9564 - val_loss: 0.4382 - val_accuracy: 0.7553\n",
      "Epoch 32/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0927 - accuracy: 0.9658 - val_loss: 0.4243 - val_accuracy: 0.8085\n",
      "Epoch 33/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0897 - accuracy: 0.9647 - val_loss: 0.2856 - val_accuracy: 0.8191\n",
      "Epoch 34/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0671 - accuracy: 0.9741 - val_loss: 0.3003 - val_accuracy: 0.8298\n",
      "Epoch 35/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0754 - accuracy: 0.9729 - val_loss: 0.6496 - val_accuracy: 0.7021\n",
      "Epoch 36/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0812 - accuracy: 0.9623 - val_loss: 0.3056 - val_accuracy: 0.8723\n",
      "Epoch 37/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0923 - accuracy: 0.9647 - val_loss: 0.3576 - val_accuracy: 0.8085\n",
      "Epoch 38/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0815 - accuracy: 0.9694 - val_loss: 0.5802 - val_accuracy: 0.7128\n",
      "Epoch 39/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.1028 - accuracy: 0.9600 - val_loss: 0.3742 - val_accuracy: 0.7766\n",
      "Epoch 40/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0578 - accuracy: 0.9823 - val_loss: 0.3338 - val_accuracy: 0.8085\n",
      "Epoch 41/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0884 - accuracy: 0.9647 - val_loss: 0.3769 - val_accuracy: 0.8085\n",
      "Epoch 42/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0815 - accuracy: 0.9694 - val_loss: 0.4537 - val_accuracy: 0.7660\n",
      "Epoch 43/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0900 - accuracy: 0.9541 - val_loss: 0.2696 - val_accuracy: 0.8511\n",
      "Epoch 44/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0605 - accuracy: 0.9753 - val_loss: 0.2841 - val_accuracy: 0.8404\n",
      "Epoch 45/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0716 - accuracy: 0.9788 - val_loss: 0.2379 - val_accuracy: 0.8617\n",
      "Epoch 46/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0597 - accuracy: 0.9764 - val_loss: 0.2506 - val_accuracy: 0.8511\n",
      "Epoch 47/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0555 - accuracy: 0.9823 - val_loss: 0.3774 - val_accuracy: 0.7766\n",
      "Epoch 48/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0689 - accuracy: 0.9753 - val_loss: 0.3368 - val_accuracy: 0.7979\n",
      "Epoch 49/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0757 - accuracy: 0.9706 - val_loss: 0.7665 - val_accuracy: 0.6702\n",
      "Epoch 50/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0621 - accuracy: 0.9741 - val_loss: 0.2491 - val_accuracy: 0.8511\n",
      "Epoch 51/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0617 - accuracy: 0.9823 - val_loss: 0.2980 - val_accuracy: 0.8298\n",
      "Epoch 52/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0771 - accuracy: 0.9682 - val_loss: 0.2509 - val_accuracy: 0.8617\n",
      "Epoch 53/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0511 - accuracy: 0.9859 - val_loss: 0.3322 - val_accuracy: 0.8085\n",
      "Epoch 54/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0786 - accuracy: 0.9741 - val_loss: 0.2743 - val_accuracy: 0.8298\n",
      "Epoch 55/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0604 - accuracy: 0.9800 - val_loss: 0.4240 - val_accuracy: 0.7660\n",
      "Epoch 56/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0864 - accuracy: 0.9611 - val_loss: 0.3153 - val_accuracy: 0.8191\n",
      "Epoch 57/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0651 - accuracy: 0.9741 - val_loss: 0.3239 - val_accuracy: 0.7979\n",
      "Epoch 58/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0828 - accuracy: 0.9647 - val_loss: 0.2273 - val_accuracy: 0.8511\n",
      "Epoch 59/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0859 - accuracy: 0.9623 - val_loss: 0.3001 - val_accuracy: 0.8298\n",
      "Epoch 60/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0526 - accuracy: 0.9812 - val_loss: 0.2437 - val_accuracy: 0.8617\n",
      "Epoch 61/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0376 - accuracy: 0.9918 - val_loss: 0.2672 - val_accuracy: 0.8404\n",
      "Epoch 62/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0577 - accuracy: 0.9823 - val_loss: 0.3947 - val_accuracy: 0.7766\n",
      "Epoch 63/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0708 - accuracy: 0.9776 - val_loss: 0.2902 - val_accuracy: 0.8085\n",
      "Epoch 64/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0865 - accuracy: 0.9647 - val_loss: 0.2669 - val_accuracy: 0.8191\n",
      "Epoch 65/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0545 - accuracy: 0.9776 - val_loss: 0.2614 - val_accuracy: 0.8617\n",
      "Epoch 66/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0690 - accuracy: 0.9729 - val_loss: 0.3303 - val_accuracy: 0.7872\n",
      "Epoch 67/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0394 - accuracy: 0.9870 - val_loss: 0.2520 - val_accuracy: 0.8617\n",
      "Epoch 68/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0597 - accuracy: 0.9812 - val_loss: 0.2650 - val_accuracy: 0.8511\n",
      "Epoch 69/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0492 - accuracy: 0.9823 - val_loss: 0.2601 - val_accuracy: 0.8404\n",
      "Epoch 70/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0742 - accuracy: 0.9623 - val_loss: 0.2441 - val_accuracy: 0.8617\n",
      "Epoch 71/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0780 - accuracy: 0.9682 - val_loss: 0.3384 - val_accuracy: 0.7979\n",
      "Epoch 72/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0585 - accuracy: 0.9800 - val_loss: 0.2737 - val_accuracy: 0.8404\n",
      "Epoch 73/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0682 - accuracy: 0.9717 - val_loss: 0.2605 - val_accuracy: 0.8404\n",
      "Epoch 74/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0482 - accuracy: 0.9812 - val_loss: 0.2572 - val_accuracy: 0.8404\n",
      "Epoch 75/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0589 - accuracy: 0.9764 - val_loss: 0.2762 - val_accuracy: 0.8298\n",
      "Epoch 76/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0511 - accuracy: 0.9776 - val_loss: 0.2556 - val_accuracy: 0.8511\n",
      "Epoch 77/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0697 - accuracy: 0.9694 - val_loss: 0.2394 - val_accuracy: 0.8511\n",
      "Epoch 78/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0837 - accuracy: 0.9647 - val_loss: 0.2692 - val_accuracy: 0.8404\n",
      "Epoch 79/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0484 - accuracy: 0.9800 - val_loss: 0.4299 - val_accuracy: 0.7979\n",
      "Epoch 80/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0726 - accuracy: 0.9658 - val_loss: 0.2772 - val_accuracy: 0.8298\n",
      "Epoch 81/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0427 - accuracy: 0.9847 - val_loss: 0.2581 - val_accuracy: 0.8511\n",
      "Epoch 82/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0979 - accuracy: 0.9529 - val_loss: 0.2541 - val_accuracy: 0.8404\n",
      "Epoch 83/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0567 - accuracy: 0.9764 - val_loss: 0.2741 - val_accuracy: 0.8298\n",
      "Epoch 84/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0510 - accuracy: 0.9812 - val_loss: 0.2448 - val_accuracy: 0.8617\n",
      "Epoch 85/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0374 - accuracy: 0.9870 - val_loss: 0.2616 - val_accuracy: 0.8404\n",
      "Epoch 86/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0709 - accuracy: 0.9682 - val_loss: 0.2450 - val_accuracy: 0.8723\n",
      "Epoch 87/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0756 - accuracy: 0.9670 - val_loss: 0.2413 - val_accuracy: 0.8723\n",
      "Epoch 88/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0727 - accuracy: 0.9706 - val_loss: 0.2829 - val_accuracy: 0.8404\n",
      "Epoch 89/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0497 - accuracy: 0.9847 - val_loss: 0.2655 - val_accuracy: 0.8298\n",
      "Epoch 90/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0696 - accuracy: 0.9635 - val_loss: 0.3193 - val_accuracy: 0.7979\n",
      "Epoch 91/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0430 - accuracy: 0.9812 - val_loss: 0.3206 - val_accuracy: 0.8085\n",
      "Epoch 92/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0575 - accuracy: 0.9788 - val_loss: 0.2667 - val_accuracy: 0.8404\n",
      "Epoch 93/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0552 - accuracy: 0.9753 - val_loss: 0.3009 - val_accuracy: 0.8511\n",
      "Epoch 94/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0689 - accuracy: 0.9647 - val_loss: 0.2725 - val_accuracy: 0.8404\n",
      "Epoch 95/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0795 - accuracy: 0.9611 - val_loss: 0.2649 - val_accuracy: 0.8511\n",
      "Epoch 96/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0450 - accuracy: 0.9812 - val_loss: 0.2630 - val_accuracy: 0.8511\n",
      "Epoch 97/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0694 - accuracy: 0.9694 - val_loss: 0.2620 - val_accuracy: 0.8404\n",
      "Epoch 98/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0444 - accuracy: 0.9859 - val_loss: 0.2424 - val_accuracy: 0.8511\n",
      "Epoch 99/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0815 - accuracy: 0.9635 - val_loss: 0.2432 - val_accuracy: 0.8723\n",
      "Epoch 100/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0837 - accuracy: 0.9576 - val_loss: 0.2574 - val_accuracy: 0.8404\n",
      "Score for fold 7: loss of 0.25741422176361084; accuracy of 84.04255509376526%\n",
      "(849,) (94,)\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 8 ...\n",
      "Epoch 1/100\n",
      "213/213 [==============================] - 26s 100ms/step - loss: 0.5582 - accuracy: 0.5183 - val_loss: 1.2026 - val_accuracy: 0.3404\n",
      "Epoch 2/100\n",
      "213/213 [==============================] - 21s 96ms/step - loss: 0.5095 - accuracy: 0.5878 - val_loss: 0.7327 - val_accuracy: 0.3298\n",
      "Epoch 3/100\n",
      "213/213 [==============================] - 21s 96ms/step - loss: 0.4479 - accuracy: 0.6620 - val_loss: 0.6147 - val_accuracy: 0.3404\n",
      "Epoch 4/100\n",
      "213/213 [==============================] - 20s 96ms/step - loss: 0.4363 - accuracy: 0.6655 - val_loss: 2.2429 - val_accuracy: 0.3298\n",
      "Epoch 5/100\n",
      "213/213 [==============================] - 21s 96ms/step - loss: 0.4083 - accuracy: 0.6914 - val_loss: 0.7970 - val_accuracy: 0.3936\n",
      "Epoch 6/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.3786 - accuracy: 0.7633 - val_loss: 0.9918 - val_accuracy: 0.6596\n",
      "Epoch 7/100\n",
      "213/213 [==============================] - 21s 96ms/step - loss: 0.3366 - accuracy: 0.7585 - val_loss: 5.0408 - val_accuracy: 0.3298\n",
      "Epoch 8/100\n",
      "213/213 [==============================] - 20s 96ms/step - loss: 0.3238 - accuracy: 0.7903 - val_loss: 0.4393 - val_accuracy: 0.7128\n",
      "Epoch 9/100\n",
      "213/213 [==============================] - 20s 96ms/step - loss: 0.2735 - accuracy: 0.8386 - val_loss: 1.5065 - val_accuracy: 0.3723\n",
      "Epoch 10/100\n",
      "213/213 [==============================] - 20s 96ms/step - loss: 0.2412 - accuracy: 0.8634 - val_loss: 2.1863 - val_accuracy: 0.3298\n",
      "Epoch 11/100\n",
      "213/213 [==============================] - 21s 96ms/step - loss: 0.2412 - accuracy: 0.8645 - val_loss: 0.4231 - val_accuracy: 0.7447\n",
      "Epoch 12/100\n",
      "213/213 [==============================] - 21s 96ms/step - loss: 0.2277 - accuracy: 0.8704 - val_loss: 0.8054 - val_accuracy: 0.6702\n",
      "Epoch 13/100\n",
      "213/213 [==============================] - 21s 96ms/step - loss: 0.2155 - accuracy: 0.8834 - val_loss: 3.3441 - val_accuracy: 0.3298\n",
      "Epoch 14/100\n",
      "213/213 [==============================] - 20s 96ms/step - loss: 0.1722 - accuracy: 0.9105 - val_loss: 0.5047 - val_accuracy: 0.7128\n",
      "Epoch 15/100\n",
      "213/213 [==============================] - 20s 96ms/step - loss: 0.1748 - accuracy: 0.9152 - val_loss: 2.3788 - val_accuracy: 0.3617\n",
      "Epoch 16/100\n",
      "213/213 [==============================] - 21s 96ms/step - loss: 0.1458 - accuracy: 0.9411 - val_loss: 0.2513 - val_accuracy: 0.8511\n",
      "Epoch 17/100\n",
      "213/213 [==============================] - 21s 96ms/step - loss: 0.1407 - accuracy: 0.9246 - val_loss: 1.6577 - val_accuracy: 0.3617\n",
      "Epoch 18/100\n",
      "213/213 [==============================] - 20s 96ms/step - loss: 0.1637 - accuracy: 0.9176 - val_loss: 2.3552 - val_accuracy: 0.5426\n",
      "Epoch 19/100\n",
      "213/213 [==============================] - 21s 96ms/step - loss: 0.1419 - accuracy: 0.9388 - val_loss: 1.6639 - val_accuracy: 0.6489\n",
      "Epoch 20/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.1629 - accuracy: 0.9234 - val_loss: 1.1825 - val_accuracy: 0.3830\n",
      "Epoch 21/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0947 - accuracy: 0.9588 - val_loss: 0.4008 - val_accuracy: 0.7979\n",
      "Epoch 22/100\n",
      "213/213 [==============================] - 21s 96ms/step - loss: 0.1239 - accuracy: 0.9458 - val_loss: 1.0278 - val_accuracy: 0.4894\n",
      "Epoch 23/100\n",
      "213/213 [==============================] - 21s 96ms/step - loss: 0.1072 - accuracy: 0.9482 - val_loss: 1.0589 - val_accuracy: 0.6277\n",
      "Epoch 24/100\n",
      "213/213 [==============================] - 21s 96ms/step - loss: 0.1092 - accuracy: 0.9505 - val_loss: 0.2736 - val_accuracy: 0.8298\n",
      "Epoch 25/100\n",
      "213/213 [==============================] - 20s 96ms/step - loss: 0.1092 - accuracy: 0.9600 - val_loss: 0.3613 - val_accuracy: 0.7872\n",
      "Epoch 26/100\n",
      "213/213 [==============================] - 20s 96ms/step - loss: 0.0878 - accuracy: 0.9611 - val_loss: 0.3464 - val_accuracy: 0.8191\n",
      "Epoch 27/100\n",
      "213/213 [==============================] - 21s 96ms/step - loss: 0.0931 - accuracy: 0.9694 - val_loss: 0.6618 - val_accuracy: 0.6702\n",
      "Epoch 28/100\n",
      "213/213 [==============================] - 21s 96ms/step - loss: 0.0971 - accuracy: 0.9717 - val_loss: 0.4269 - val_accuracy: 0.7979\n",
      "Epoch 29/100\n",
      "213/213 [==============================] - 21s 96ms/step - loss: 0.0734 - accuracy: 0.9812 - val_loss: 0.3581 - val_accuracy: 0.8617\n",
      "Epoch 30/100\n",
      "213/213 [==============================] - 21s 96ms/step - loss: 0.1119 - accuracy: 0.9552 - val_loss: 0.3898 - val_accuracy: 0.7979\n",
      "Epoch 31/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0633 - accuracy: 0.9753 - val_loss: 0.3084 - val_accuracy: 0.8191\n",
      "Epoch 32/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.1030 - accuracy: 0.9564 - val_loss: 0.5481 - val_accuracy: 0.7660\n",
      "Epoch 33/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0791 - accuracy: 0.9729 - val_loss: 0.3088 - val_accuracy: 0.8511\n",
      "Epoch 34/100\n",
      "213/213 [==============================] - 21s 96ms/step - loss: 0.0643 - accuracy: 0.9835 - val_loss: 0.4049 - val_accuracy: 0.7660\n",
      "Epoch 35/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0991 - accuracy: 0.9588 - val_loss: 0.4661 - val_accuracy: 0.7447\n",
      "Epoch 36/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0587 - accuracy: 0.9788 - val_loss: 0.2197 - val_accuracy: 0.8511\n",
      "Epoch 37/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0559 - accuracy: 0.9823 - val_loss: 0.2543 - val_accuracy: 0.8723\n",
      "Epoch 38/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0665 - accuracy: 0.9776 - val_loss: 0.2467 - val_accuracy: 0.8723\n",
      "Epoch 39/100\n",
      "213/213 [==============================] - 21s 96ms/step - loss: 0.0589 - accuracy: 0.9764 - val_loss: 0.3181 - val_accuracy: 0.8404\n",
      "Epoch 40/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0810 - accuracy: 0.9717 - val_loss: 0.4100 - val_accuracy: 0.8085\n",
      "Epoch 41/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0538 - accuracy: 0.9835 - val_loss: 0.8566 - val_accuracy: 0.6383\n",
      "Epoch 42/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0697 - accuracy: 0.9764 - val_loss: 0.3752 - val_accuracy: 0.8085\n",
      "Epoch 43/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0513 - accuracy: 0.9847 - val_loss: 0.2633 - val_accuracy: 0.8830\n",
      "Epoch 44/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0603 - accuracy: 0.9788 - val_loss: 0.3358 - val_accuracy: 0.8298\n",
      "Epoch 45/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0852 - accuracy: 0.9611 - val_loss: 0.2723 - val_accuracy: 0.8511\n",
      "Epoch 46/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0704 - accuracy: 0.9706 - val_loss: 0.2625 - val_accuracy: 0.8617\n",
      "Epoch 47/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0555 - accuracy: 0.9812 - val_loss: 0.3446 - val_accuracy: 0.8191\n",
      "Epoch 48/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0852 - accuracy: 0.9670 - val_loss: 0.2630 - val_accuracy: 0.8404\n",
      "Epoch 49/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0421 - accuracy: 0.9882 - val_loss: 0.2442 - val_accuracy: 0.8511\n",
      "Epoch 50/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0541 - accuracy: 0.9812 - val_loss: 0.3468 - val_accuracy: 0.8191\n",
      "Epoch 51/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0811 - accuracy: 0.9658 - val_loss: 0.2331 - val_accuracy: 0.8511\n",
      "Epoch 52/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0825 - accuracy: 0.9658 - val_loss: 0.2346 - val_accuracy: 0.8298\n",
      "Epoch 53/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0554 - accuracy: 0.9823 - val_loss: 0.2908 - val_accuracy: 0.8511\n",
      "Epoch 54/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0859 - accuracy: 0.9623 - val_loss: 0.4047 - val_accuracy: 0.8191\n",
      "Epoch 55/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0336 - accuracy: 0.9906 - val_loss: 0.3452 - val_accuracy: 0.8617\n",
      "Epoch 56/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0565 - accuracy: 0.9800 - val_loss: 0.2885 - val_accuracy: 0.8191\n",
      "Epoch 57/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0701 - accuracy: 0.9729 - val_loss: 0.3440 - val_accuracy: 0.8617\n",
      "Epoch 58/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0309 - accuracy: 0.9918 - val_loss: 0.2946 - val_accuracy: 0.8723\n",
      "Epoch 59/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0620 - accuracy: 0.9753 - val_loss: 0.3147 - val_accuracy: 0.8723\n",
      "Epoch 60/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0335 - accuracy: 0.9906 - val_loss: 0.2894 - val_accuracy: 0.8511\n",
      "Epoch 61/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0633 - accuracy: 0.9753 - val_loss: 0.2800 - val_accuracy: 0.8511\n",
      "Epoch 62/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0711 - accuracy: 0.9717 - val_loss: 0.3020 - val_accuracy: 0.8511\n",
      "Epoch 63/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0924 - accuracy: 0.9611 - val_loss: 0.2989 - val_accuracy: 0.8723\n",
      "Epoch 64/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0364 - accuracy: 0.9929 - val_loss: 0.2426 - val_accuracy: 0.8617\n",
      "Epoch 65/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0672 - accuracy: 0.9658 - val_loss: 0.3142 - val_accuracy: 0.8617\n",
      "Epoch 66/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0425 - accuracy: 0.9835 - val_loss: 0.2701 - val_accuracy: 0.8511\n",
      "Epoch 67/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0557 - accuracy: 0.9812 - val_loss: 0.2690 - val_accuracy: 0.8723\n",
      "Epoch 68/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0606 - accuracy: 0.9812 - val_loss: 0.2939 - val_accuracy: 0.8511\n",
      "Epoch 69/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0543 - accuracy: 0.9812 - val_loss: 0.2923 - val_accuracy: 0.8830\n",
      "Epoch 70/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0365 - accuracy: 0.9882 - val_loss: 0.2506 - val_accuracy: 0.8617\n",
      "Epoch 71/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0723 - accuracy: 0.9623 - val_loss: 0.3687 - val_accuracy: 0.8511\n",
      "Epoch 72/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0772 - accuracy: 0.9658 - val_loss: 0.3160 - val_accuracy: 0.8085\n",
      "Epoch 73/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0645 - accuracy: 0.9753 - val_loss: 0.2931 - val_accuracy: 0.8617\n",
      "Epoch 74/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0688 - accuracy: 0.9717 - val_loss: 0.3183 - val_accuracy: 0.8723\n",
      "Epoch 75/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0804 - accuracy: 0.9682 - val_loss: 0.2421 - val_accuracy: 0.8617\n",
      "Epoch 76/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0764 - accuracy: 0.9694 - val_loss: 0.3350 - val_accuracy: 0.8404\n",
      "Epoch 77/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0495 - accuracy: 0.9800 - val_loss: 0.3191 - val_accuracy: 0.8298\n",
      "Epoch 78/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0778 - accuracy: 0.9611 - val_loss: 0.5310 - val_accuracy: 0.8191\n",
      "Epoch 79/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0521 - accuracy: 0.9823 - val_loss: 0.3362 - val_accuracy: 0.8511\n",
      "Epoch 80/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0745 - accuracy: 0.9729 - val_loss: 0.2627 - val_accuracy: 0.8404\n",
      "Epoch 81/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0704 - accuracy: 0.9670 - val_loss: 0.2578 - val_accuracy: 0.8511\n",
      "Epoch 82/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0656 - accuracy: 0.9753 - val_loss: 0.2762 - val_accuracy: 0.8404\n",
      "Epoch 83/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0508 - accuracy: 0.9788 - val_loss: 0.2587 - val_accuracy: 0.8511\n",
      "Epoch 84/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0554 - accuracy: 0.9764 - val_loss: 0.2850 - val_accuracy: 0.8617\n",
      "Epoch 85/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0508 - accuracy: 0.9812 - val_loss: 0.2515 - val_accuracy: 0.8617\n",
      "Epoch 86/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0462 - accuracy: 0.9823 - val_loss: 0.2818 - val_accuracy: 0.8511\n",
      "Epoch 87/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0265 - accuracy: 0.9941 - val_loss: 0.2929 - val_accuracy: 0.8404\n",
      "Epoch 88/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0641 - accuracy: 0.9717 - val_loss: 0.2606 - val_accuracy: 0.8617\n",
      "Epoch 89/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0879 - accuracy: 0.9623 - val_loss: 0.3096 - val_accuracy: 0.8404\n",
      "Epoch 90/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0561 - accuracy: 0.9764 - val_loss: 0.2649 - val_accuracy: 0.8511\n",
      "Epoch 91/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0304 - accuracy: 0.9941 - val_loss: 0.2721 - val_accuracy: 0.8404\n",
      "Epoch 92/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0869 - accuracy: 0.9611 - val_loss: 0.2485 - val_accuracy: 0.8511\n",
      "Epoch 93/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0462 - accuracy: 0.9859 - val_loss: 0.3371 - val_accuracy: 0.8404\n",
      "Epoch 94/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0612 - accuracy: 0.9729 - val_loss: 0.3134 - val_accuracy: 0.8511\n",
      "Epoch 95/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0477 - accuracy: 0.9800 - val_loss: 0.3266 - val_accuracy: 0.8085\n",
      "Epoch 96/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0565 - accuracy: 0.9741 - val_loss: 0.2584 - val_accuracy: 0.8404\n",
      "Epoch 97/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0515 - accuracy: 0.9788 - val_loss: 0.2905 - val_accuracy: 0.8298\n",
      "Epoch 98/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0649 - accuracy: 0.9729 - val_loss: 0.2788 - val_accuracy: 0.8511\n",
      "Epoch 99/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0616 - accuracy: 0.9741 - val_loss: 0.2789 - val_accuracy: 0.8404\n",
      "Epoch 100/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0786 - accuracy: 0.9600 - val_loss: 0.2975 - val_accuracy: 0.8191\n",
      "Score for fold 8: loss of 0.29751789569854736; accuracy of 81.91489577293396%\n",
      "(849,) (94,)\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 9 ...\n",
      "Epoch 1/100\n",
      "213/213 [==============================] - 26s 102ms/step - loss: 0.5829 - accuracy: 0.4770 - val_loss: 1.1214 - val_accuracy: 0.3404\n",
      "Epoch 2/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.4897 - accuracy: 0.6019 - val_loss: 2.5478 - val_accuracy: 0.3298\n",
      "Epoch 3/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.4502 - accuracy: 0.6631 - val_loss: 21.6003 - val_accuracy: 0.3298\n",
      "Epoch 4/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.4340 - accuracy: 0.6678 - val_loss: 2.2600 - val_accuracy: 0.3404\n",
      "Epoch 5/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.4126 - accuracy: 0.6690 - val_loss: 0.2955 - val_accuracy: 0.7447\n",
      "Epoch 6/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.3760 - accuracy: 0.7409 - val_loss: 3.5005 - val_accuracy: 0.3298\n",
      "Epoch 7/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.3536 - accuracy: 0.7680 - val_loss: 1.4830 - val_accuracy: 0.5957\n",
      "Epoch 8/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.3019 - accuracy: 0.8163 - val_loss: 2.5514 - val_accuracy: 0.4468\n",
      "Epoch 9/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.3170 - accuracy: 0.8021 - val_loss: 0.2009 - val_accuracy: 0.8617\n",
      "Epoch 10/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.2774 - accuracy: 0.8492 - val_loss: 0.9575 - val_accuracy: 0.6383\n",
      "Epoch 11/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.2738 - accuracy: 0.8445 - val_loss: 1.1871 - val_accuracy: 0.6596\n",
      "Epoch 12/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.2385 - accuracy: 0.8634 - val_loss: 2.8653 - val_accuracy: 0.3617\n",
      "Epoch 13/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.2297 - accuracy: 0.8539 - val_loss: 12.5467 - val_accuracy: 0.3298\n",
      "Epoch 14/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.2047 - accuracy: 0.8893 - val_loss: 2.3722 - val_accuracy: 0.4787\n",
      "Epoch 15/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.1952 - accuracy: 0.8881 - val_loss: 8.6202 - val_accuracy: 0.3298\n",
      "Epoch 16/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.1721 - accuracy: 0.9058 - val_loss: 2.1872 - val_accuracy: 0.4043\n",
      "Epoch 17/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.1742 - accuracy: 0.9011 - val_loss: 1.6011 - val_accuracy: 0.5957\n",
      "Epoch 18/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.2040 - accuracy: 0.8799 - val_loss: 1.6078 - val_accuracy: 0.6170\n",
      "Epoch 19/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.1398 - accuracy: 0.9388 - val_loss: 0.3359 - val_accuracy: 0.8830\n",
      "Epoch 20/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.1353 - accuracy: 0.9388 - val_loss: 3.6697 - val_accuracy: 0.3404\n",
      "Epoch 21/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0980 - accuracy: 0.9552 - val_loss: 4.9816 - val_accuracy: 0.3830\n",
      "Epoch 22/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.1191 - accuracy: 0.9494 - val_loss: 2.1017 - val_accuracy: 0.5426\n",
      "Epoch 23/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.1131 - accuracy: 0.9494 - val_loss: 30.7146 - val_accuracy: 0.3298\n",
      "Epoch 24/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.1258 - accuracy: 0.9411 - val_loss: 1.0973 - val_accuracy: 0.6170\n",
      "Epoch 25/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.1233 - accuracy: 0.9482 - val_loss: 0.5401 - val_accuracy: 0.7553\n",
      "Epoch 26/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0943 - accuracy: 0.9623 - val_loss: 26.9557 - val_accuracy: 0.3298\n",
      "Epoch 27/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.1190 - accuracy: 0.9458 - val_loss: 0.4589 - val_accuracy: 0.8191\n",
      "Epoch 28/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.1078 - accuracy: 0.9600 - val_loss: 2.8857 - val_accuracy: 0.6596\n",
      "Epoch 29/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0977 - accuracy: 0.9576 - val_loss: 0.1773 - val_accuracy: 0.9043\n",
      "Epoch 30/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0885 - accuracy: 0.9694 - val_loss: 0.8378 - val_accuracy: 0.7234\n",
      "Epoch 31/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0859 - accuracy: 0.9694 - val_loss: 1.2943 - val_accuracy: 0.6915\n",
      "Epoch 32/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0882 - accuracy: 0.9694 - val_loss: 1.3111 - val_accuracy: 0.6596\n",
      "Epoch 33/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0987 - accuracy: 0.9600 - val_loss: 0.8470 - val_accuracy: 0.7021\n",
      "Epoch 34/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.1250 - accuracy: 0.9340 - val_loss: 4.3807 - val_accuracy: 0.3404\n",
      "Epoch 35/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0762 - accuracy: 0.9764 - val_loss: 0.3718 - val_accuracy: 0.8617\n",
      "Epoch 36/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0859 - accuracy: 0.9670 - val_loss: 0.2910 - val_accuracy: 0.9043\n",
      "Epoch 37/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0757 - accuracy: 0.9776 - val_loss: 0.6074 - val_accuracy: 0.7340\n",
      "Epoch 38/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0966 - accuracy: 0.9576 - val_loss: 0.2664 - val_accuracy: 0.9043\n",
      "Epoch 39/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0746 - accuracy: 0.9729 - val_loss: 0.2002 - val_accuracy: 0.8617\n",
      "Epoch 40/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0499 - accuracy: 0.9800 - val_loss: 0.3110 - val_accuracy: 0.8936\n",
      "Epoch 41/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0823 - accuracy: 0.9670 - val_loss: 3.2457 - val_accuracy: 0.5319\n",
      "Epoch 42/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0706 - accuracy: 0.9753 - val_loss: 3.2491 - val_accuracy: 0.5000\n",
      "Epoch 43/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0486 - accuracy: 0.9847 - val_loss: 0.8411 - val_accuracy: 0.6702\n",
      "Epoch 44/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0942 - accuracy: 0.9600 - val_loss: 2.6454 - val_accuracy: 0.6277\n",
      "Epoch 45/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0756 - accuracy: 0.9694 - val_loss: 0.1605 - val_accuracy: 0.9149\n",
      "Epoch 46/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0991 - accuracy: 0.9623 - val_loss: 0.6094 - val_accuracy: 0.7234\n",
      "Epoch 47/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0717 - accuracy: 0.9717 - val_loss: 0.2963 - val_accuracy: 0.9043\n",
      "Epoch 48/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0776 - accuracy: 0.9670 - val_loss: 1.9663 - val_accuracy: 0.6809\n",
      "Epoch 49/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0978 - accuracy: 0.9517 - val_loss: 0.1846 - val_accuracy: 0.9043\n",
      "Epoch 50/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0782 - accuracy: 0.9670 - val_loss: 0.1922 - val_accuracy: 0.9149\n",
      "Epoch 51/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0651 - accuracy: 0.9717 - val_loss: 0.1628 - val_accuracy: 0.9043\n",
      "Epoch 52/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0982 - accuracy: 0.9517 - val_loss: 0.7017 - val_accuracy: 0.7128\n",
      "Epoch 53/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0746 - accuracy: 0.9694 - val_loss: 0.3767 - val_accuracy: 0.8617\n",
      "Epoch 54/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0809 - accuracy: 0.9682 - val_loss: 0.2944 - val_accuracy: 0.8936\n",
      "Epoch 55/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0608 - accuracy: 0.9753 - val_loss: 0.2502 - val_accuracy: 0.7979\n",
      "Epoch 56/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0479 - accuracy: 0.9812 - val_loss: 0.1721 - val_accuracy: 0.9149\n",
      "Epoch 57/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0750 - accuracy: 0.9717 - val_loss: 0.2834 - val_accuracy: 0.8830\n",
      "Epoch 58/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0784 - accuracy: 0.9729 - val_loss: 0.1673 - val_accuracy: 0.9043\n",
      "Epoch 59/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0500 - accuracy: 0.9847 - val_loss: 0.1816 - val_accuracy: 0.9255\n",
      "Epoch 60/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0740 - accuracy: 0.9706 - val_loss: 0.2711 - val_accuracy: 0.8191\n",
      "Epoch 61/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0660 - accuracy: 0.9741 - val_loss: 0.1732 - val_accuracy: 0.8830\n",
      "Epoch 62/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0469 - accuracy: 0.9788 - val_loss: 0.3675 - val_accuracy: 0.8617\n",
      "Epoch 63/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0705 - accuracy: 0.9706 - val_loss: 0.1928 - val_accuracy: 0.9255\n",
      "Epoch 64/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0694 - accuracy: 0.9741 - val_loss: 0.2314 - val_accuracy: 0.8936\n",
      "Epoch 65/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0644 - accuracy: 0.9776 - val_loss: 0.1655 - val_accuracy: 0.9149\n",
      "Epoch 66/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0644 - accuracy: 0.9753 - val_loss: 0.1635 - val_accuracy: 0.9255\n",
      "Epoch 67/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0891 - accuracy: 0.9564 - val_loss: 0.3271 - val_accuracy: 0.8830\n",
      "Epoch 68/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0845 - accuracy: 0.9647 - val_loss: 0.2108 - val_accuracy: 0.8830\n",
      "Epoch 69/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0568 - accuracy: 0.9812 - val_loss: 0.3338 - val_accuracy: 0.8830\n",
      "Epoch 70/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0662 - accuracy: 0.9741 - val_loss: 0.2452 - val_accuracy: 0.9043\n",
      "Epoch 71/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0526 - accuracy: 0.9823 - val_loss: 0.1901 - val_accuracy: 0.9149\n",
      "Epoch 72/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0463 - accuracy: 0.9847 - val_loss: 0.3643 - val_accuracy: 0.9043\n",
      "Epoch 73/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0592 - accuracy: 0.9823 - val_loss: 0.1548 - val_accuracy: 0.9043\n",
      "Epoch 74/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0681 - accuracy: 0.9647 - val_loss: 0.1556 - val_accuracy: 0.9043\n",
      "Epoch 75/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0762 - accuracy: 0.9729 - val_loss: 0.2112 - val_accuracy: 0.9043\n",
      "Epoch 76/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0729 - accuracy: 0.9741 - val_loss: 0.1634 - val_accuracy: 0.8936\n",
      "Epoch 77/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0526 - accuracy: 0.9788 - val_loss: 0.1685 - val_accuracy: 0.9043\n",
      "Epoch 78/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0573 - accuracy: 0.9741 - val_loss: 0.1576 - val_accuracy: 0.9149\n",
      "Epoch 79/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0555 - accuracy: 0.9800 - val_loss: 0.1896 - val_accuracy: 0.8936\n",
      "Epoch 80/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0741 - accuracy: 0.9635 - val_loss: 0.2384 - val_accuracy: 0.9043\n",
      "Epoch 81/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0698 - accuracy: 0.9706 - val_loss: 0.2947 - val_accuracy: 0.8085\n",
      "Epoch 82/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0700 - accuracy: 0.9682 - val_loss: 0.2094 - val_accuracy: 0.8830\n",
      "Epoch 83/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0666 - accuracy: 0.9764 - val_loss: 0.1622 - val_accuracy: 0.9255\n",
      "Epoch 84/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0634 - accuracy: 0.9741 - val_loss: 0.2411 - val_accuracy: 0.9043\n",
      "Epoch 85/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0639 - accuracy: 0.9788 - val_loss: 0.1576 - val_accuracy: 0.9149\n",
      "Epoch 86/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0651 - accuracy: 0.9729 - val_loss: 0.4026 - val_accuracy: 0.8404\n",
      "Epoch 87/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0876 - accuracy: 0.9564 - val_loss: 0.2566 - val_accuracy: 0.7872\n",
      "Epoch 88/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0533 - accuracy: 0.9823 - val_loss: 0.1564 - val_accuracy: 0.9149\n",
      "Epoch 89/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0772 - accuracy: 0.9635 - val_loss: 0.2248 - val_accuracy: 0.9043\n",
      "Epoch 90/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0600 - accuracy: 0.9764 - val_loss: 0.2509 - val_accuracy: 0.8191\n",
      "Epoch 91/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0491 - accuracy: 0.9800 - val_loss: 0.1950 - val_accuracy: 0.8830\n",
      "Epoch 92/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0606 - accuracy: 0.9800 - val_loss: 0.1840 - val_accuracy: 0.9043\n",
      "Epoch 93/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0833 - accuracy: 0.9647 - val_loss: 0.2552 - val_accuracy: 0.9043\n",
      "Epoch 94/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0804 - accuracy: 0.9588 - val_loss: 0.2207 - val_accuracy: 0.8936\n",
      "Epoch 95/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0401 - accuracy: 0.9847 - val_loss: 0.2579 - val_accuracy: 0.9043\n",
      "Epoch 96/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0518 - accuracy: 0.9800 - val_loss: 0.2233 - val_accuracy: 0.9043\n",
      "Epoch 97/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0549 - accuracy: 0.9812 - val_loss: 0.1659 - val_accuracy: 0.9043\n",
      "Epoch 98/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0728 - accuracy: 0.9670 - val_loss: 0.1817 - val_accuracy: 0.9043\n",
      "Epoch 99/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0420 - accuracy: 0.9847 - val_loss: 0.2018 - val_accuracy: 0.9043\n",
      "Epoch 100/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0720 - accuracy: 0.9658 - val_loss: 0.1628 - val_accuracy: 0.9149\n",
      "Score for fold 9: loss of 0.16276131570339203; accuracy of 91.4893627166748%\n",
      "(849,) (94,)\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 10 ...\n",
      "Epoch 1/100\n",
      "213/213 [==============================] - 26s 101ms/step - loss: 0.5385 - accuracy: 0.5536 - val_loss: 1.6896 - val_accuracy: 0.3404\n",
      "Epoch 2/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.4991 - accuracy: 0.6172 - val_loss: 0.7739 - val_accuracy: 0.3298\n",
      "Epoch 3/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.4422 - accuracy: 0.6832 - val_loss: 3.6451 - val_accuracy: 0.3404\n",
      "Epoch 4/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.4237 - accuracy: 0.6855 - val_loss: 4.4898 - val_accuracy: 0.3298\n",
      "Epoch 5/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.3514 - accuracy: 0.7444 - val_loss: 2.7918 - val_accuracy: 0.3830\n",
      "Epoch 6/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.3187 - accuracy: 0.8198 - val_loss: 3.3460 - val_accuracy: 0.3404\n",
      "Epoch 7/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.3020 - accuracy: 0.8068 - val_loss: 1.5820 - val_accuracy: 0.3298\n",
      "Epoch 8/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.2900 - accuracy: 0.8151 - val_loss: 1.3030 - val_accuracy: 0.3511\n",
      "Epoch 9/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.2896 - accuracy: 0.8269 - val_loss: 0.5161 - val_accuracy: 0.6809\n",
      "Epoch 10/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.2314 - accuracy: 0.8681 - val_loss: 2.5168 - val_accuracy: 0.3298\n",
      "Epoch 11/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.2482 - accuracy: 0.8481 - val_loss: 1.0281 - val_accuracy: 0.6277\n",
      "Epoch 12/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.2037 - accuracy: 0.8657 - val_loss: 0.9100 - val_accuracy: 0.4574\n",
      "Epoch 13/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.2078 - accuracy: 0.8869 - val_loss: 1.3309 - val_accuracy: 0.5532\n",
      "Epoch 14/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.1888 - accuracy: 0.8905 - val_loss: 0.6623 - val_accuracy: 0.6915\n",
      "Epoch 15/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.1943 - accuracy: 0.8881 - val_loss: 0.6389 - val_accuracy: 0.6915\n",
      "Epoch 16/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.1768 - accuracy: 0.9046 - val_loss: 1.6206 - val_accuracy: 0.5426\n",
      "Epoch 17/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.1584 - accuracy: 0.9223 - val_loss: 0.9035 - val_accuracy: 0.7021\n",
      "Epoch 18/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.1582 - accuracy: 0.9199 - val_loss: 0.4628 - val_accuracy: 0.7340\n",
      "Epoch 19/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.1686 - accuracy: 0.9164 - val_loss: 0.3979 - val_accuracy: 0.8085\n",
      "Epoch 20/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.1601 - accuracy: 0.9270 - val_loss: 0.3123 - val_accuracy: 0.8191\n",
      "Epoch 21/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.1250 - accuracy: 0.9470 - val_loss: 0.5216 - val_accuracy: 0.7872\n",
      "Epoch 22/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.1547 - accuracy: 0.9293 - val_loss: 0.3478 - val_accuracy: 0.7872\n",
      "Epoch 23/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.1498 - accuracy: 0.9317 - val_loss: 0.5270 - val_accuracy: 0.8617\n",
      "Epoch 24/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.1487 - accuracy: 0.9364 - val_loss: 0.3050 - val_accuracy: 0.8511\n",
      "Epoch 25/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.1123 - accuracy: 0.9552 - val_loss: 0.3076 - val_accuracy: 0.7872\n",
      "Epoch 26/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0986 - accuracy: 0.9635 - val_loss: 0.7814 - val_accuracy: 0.7128\n",
      "Epoch 27/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.1214 - accuracy: 0.9529 - val_loss: 0.5087 - val_accuracy: 0.7766\n",
      "Epoch 28/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.1119 - accuracy: 0.9482 - val_loss: 0.3647 - val_accuracy: 0.7979\n",
      "Epoch 29/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0943 - accuracy: 0.9647 - val_loss: 0.2640 - val_accuracy: 0.8617\n",
      "Epoch 30/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0777 - accuracy: 0.9776 - val_loss: 0.4937 - val_accuracy: 0.7766\n",
      "Epoch 31/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.1072 - accuracy: 0.9588 - val_loss: 0.4090 - val_accuracy: 0.8298\n",
      "Epoch 32/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0799 - accuracy: 0.9670 - val_loss: 0.3877 - val_accuracy: 0.8404\n",
      "Epoch 33/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0740 - accuracy: 0.9741 - val_loss: 0.5680 - val_accuracy: 0.7766\n",
      "Epoch 34/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0806 - accuracy: 0.9706 - val_loss: 0.3057 - val_accuracy: 0.8511\n",
      "Epoch 35/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0741 - accuracy: 0.9694 - val_loss: 0.3502 - val_accuracy: 0.8298\n",
      "Epoch 36/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0709 - accuracy: 0.9706 - val_loss: 0.2870 - val_accuracy: 0.8617\n",
      "Epoch 37/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0669 - accuracy: 0.9729 - val_loss: 0.3301 - val_accuracy: 0.8298\n",
      "Epoch 38/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0778 - accuracy: 0.9670 - val_loss: 0.3547 - val_accuracy: 0.8617\n",
      "Epoch 39/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0904 - accuracy: 0.9647 - val_loss: 0.2896 - val_accuracy: 0.8404\n",
      "Epoch 40/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0621 - accuracy: 0.9800 - val_loss: 0.2957 - val_accuracy: 0.8298\n",
      "Epoch 41/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.1054 - accuracy: 0.9529 - val_loss: 0.3287 - val_accuracy: 0.8085\n",
      "Epoch 42/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0765 - accuracy: 0.9694 - val_loss: 0.5005 - val_accuracy: 0.7979\n",
      "Epoch 43/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0903 - accuracy: 0.9576 - val_loss: 0.2623 - val_accuracy: 0.8298\n",
      "Epoch 44/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.1071 - accuracy: 0.9482 - val_loss: 0.2790 - val_accuracy: 0.8617\n",
      "Epoch 45/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0745 - accuracy: 0.9694 - val_loss: 0.2600 - val_accuracy: 0.8404\n",
      "Epoch 46/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0619 - accuracy: 0.9729 - val_loss: 0.2774 - val_accuracy: 0.8723\n",
      "Epoch 47/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0775 - accuracy: 0.9670 - val_loss: 0.3293 - val_accuracy: 0.7872\n",
      "Epoch 48/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0490 - accuracy: 0.9882 - val_loss: 0.3023 - val_accuracy: 0.8511\n",
      "Epoch 49/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0646 - accuracy: 0.9788 - val_loss: 0.3796 - val_accuracy: 0.8404\n",
      "Epoch 50/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0706 - accuracy: 0.9776 - val_loss: 0.3428 - val_accuracy: 0.8298\n",
      "Epoch 51/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0437 - accuracy: 0.9859 - val_loss: 0.2858 - val_accuracy: 0.8617\n",
      "Epoch 52/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0592 - accuracy: 0.9776 - val_loss: 0.2837 - val_accuracy: 0.8511\n",
      "Epoch 53/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0589 - accuracy: 0.9717 - val_loss: 0.2990 - val_accuracy: 0.8617\n",
      "Epoch 54/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0537 - accuracy: 0.9847 - val_loss: 0.3013 - val_accuracy: 0.8617\n",
      "Epoch 55/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0632 - accuracy: 0.9706 - val_loss: 0.3905 - val_accuracy: 0.8191\n",
      "Epoch 56/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0839 - accuracy: 0.9623 - val_loss: 0.3847 - val_accuracy: 0.8404\n",
      "Epoch 57/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0626 - accuracy: 0.9741 - val_loss: 0.2958 - val_accuracy: 0.8723\n",
      "Epoch 58/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0556 - accuracy: 0.9776 - val_loss: 0.2910 - val_accuracy: 0.8617\n",
      "Epoch 59/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0772 - accuracy: 0.9670 - val_loss: 0.2967 - val_accuracy: 0.8617\n",
      "Epoch 60/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0390 - accuracy: 0.9870 - val_loss: 0.2832 - val_accuracy: 0.8723\n",
      "Epoch 61/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0734 - accuracy: 0.9611 - val_loss: 0.3137 - val_accuracy: 0.8723\n",
      "Epoch 62/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0617 - accuracy: 0.9764 - val_loss: 0.2870 - val_accuracy: 0.8617\n",
      "Epoch 63/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0650 - accuracy: 0.9717 - val_loss: 0.3232 - val_accuracy: 0.8723\n",
      "Epoch 64/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0620 - accuracy: 0.9729 - val_loss: 0.3047 - val_accuracy: 0.8617\n",
      "Epoch 65/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0645 - accuracy: 0.9717 - val_loss: 0.3006 - val_accuracy: 0.8723\n",
      "Epoch 66/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0946 - accuracy: 0.9552 - val_loss: 0.2898 - val_accuracy: 0.8511\n",
      "Epoch 67/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0532 - accuracy: 0.9788 - val_loss: 0.3053 - val_accuracy: 0.8404\n",
      "Epoch 68/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0730 - accuracy: 0.9682 - val_loss: 0.2832 - val_accuracy: 0.8723\n",
      "Epoch 69/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0471 - accuracy: 0.9800 - val_loss: 0.2972 - val_accuracy: 0.8830\n",
      "Epoch 70/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0574 - accuracy: 0.9706 - val_loss: 0.3336 - val_accuracy: 0.8723\n",
      "Epoch 71/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0539 - accuracy: 0.9764 - val_loss: 0.2943 - val_accuracy: 0.8511\n",
      "Epoch 72/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0812 - accuracy: 0.9623 - val_loss: 0.3160 - val_accuracy: 0.8511\n",
      "Epoch 73/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0925 - accuracy: 0.9564 - val_loss: 0.3992 - val_accuracy: 0.8085\n",
      "Epoch 74/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0682 - accuracy: 0.9694 - val_loss: 0.4008 - val_accuracy: 0.8085\n",
      "Epoch 75/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0781 - accuracy: 0.9576 - val_loss: 0.3159 - val_accuracy: 0.8511\n",
      "Epoch 76/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0718 - accuracy: 0.9682 - val_loss: 0.2940 - val_accuracy: 0.8723\n",
      "Epoch 77/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0592 - accuracy: 0.9753 - val_loss: 0.3043 - val_accuracy: 0.8723\n",
      "Epoch 78/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0560 - accuracy: 0.9776 - val_loss: 0.2969 - val_accuracy: 0.8617\n",
      "Epoch 79/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0664 - accuracy: 0.9741 - val_loss: 0.3311 - val_accuracy: 0.8617\n",
      "Epoch 80/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0626 - accuracy: 0.9670 - val_loss: 0.3105 - val_accuracy: 0.8723\n",
      "Epoch 81/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0513 - accuracy: 0.9812 - val_loss: 0.3019 - val_accuracy: 0.8723\n",
      "Epoch 82/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0572 - accuracy: 0.9717 - val_loss: 0.3464 - val_accuracy: 0.8511\n",
      "Epoch 83/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0713 - accuracy: 0.9706 - val_loss: 0.3000 - val_accuracy: 0.8617\n",
      "Epoch 84/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0432 - accuracy: 0.9847 - val_loss: 0.2992 - val_accuracy: 0.8723\n",
      "Epoch 85/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0282 - accuracy: 0.9906 - val_loss: 0.3162 - val_accuracy: 0.8617\n",
      "Epoch 86/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0624 - accuracy: 0.9753 - val_loss: 0.3127 - val_accuracy: 0.8511\n",
      "Epoch 87/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0558 - accuracy: 0.9764 - val_loss: 0.3220 - val_accuracy: 0.8617\n",
      "Epoch 88/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0576 - accuracy: 0.9776 - val_loss: 0.3057 - val_accuracy: 0.8617\n",
      "Epoch 89/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0330 - accuracy: 0.9870 - val_loss: 0.3135 - val_accuracy: 0.8936\n",
      "Epoch 90/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0399 - accuracy: 0.9870 - val_loss: 0.3623 - val_accuracy: 0.8617\n",
      "Epoch 91/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0602 - accuracy: 0.9717 - val_loss: 0.3339 - val_accuracy: 0.8511\n",
      "Epoch 92/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0318 - accuracy: 0.9859 - val_loss: 0.3167 - val_accuracy: 0.8723\n",
      "Epoch 93/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0670 - accuracy: 0.9706 - val_loss: 0.3331 - val_accuracy: 0.8723\n",
      "Epoch 94/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0561 - accuracy: 0.9753 - val_loss: 0.3095 - val_accuracy: 0.8723\n",
      "Epoch 95/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0484 - accuracy: 0.9823 - val_loss: 0.3346 - val_accuracy: 0.8511\n",
      "Epoch 96/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0585 - accuracy: 0.9764 - val_loss: 0.3215 - val_accuracy: 0.8617\n",
      "Epoch 97/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0636 - accuracy: 0.9729 - val_loss: 0.3044 - val_accuracy: 0.8830\n",
      "Epoch 98/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0525 - accuracy: 0.9776 - val_loss: 0.3071 - val_accuracy: 0.8617\n",
      "Epoch 99/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0554 - accuracy: 0.9764 - val_loss: 0.3064 - val_accuracy: 0.8617\n",
      "Epoch 100/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0543 - accuracy: 0.9741 - val_loss: 0.3044 - val_accuracy: 0.8617\n",
      "Score for fold 10: loss of 0.3044358491897583; accuracy of 86.17021441459656%\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 0.4486108124256134 - Accuracy: 77.89473533630371%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 0.2747935950756073 - Accuracy: 84.21052694320679%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 0.3717457056045532 - Accuracy: 81.05263113975525%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 0.23704662919044495 - Accuracy: 88.29787373542786%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 0.2944486737251282 - Accuracy: 81.91489577293396%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 6 - Loss: 0.23681634664535522 - Accuracy: 88.29787373542786%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 7 - Loss: 0.25741422176361084 - Accuracy: 84.04255509376526%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 8 - Loss: 0.29751789569854736 - Accuracy: 81.91489577293396%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 9 - Loss: 0.16276131570339203 - Accuracy: 91.4893627166748%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 10 - Loss: 0.3044358491897583 - Accuracy: 86.17021441459656%\n",
      "------------------------------------------------------------------------\n",
      "Average scores for all folds:\n",
      "> Accuracy: 84.5285564661026 (+- 3.8684983396327244)\n",
      "> Loss: 0.28855910450220107\n",
      "------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "acc_per_fold = []\n",
    "loss_per_fold = []\n",
    "\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True)\n",
    "\n",
    "fold_no = 1\n",
    "\n",
    "with tf.device('/gpu:0'):\n",
    "    for train, test in kfold.split(X_train, y_train):\n",
    "        print(train.shape, test.shape)\n",
    "        \n",
    "        input = Input(shape=(300, 300, 2))\n",
    "        model = EfficientNetB0(input_tensor=input, include_top=False, weights=None, pooling='avg')\n",
    "        \n",
    "        x = model.output\n",
    "\n",
    "        x = Dense(3, activation='softmax', name='softmax', kernel_initializer='he_normal')(x)\n",
    "        model = Model(model.input, x)\n",
    "\n",
    "        \n",
    "        optimizer = tf.keras.optimizers.SGD(lr=0.01, decay=1e-3, momentum=0.9, nesterov=True)\n",
    "        #optimizer = optimizers.Adam(lr=0.001)\n",
    "        \n",
    "        #callbacks_list = [LearningRateSchedule([20,40])]\n",
    "        \n",
    "        model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "        \n",
    "        print('------------------------------------------------------------------------')\n",
    "        print(f'Training for fold {fold_no} ...')\n",
    "        \n",
    "        history = model.fit(inputs[train], targets[train], \n",
    "                            batch_size=4, \n",
    "                            epochs=100, \n",
    "                            verbose=1,\n",
    "                            validation_data=(inputs[test], targets[test]),\n",
    "                            callbacks = tensorboard_callback) # 여기에 Validation set을 넣어야되는거 아닌가?\n",
    "        \n",
    "        scores = model.evaluate(inputs[test], targets[test], verbose=0)\n",
    "        print(f'Score for fold {fold_no}: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}%')\n",
    "        acc_per_fold.append(scores[1] * 100)\n",
    "        loss_per_fold.append(scores[0])\n",
    "        \n",
    "        model.save('E:/Result/ver.3.24/Efficientnet/' + f'Eff_{fold_no}.h5',fold_no)\n",
    "        \n",
    "        fold_no = fold_no + 1\n",
    "\n",
    "# == Provide average scores ==\n",
    "print('------------------------------------------------------------------------')\n",
    "print('Score per fold')\n",
    "for i in range(0, len(acc_per_fold)):\n",
    "  print('------------------------------------------------------------------------')\n",
    "  print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]}%')\n",
    "print('------------------------------------------------------------------------')\n",
    "print('Average scores for all folds:')\n",
    "print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n",
    "print(f'> Loss: {np.mean(loss_per_fold)}')\n",
    "print('------------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pylab as plt\n",
    "\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.legend(['training', 'validation'], loc= 'upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.legend(['training', 'validaion'], loc= 'upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "351c676f2e382adcf1b705bc0333b8a2b296b5ec0e03cae74b6f4b5ae2fa5d28"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('tf2.0-gpu')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
