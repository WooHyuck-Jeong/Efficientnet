{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7.0\n",
      "WARNING:tensorflow:From C:\\Users\\jwhyu\\AppData\\Local\\Temp/ipykernel_6036/1550206244.py:3: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.list_physical_devices('GPU')` instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "tf.test.is_gpu_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 9582706762201358604\n",
      "xla_global_id: -1\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 6300696576\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 7211008039155839469\n",
      "physical_device_desc: \"device: 0, name: NVIDIA GeForce RTX 2060 SUPER, pci bus id: 0000:0a:00.0, compute capability: 7.5\"\n",
      "xla_global_id: 416903419\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from math import ceil\n",
    "from numba import njit, prange\n",
    "from sklearn.utils.validation import check_array\n",
    "from pyts.preprocessing import MinMaxScaler\n",
    "from pyts.approximation import PiecewiseAggregateApproximation\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.utils import np_utils\n",
    "import os \n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Welding_data = np.load('E:/Result/ver.3.22/RP/RP.npz')\n",
    "\n",
    "X_data = Welding_data['X_data']\n",
    "y_data = Welding_data['y_data']\n",
    "i_data = Welding_data['i_data']\n",
    "\n",
    "Welding_data.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 7\n",
    "np.random.seed()\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test,i_train, i_test = train_test_split(X_data, y_data, i_data, test_size= 0.2, shuffle= True, random_state= seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = np.concatenate((X_train, X_test))\n",
    "targets = np.concatenate((y_train, y_test))\n",
    "index = np.concatenate((i_train, i_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = np_utils.to_categorical(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.efficientnet import EfficientNetB3\n",
    "from keras.layers import Dense, Input, Activation, Flatten, Dropout\n",
    "from keras.models import Model\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras import optimizers\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers.normalization.batch_normalization import BatchNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 300, 300, 2  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " rescaling (Rescaling)          (None, 300, 300, 2)  0           ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " normalization (Normalization)  (None, 300, 300, 2)  5           ['rescaling[0][0]']              \n",
      "                                                                                                  \n",
      " stem_conv_pad (ZeroPadding2D)  (None, 301, 301, 2)  0           ['normalization[0][0]']          \n",
      "                                                                                                  \n",
      " stem_conv (Conv2D)             (None, 150, 150, 40  720         ['stem_conv_pad[0][0]']          \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " stem_bn (BatchNormalization)   (None, 150, 150, 40  160         ['stem_conv[0][0]']              \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " stem_activation (Activation)   (None, 150, 150, 40  0           ['stem_bn[0][0]']                \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " block1a_dwconv (DepthwiseConv2  (None, 150, 150, 40  360        ['stem_activation[0][0]']        \n",
      " D)                             )                                                                 \n",
      "                                                                                                  \n",
      " block1a_bn (BatchNormalization  (None, 150, 150, 40  160        ['block1a_dwconv[0][0]']         \n",
      " )                              )                                                                 \n",
      "                                                                                                  \n",
      " block1a_activation (Activation  (None, 150, 150, 40  0          ['block1a_bn[0][0]']             \n",
      " )                              )                                                                 \n",
      "                                                                                                  \n",
      " block1a_se_squeeze (GlobalAver  (None, 40)          0           ['block1a_activation[0][0]']     \n",
      " agePooling2D)                                                                                    \n",
      "                                                                                                  \n",
      " block1a_se_reshape (Reshape)   (None, 1, 1, 40)     0           ['block1a_se_squeeze[0][0]']     \n",
      "                                                                                                  \n",
      " block1a_se_reduce (Conv2D)     (None, 1, 1, 10)     410         ['block1a_se_reshape[0][0]']     \n",
      "                                                                                                  \n",
      " block1a_se_expand (Conv2D)     (None, 1, 1, 40)     440         ['block1a_se_reduce[0][0]']      \n",
      "                                                                                                  \n",
      " block1a_se_excite (Multiply)   (None, 150, 150, 40  0           ['block1a_activation[0][0]',     \n",
      "                                )                                 'block1a_se_expand[0][0]']      \n",
      "                                                                                                  \n",
      " block1a_project_conv (Conv2D)  (None, 150, 150, 24  960         ['block1a_se_excite[0][0]']      \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " block1a_project_bn (BatchNorma  (None, 150, 150, 24  96         ['block1a_project_conv[0][0]']   \n",
      " lization)                      )                                                                 \n",
      "                                                                                                  \n",
      " block1b_dwconv (DepthwiseConv2  (None, 150, 150, 24  216        ['block1a_project_bn[0][0]']     \n",
      " D)                             )                                                                 \n",
      "                                                                                                  \n",
      " block1b_bn (BatchNormalization  (None, 150, 150, 24  96         ['block1b_dwconv[0][0]']         \n",
      " )                              )                                                                 \n",
      "                                                                                                  \n",
      " block1b_activation (Activation  (None, 150, 150, 24  0          ['block1b_bn[0][0]']             \n",
      " )                              )                                                                 \n",
      "                                                                                                  \n",
      " block1b_se_squeeze (GlobalAver  (None, 24)          0           ['block1b_activation[0][0]']     \n",
      " agePooling2D)                                                                                    \n",
      "                                                                                                  \n",
      " block1b_se_reshape (Reshape)   (None, 1, 1, 24)     0           ['block1b_se_squeeze[0][0]']     \n",
      "                                                                                                  \n",
      " block1b_se_reduce (Conv2D)     (None, 1, 1, 6)      150         ['block1b_se_reshape[0][0]']     \n",
      "                                                                                                  \n",
      " block1b_se_expand (Conv2D)     (None, 1, 1, 24)     168         ['block1b_se_reduce[0][0]']      \n",
      "                                                                                                  \n",
      " block1b_se_excite (Multiply)   (None, 150, 150, 24  0           ['block1b_activation[0][0]',     \n",
      "                                )                                 'block1b_se_expand[0][0]']      \n",
      "                                                                                                  \n",
      " block1b_project_conv (Conv2D)  (None, 150, 150, 24  576         ['block1b_se_excite[0][0]']      \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " block1b_project_bn (BatchNorma  (None, 150, 150, 24  96         ['block1b_project_conv[0][0]']   \n",
      " lization)                      )                                                                 \n",
      "                                                                                                  \n",
      " block1b_drop (Dropout)         (None, 150, 150, 24  0           ['block1b_project_bn[0][0]']     \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " block1b_add (Add)              (None, 150, 150, 24  0           ['block1b_drop[0][0]',           \n",
      "                                )                                 'block1a_project_bn[0][0]']     \n",
      "                                                                                                  \n",
      " block2a_expand_conv (Conv2D)   (None, 150, 150, 14  3456        ['block1b_add[0][0]']            \n",
      "                                4)                                                                \n",
      "                                                                                                  \n",
      " block2a_expand_bn (BatchNormal  (None, 150, 150, 14  576        ['block2a_expand_conv[0][0]']    \n",
      " ization)                       4)                                                                \n",
      "                                                                                                  \n",
      " block2a_expand_activation (Act  (None, 150, 150, 14  0          ['block2a_expand_bn[0][0]']      \n",
      " ivation)                       4)                                                                \n",
      "                                                                                                  \n",
      " block2a_dwconv_pad (ZeroPaddin  (None, 151, 151, 14  0          ['block2a_expand_activation[0][0]\n",
      " g2D)                           4)                               ']                               \n",
      "                                                                                                  \n",
      " block2a_dwconv (DepthwiseConv2  (None, 75, 75, 144)  1296       ['block2a_dwconv_pad[0][0]']     \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " block2a_bn (BatchNormalization  (None, 75, 75, 144)  576        ['block2a_dwconv[0][0]']         \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block2a_activation (Activation  (None, 75, 75, 144)  0          ['block2a_bn[0][0]']             \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block2a_se_squeeze (GlobalAver  (None, 144)         0           ['block2a_activation[0][0]']     \n",
      " agePooling2D)                                                                                    \n",
      "                                                                                                  \n",
      " block2a_se_reshape (Reshape)   (None, 1, 1, 144)    0           ['block2a_se_squeeze[0][0]']     \n",
      "                                                                                                  \n",
      " block2a_se_reduce (Conv2D)     (None, 1, 1, 6)      870         ['block2a_se_reshape[0][0]']     \n",
      "                                                                                                  \n",
      " block2a_se_expand (Conv2D)     (None, 1, 1, 144)    1008        ['block2a_se_reduce[0][0]']      \n",
      "                                                                                                  \n",
      " block2a_se_excite (Multiply)   (None, 75, 75, 144)  0           ['block2a_activation[0][0]',     \n",
      "                                                                  'block2a_se_expand[0][0]']      \n",
      "                                                                                                  \n",
      " block2a_project_conv (Conv2D)  (None, 75, 75, 32)   4608        ['block2a_se_excite[0][0]']      \n",
      "                                                                                                  \n",
      " block2a_project_bn (BatchNorma  (None, 75, 75, 32)  128         ['block2a_project_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block2b_expand_conv (Conv2D)   (None, 75, 75, 192)  6144        ['block2a_project_bn[0][0]']     \n",
      "                                                                                                  \n",
      " block2b_expand_bn (BatchNormal  (None, 75, 75, 192)  768        ['block2b_expand_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block2b_expand_activation (Act  (None, 75, 75, 192)  0          ['block2b_expand_bn[0][0]']      \n",
      " ivation)                                                                                         \n",
      "                                                                                                  \n",
      " block2b_dwconv (DepthwiseConv2  (None, 75, 75, 192)  1728       ['block2b_expand_activation[0][0]\n",
      " D)                                                              ']                               \n",
      "                                                                                                  \n",
      " block2b_bn (BatchNormalization  (None, 75, 75, 192)  768        ['block2b_dwconv[0][0]']         \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block2b_activation (Activation  (None, 75, 75, 192)  0          ['block2b_bn[0][0]']             \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block2b_se_squeeze (GlobalAver  (None, 192)         0           ['block2b_activation[0][0]']     \n",
      " agePooling2D)                                                                                    \n",
      "                                                                                                  \n",
      " block2b_se_reshape (Reshape)   (None, 1, 1, 192)    0           ['block2b_se_squeeze[0][0]']     \n",
      "                                                                                                  \n",
      " block2b_se_reduce (Conv2D)     (None, 1, 1, 8)      1544        ['block2b_se_reshape[0][0]']     \n",
      "                                                                                                  \n",
      " block2b_se_expand (Conv2D)     (None, 1, 1, 192)    1728        ['block2b_se_reduce[0][0]']      \n",
      "                                                                                                  \n",
      " block2b_se_excite (Multiply)   (None, 75, 75, 192)  0           ['block2b_activation[0][0]',     \n",
      "                                                                  'block2b_se_expand[0][0]']      \n",
      "                                                                                                  \n",
      " block2b_project_conv (Conv2D)  (None, 75, 75, 32)   6144        ['block2b_se_excite[0][0]']      \n",
      "                                                                                                  \n",
      " block2b_project_bn (BatchNorma  (None, 75, 75, 32)  128         ['block2b_project_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block2b_drop (Dropout)         (None, 75, 75, 32)   0           ['block2b_project_bn[0][0]']     \n",
      "                                                                                                  \n",
      " block2b_add (Add)              (None, 75, 75, 32)   0           ['block2b_drop[0][0]',           \n",
      "                                                                  'block2a_project_bn[0][0]']     \n",
      "                                                                                                  \n",
      " block2c_expand_conv (Conv2D)   (None, 75, 75, 192)  6144        ['block2b_add[0][0]']            \n",
      "                                                                                                  \n",
      " block2c_expand_bn (BatchNormal  (None, 75, 75, 192)  768        ['block2c_expand_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block2c_expand_activation (Act  (None, 75, 75, 192)  0          ['block2c_expand_bn[0][0]']      \n",
      " ivation)                                                                                         \n",
      "                                                                                                  \n",
      " block2c_dwconv (DepthwiseConv2  (None, 75, 75, 192)  1728       ['block2c_expand_activation[0][0]\n",
      " D)                                                              ']                               \n",
      "                                                                                                  \n",
      " block2c_bn (BatchNormalization  (None, 75, 75, 192)  768        ['block2c_dwconv[0][0]']         \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block2c_activation (Activation  (None, 75, 75, 192)  0          ['block2c_bn[0][0]']             \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block2c_se_squeeze (GlobalAver  (None, 192)         0           ['block2c_activation[0][0]']     \n",
      " agePooling2D)                                                                                    \n",
      "                                                                                                  \n",
      " block2c_se_reshape (Reshape)   (None, 1, 1, 192)    0           ['block2c_se_squeeze[0][0]']     \n",
      "                                                                                                  \n",
      " block2c_se_reduce (Conv2D)     (None, 1, 1, 8)      1544        ['block2c_se_reshape[0][0]']     \n",
      "                                                                                                  \n",
      " block2c_se_expand (Conv2D)     (None, 1, 1, 192)    1728        ['block2c_se_reduce[0][0]']      \n",
      "                                                                                                  \n",
      " block2c_se_excite (Multiply)   (None, 75, 75, 192)  0           ['block2c_activation[0][0]',     \n",
      "                                                                  'block2c_se_expand[0][0]']      \n",
      "                                                                                                  \n",
      " block2c_project_conv (Conv2D)  (None, 75, 75, 32)   6144        ['block2c_se_excite[0][0]']      \n",
      "                                                                                                  \n",
      " block2c_project_bn (BatchNorma  (None, 75, 75, 32)  128         ['block2c_project_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block2c_drop (Dropout)         (None, 75, 75, 32)   0           ['block2c_project_bn[0][0]']     \n",
      "                                                                                                  \n",
      " block2c_add (Add)              (None, 75, 75, 32)   0           ['block2c_drop[0][0]',           \n",
      "                                                                  'block2b_add[0][0]']            \n",
      "                                                                                                  \n",
      " block3a_expand_conv (Conv2D)   (None, 75, 75, 192)  6144        ['block2c_add[0][0]']            \n",
      "                                                                                                  \n",
      " block3a_expand_bn (BatchNormal  (None, 75, 75, 192)  768        ['block3a_expand_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block3a_expand_activation (Act  (None, 75, 75, 192)  0          ['block3a_expand_bn[0][0]']      \n",
      " ivation)                                                                                         \n",
      "                                                                                                  \n",
      " block3a_dwconv_pad (ZeroPaddin  (None, 79, 79, 192)  0          ['block3a_expand_activation[0][0]\n",
      " g2D)                                                            ']                               \n",
      "                                                                                                  \n",
      " block3a_dwconv (DepthwiseConv2  (None, 38, 38, 192)  4800       ['block3a_dwconv_pad[0][0]']     \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " block3a_bn (BatchNormalization  (None, 38, 38, 192)  768        ['block3a_dwconv[0][0]']         \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block3a_activation (Activation  (None, 38, 38, 192)  0          ['block3a_bn[0][0]']             \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block3a_se_squeeze (GlobalAver  (None, 192)         0           ['block3a_activation[0][0]']     \n",
      " agePooling2D)                                                                                    \n",
      "                                                                                                  \n",
      " block3a_se_reshape (Reshape)   (None, 1, 1, 192)    0           ['block3a_se_squeeze[0][0]']     \n",
      "                                                                                                  \n",
      " block3a_se_reduce (Conv2D)     (None, 1, 1, 8)      1544        ['block3a_se_reshape[0][0]']     \n",
      "                                                                                                  \n",
      " block3a_se_expand (Conv2D)     (None, 1, 1, 192)    1728        ['block3a_se_reduce[0][0]']      \n",
      "                                                                                                  \n",
      " block3a_se_excite (Multiply)   (None, 38, 38, 192)  0           ['block3a_activation[0][0]',     \n",
      "                                                                  'block3a_se_expand[0][0]']      \n",
      "                                                                                                  \n",
      " block3a_project_conv (Conv2D)  (None, 38, 38, 48)   9216        ['block3a_se_excite[0][0]']      \n",
      "                                                                                                  \n",
      " block3a_project_bn (BatchNorma  (None, 38, 38, 48)  192         ['block3a_project_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block3b_expand_conv (Conv2D)   (None, 38, 38, 288)  13824       ['block3a_project_bn[0][0]']     \n",
      "                                                                                                  \n",
      " block3b_expand_bn (BatchNormal  (None, 38, 38, 288)  1152       ['block3b_expand_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block3b_expand_activation (Act  (None, 38, 38, 288)  0          ['block3b_expand_bn[0][0]']      \n",
      " ivation)                                                                                         \n",
      "                                                                                                  \n",
      " block3b_dwconv (DepthwiseConv2  (None, 38, 38, 288)  7200       ['block3b_expand_activation[0][0]\n",
      " D)                                                              ']                               \n",
      "                                                                                                  \n",
      " block3b_bn (BatchNormalization  (None, 38, 38, 288)  1152       ['block3b_dwconv[0][0]']         \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block3b_activation (Activation  (None, 38, 38, 288)  0          ['block3b_bn[0][0]']             \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block3b_se_squeeze (GlobalAver  (None, 288)         0           ['block3b_activation[0][0]']     \n",
      " agePooling2D)                                                                                    \n",
      "                                                                                                  \n",
      " block3b_se_reshape (Reshape)   (None, 1, 1, 288)    0           ['block3b_se_squeeze[0][0]']     \n",
      "                                                                                                  \n",
      " block3b_se_reduce (Conv2D)     (None, 1, 1, 12)     3468        ['block3b_se_reshape[0][0]']     \n",
      "                                                                                                  \n",
      " block3b_se_expand (Conv2D)     (None, 1, 1, 288)    3744        ['block3b_se_reduce[0][0]']      \n",
      "                                                                                                  \n",
      " block3b_se_excite (Multiply)   (None, 38, 38, 288)  0           ['block3b_activation[0][0]',     \n",
      "                                                                  'block3b_se_expand[0][0]']      \n",
      "                                                                                                  \n",
      " block3b_project_conv (Conv2D)  (None, 38, 38, 48)   13824       ['block3b_se_excite[0][0]']      \n",
      "                                                                                                  \n",
      " block3b_project_bn (BatchNorma  (None, 38, 38, 48)  192         ['block3b_project_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block3b_drop (Dropout)         (None, 38, 38, 48)   0           ['block3b_project_bn[0][0]']     \n",
      "                                                                                                  \n",
      " block3b_add (Add)              (None, 38, 38, 48)   0           ['block3b_drop[0][0]',           \n",
      "                                                                  'block3a_project_bn[0][0]']     \n",
      "                                                                                                  \n",
      " block3c_expand_conv (Conv2D)   (None, 38, 38, 288)  13824       ['block3b_add[0][0]']            \n",
      "                                                                                                  \n",
      " block3c_expand_bn (BatchNormal  (None, 38, 38, 288)  1152       ['block3c_expand_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block3c_expand_activation (Act  (None, 38, 38, 288)  0          ['block3c_expand_bn[0][0]']      \n",
      " ivation)                                                                                         \n",
      "                                                                                                  \n",
      " block3c_dwconv (DepthwiseConv2  (None, 38, 38, 288)  7200       ['block3c_expand_activation[0][0]\n",
      " D)                                                              ']                               \n",
      "                                                                                                  \n",
      " block3c_bn (BatchNormalization  (None, 38, 38, 288)  1152       ['block3c_dwconv[0][0]']         \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block3c_activation (Activation  (None, 38, 38, 288)  0          ['block3c_bn[0][0]']             \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block3c_se_squeeze (GlobalAver  (None, 288)         0           ['block3c_activation[0][0]']     \n",
      " agePooling2D)                                                                                    \n",
      "                                                                                                  \n",
      " block3c_se_reshape (Reshape)   (None, 1, 1, 288)    0           ['block3c_se_squeeze[0][0]']     \n",
      "                                                                                                  \n",
      " block3c_se_reduce (Conv2D)     (None, 1, 1, 12)     3468        ['block3c_se_reshape[0][0]']     \n",
      "                                                                                                  \n",
      " block3c_se_expand (Conv2D)     (None, 1, 1, 288)    3744        ['block3c_se_reduce[0][0]']      \n",
      "                                                                                                  \n",
      " block3c_se_excite (Multiply)   (None, 38, 38, 288)  0           ['block3c_activation[0][0]',     \n",
      "                                                                  'block3c_se_expand[0][0]']      \n",
      "                                                                                                  \n",
      " block3c_project_conv (Conv2D)  (None, 38, 38, 48)   13824       ['block3c_se_excite[0][0]']      \n",
      "                                                                                                  \n",
      " block3c_project_bn (BatchNorma  (None, 38, 38, 48)  192         ['block3c_project_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block3c_drop (Dropout)         (None, 38, 38, 48)   0           ['block3c_project_bn[0][0]']     \n",
      "                                                                                                  \n",
      " block3c_add (Add)              (None, 38, 38, 48)   0           ['block3c_drop[0][0]',           \n",
      "                                                                  'block3b_add[0][0]']            \n",
      "                                                                                                  \n",
      " block4a_expand_conv (Conv2D)   (None, 38, 38, 288)  13824       ['block3c_add[0][0]']            \n",
      "                                                                                                  \n",
      " block4a_expand_bn (BatchNormal  (None, 38, 38, 288)  1152       ['block4a_expand_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block4a_expand_activation (Act  (None, 38, 38, 288)  0          ['block4a_expand_bn[0][0]']      \n",
      " ivation)                                                                                         \n",
      "                                                                                                  \n",
      " block4a_dwconv_pad (ZeroPaddin  (None, 39, 39, 288)  0          ['block4a_expand_activation[0][0]\n",
      " g2D)                                                            ']                               \n",
      "                                                                                                  \n",
      " block4a_dwconv (DepthwiseConv2  (None, 19, 19, 288)  2592       ['block4a_dwconv_pad[0][0]']     \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " block4a_bn (BatchNormalization  (None, 19, 19, 288)  1152       ['block4a_dwconv[0][0]']         \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block4a_activation (Activation  (None, 19, 19, 288)  0          ['block4a_bn[0][0]']             \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block4a_se_squeeze (GlobalAver  (None, 288)         0           ['block4a_activation[0][0]']     \n",
      " agePooling2D)                                                                                    \n",
      "                                                                                                  \n",
      " block4a_se_reshape (Reshape)   (None, 1, 1, 288)    0           ['block4a_se_squeeze[0][0]']     \n",
      "                                                                                                  \n",
      " block4a_se_reduce (Conv2D)     (None, 1, 1, 12)     3468        ['block4a_se_reshape[0][0]']     \n",
      "                                                                                                  \n",
      " block4a_se_expand (Conv2D)     (None, 1, 1, 288)    3744        ['block4a_se_reduce[0][0]']      \n",
      "                                                                                                  \n",
      " block4a_se_excite (Multiply)   (None, 19, 19, 288)  0           ['block4a_activation[0][0]',     \n",
      "                                                                  'block4a_se_expand[0][0]']      \n",
      "                                                                                                  \n",
      " block4a_project_conv (Conv2D)  (None, 19, 19, 96)   27648       ['block4a_se_excite[0][0]']      \n",
      "                                                                                                  \n",
      " block4a_project_bn (BatchNorma  (None, 19, 19, 96)  384         ['block4a_project_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block4b_expand_conv (Conv2D)   (None, 19, 19, 576)  55296       ['block4a_project_bn[0][0]']     \n",
      "                                                                                                  \n",
      " block4b_expand_bn (BatchNormal  (None, 19, 19, 576)  2304       ['block4b_expand_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block4b_expand_activation (Act  (None, 19, 19, 576)  0          ['block4b_expand_bn[0][0]']      \n",
      " ivation)                                                                                         \n",
      "                                                                                                  \n",
      " block4b_dwconv (DepthwiseConv2  (None, 19, 19, 576)  5184       ['block4b_expand_activation[0][0]\n",
      " D)                                                              ']                               \n",
      "                                                                                                  \n",
      " block4b_bn (BatchNormalization  (None, 19, 19, 576)  2304       ['block4b_dwconv[0][0]']         \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block4b_activation (Activation  (None, 19, 19, 576)  0          ['block4b_bn[0][0]']             \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block4b_se_squeeze (GlobalAver  (None, 576)         0           ['block4b_activation[0][0]']     \n",
      " agePooling2D)                                                                                    \n",
      "                                                                                                  \n",
      " block4b_se_reshape (Reshape)   (None, 1, 1, 576)    0           ['block4b_se_squeeze[0][0]']     \n",
      "                                                                                                  \n",
      " block4b_se_reduce (Conv2D)     (None, 1, 1, 24)     13848       ['block4b_se_reshape[0][0]']     \n",
      "                                                                                                  \n",
      " block4b_se_expand (Conv2D)     (None, 1, 1, 576)    14400       ['block4b_se_reduce[0][0]']      \n",
      "                                                                                                  \n",
      " block4b_se_excite (Multiply)   (None, 19, 19, 576)  0           ['block4b_activation[0][0]',     \n",
      "                                                                  'block4b_se_expand[0][0]']      \n",
      "                                                                                                  \n",
      " block4b_project_conv (Conv2D)  (None, 19, 19, 96)   55296       ['block4b_se_excite[0][0]']      \n",
      "                                                                                                  \n",
      " block4b_project_bn (BatchNorma  (None, 19, 19, 96)  384         ['block4b_project_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block4b_drop (Dropout)         (None, 19, 19, 96)   0           ['block4b_project_bn[0][0]']     \n",
      "                                                                                                  \n",
      " block4b_add (Add)              (None, 19, 19, 96)   0           ['block4b_drop[0][0]',           \n",
      "                                                                  'block4a_project_bn[0][0]']     \n",
      "                                                                                                  \n",
      " block4c_expand_conv (Conv2D)   (None, 19, 19, 576)  55296       ['block4b_add[0][0]']            \n",
      "                                                                                                  \n",
      " block4c_expand_bn (BatchNormal  (None, 19, 19, 576)  2304       ['block4c_expand_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block4c_expand_activation (Act  (None, 19, 19, 576)  0          ['block4c_expand_bn[0][0]']      \n",
      " ivation)                                                                                         \n",
      "                                                                                                  \n",
      " block4c_dwconv (DepthwiseConv2  (None, 19, 19, 576)  5184       ['block4c_expand_activation[0][0]\n",
      " D)                                                              ']                               \n",
      "                                                                                                  \n",
      " block4c_bn (BatchNormalization  (None, 19, 19, 576)  2304       ['block4c_dwconv[0][0]']         \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block4c_activation (Activation  (None, 19, 19, 576)  0          ['block4c_bn[0][0]']             \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block4c_se_squeeze (GlobalAver  (None, 576)         0           ['block4c_activation[0][0]']     \n",
      " agePooling2D)                                                                                    \n",
      "                                                                                                  \n",
      " block4c_se_reshape (Reshape)   (None, 1, 1, 576)    0           ['block4c_se_squeeze[0][0]']     \n",
      "                                                                                                  \n",
      " block4c_se_reduce (Conv2D)     (None, 1, 1, 24)     13848       ['block4c_se_reshape[0][0]']     \n",
      "                                                                                                  \n",
      " block4c_se_expand (Conv2D)     (None, 1, 1, 576)    14400       ['block4c_se_reduce[0][0]']      \n",
      "                                                                                                  \n",
      " block4c_se_excite (Multiply)   (None, 19, 19, 576)  0           ['block4c_activation[0][0]',     \n",
      "                                                                  'block4c_se_expand[0][0]']      \n",
      "                                                                                                  \n",
      " block4c_project_conv (Conv2D)  (None, 19, 19, 96)   55296       ['block4c_se_excite[0][0]']      \n",
      "                                                                                                  \n",
      " block4c_project_bn (BatchNorma  (None, 19, 19, 96)  384         ['block4c_project_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block4c_drop (Dropout)         (None, 19, 19, 96)   0           ['block4c_project_bn[0][0]']     \n",
      "                                                                                                  \n",
      " block4c_add (Add)              (None, 19, 19, 96)   0           ['block4c_drop[0][0]',           \n",
      "                                                                  'block4b_add[0][0]']            \n",
      "                                                                                                  \n",
      " block4d_expand_conv (Conv2D)   (None, 19, 19, 576)  55296       ['block4c_add[0][0]']            \n",
      "                                                                                                  \n",
      " block4d_expand_bn (BatchNormal  (None, 19, 19, 576)  2304       ['block4d_expand_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block4d_expand_activation (Act  (None, 19, 19, 576)  0          ['block4d_expand_bn[0][0]']      \n",
      " ivation)                                                                                         \n",
      "                                                                                                  \n",
      " block4d_dwconv (DepthwiseConv2  (None, 19, 19, 576)  5184       ['block4d_expand_activation[0][0]\n",
      " D)                                                              ']                               \n",
      "                                                                                                  \n",
      " block4d_bn (BatchNormalization  (None, 19, 19, 576)  2304       ['block4d_dwconv[0][0]']         \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block4d_activation (Activation  (None, 19, 19, 576)  0          ['block4d_bn[0][0]']             \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block4d_se_squeeze (GlobalAver  (None, 576)         0           ['block4d_activation[0][0]']     \n",
      " agePooling2D)                                                                                    \n",
      "                                                                                                  \n",
      " block4d_se_reshape (Reshape)   (None, 1, 1, 576)    0           ['block4d_se_squeeze[0][0]']     \n",
      "                                                                                                  \n",
      " block4d_se_reduce (Conv2D)     (None, 1, 1, 24)     13848       ['block4d_se_reshape[0][0]']     \n",
      "                                                                                                  \n",
      " block4d_se_expand (Conv2D)     (None, 1, 1, 576)    14400       ['block4d_se_reduce[0][0]']      \n",
      "                                                                                                  \n",
      " block4d_se_excite (Multiply)   (None, 19, 19, 576)  0           ['block4d_activation[0][0]',     \n",
      "                                                                  'block4d_se_expand[0][0]']      \n",
      "                                                                                                  \n",
      " block4d_project_conv (Conv2D)  (None, 19, 19, 96)   55296       ['block4d_se_excite[0][0]']      \n",
      "                                                                                                  \n",
      " block4d_project_bn (BatchNorma  (None, 19, 19, 96)  384         ['block4d_project_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block4d_drop (Dropout)         (None, 19, 19, 96)   0           ['block4d_project_bn[0][0]']     \n",
      "                                                                                                  \n",
      " block4d_add (Add)              (None, 19, 19, 96)   0           ['block4d_drop[0][0]',           \n",
      "                                                                  'block4c_add[0][0]']            \n",
      "                                                                                                  \n",
      " block4e_expand_conv (Conv2D)   (None, 19, 19, 576)  55296       ['block4d_add[0][0]']            \n",
      "                                                                                                  \n",
      " block4e_expand_bn (BatchNormal  (None, 19, 19, 576)  2304       ['block4e_expand_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block4e_expand_activation (Act  (None, 19, 19, 576)  0          ['block4e_expand_bn[0][0]']      \n",
      " ivation)                                                                                         \n",
      "                                                                                                  \n",
      " block4e_dwconv (DepthwiseConv2  (None, 19, 19, 576)  5184       ['block4e_expand_activation[0][0]\n",
      " D)                                                              ']                               \n",
      "                                                                                                  \n",
      " block4e_bn (BatchNormalization  (None, 19, 19, 576)  2304       ['block4e_dwconv[0][0]']         \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block4e_activation (Activation  (None, 19, 19, 576)  0          ['block4e_bn[0][0]']             \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block4e_se_squeeze (GlobalAver  (None, 576)         0           ['block4e_activation[0][0]']     \n",
      " agePooling2D)                                                                                    \n",
      "                                                                                                  \n",
      " block4e_se_reshape (Reshape)   (None, 1, 1, 576)    0           ['block4e_se_squeeze[0][0]']     \n",
      "                                                                                                  \n",
      " block4e_se_reduce (Conv2D)     (None, 1, 1, 24)     13848       ['block4e_se_reshape[0][0]']     \n",
      "                                                                                                  \n",
      " block4e_se_expand (Conv2D)     (None, 1, 1, 576)    14400       ['block4e_se_reduce[0][0]']      \n",
      "                                                                                                  \n",
      " block4e_se_excite (Multiply)   (None, 19, 19, 576)  0           ['block4e_activation[0][0]',     \n",
      "                                                                  'block4e_se_expand[0][0]']      \n",
      "                                                                                                  \n",
      " block4e_project_conv (Conv2D)  (None, 19, 19, 96)   55296       ['block4e_se_excite[0][0]']      \n",
      "                                                                                                  \n",
      " block4e_project_bn (BatchNorma  (None, 19, 19, 96)  384         ['block4e_project_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block4e_drop (Dropout)         (None, 19, 19, 96)   0           ['block4e_project_bn[0][0]']     \n",
      "                                                                                                  \n",
      " block4e_add (Add)              (None, 19, 19, 96)   0           ['block4e_drop[0][0]',           \n",
      "                                                                  'block4d_add[0][0]']            \n",
      "                                                                                                  \n",
      " block5a_expand_conv (Conv2D)   (None, 19, 19, 576)  55296       ['block4e_add[0][0]']            \n",
      "                                                                                                  \n",
      " block5a_expand_bn (BatchNormal  (None, 19, 19, 576)  2304       ['block5a_expand_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block5a_expand_activation (Act  (None, 19, 19, 576)  0          ['block5a_expand_bn[0][0]']      \n",
      " ivation)                                                                                         \n",
      "                                                                                                  \n",
      " block5a_dwconv (DepthwiseConv2  (None, 19, 19, 576)  14400      ['block5a_expand_activation[0][0]\n",
      " D)                                                              ']                               \n",
      "                                                                                                  \n",
      " block5a_bn (BatchNormalization  (None, 19, 19, 576)  2304       ['block5a_dwconv[0][0]']         \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block5a_activation (Activation  (None, 19, 19, 576)  0          ['block5a_bn[0][0]']             \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block5a_se_squeeze (GlobalAver  (None, 576)         0           ['block5a_activation[0][0]']     \n",
      " agePooling2D)                                                                                    \n",
      "                                                                                                  \n",
      " block5a_se_reshape (Reshape)   (None, 1, 1, 576)    0           ['block5a_se_squeeze[0][0]']     \n",
      "                                                                                                  \n",
      " block5a_se_reduce (Conv2D)     (None, 1, 1, 24)     13848       ['block5a_se_reshape[0][0]']     \n",
      "                                                                                                  \n",
      " block5a_se_expand (Conv2D)     (None, 1, 1, 576)    14400       ['block5a_se_reduce[0][0]']      \n",
      "                                                                                                  \n",
      " block5a_se_excite (Multiply)   (None, 19, 19, 576)  0           ['block5a_activation[0][0]',     \n",
      "                                                                  'block5a_se_expand[0][0]']      \n",
      "                                                                                                  \n",
      " block5a_project_conv (Conv2D)  (None, 19, 19, 136)  78336       ['block5a_se_excite[0][0]']      \n",
      "                                                                                                  \n",
      " block5a_project_bn (BatchNorma  (None, 19, 19, 136)  544        ['block5a_project_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block5b_expand_conv (Conv2D)   (None, 19, 19, 816)  110976      ['block5a_project_bn[0][0]']     \n",
      "                                                                                                  \n",
      " block5b_expand_bn (BatchNormal  (None, 19, 19, 816)  3264       ['block5b_expand_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block5b_expand_activation (Act  (None, 19, 19, 816)  0          ['block5b_expand_bn[0][0]']      \n",
      " ivation)                                                                                         \n",
      "                                                                                                  \n",
      " block5b_dwconv (DepthwiseConv2  (None, 19, 19, 816)  20400      ['block5b_expand_activation[0][0]\n",
      " D)                                                              ']                               \n",
      "                                                                                                  \n",
      " block5b_bn (BatchNormalization  (None, 19, 19, 816)  3264       ['block5b_dwconv[0][0]']         \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block5b_activation (Activation  (None, 19, 19, 816)  0          ['block5b_bn[0][0]']             \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block5b_se_squeeze (GlobalAver  (None, 816)         0           ['block5b_activation[0][0]']     \n",
      " agePooling2D)                                                                                    \n",
      "                                                                                                  \n",
      " block5b_se_reshape (Reshape)   (None, 1, 1, 816)    0           ['block5b_se_squeeze[0][0]']     \n",
      "                                                                                                  \n",
      " block5b_se_reduce (Conv2D)     (None, 1, 1, 34)     27778       ['block5b_se_reshape[0][0]']     \n",
      "                                                                                                  \n",
      " block5b_se_expand (Conv2D)     (None, 1, 1, 816)    28560       ['block5b_se_reduce[0][0]']      \n",
      "                                                                                                  \n",
      " block5b_se_excite (Multiply)   (None, 19, 19, 816)  0           ['block5b_activation[0][0]',     \n",
      "                                                                  'block5b_se_expand[0][0]']      \n",
      "                                                                                                  \n",
      " block5b_project_conv (Conv2D)  (None, 19, 19, 136)  110976      ['block5b_se_excite[0][0]']      \n",
      "                                                                                                  \n",
      " block5b_project_bn (BatchNorma  (None, 19, 19, 136)  544        ['block5b_project_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block5b_drop (Dropout)         (None, 19, 19, 136)  0           ['block5b_project_bn[0][0]']     \n",
      "                                                                                                  \n",
      " block5b_add (Add)              (None, 19, 19, 136)  0           ['block5b_drop[0][0]',           \n",
      "                                                                  'block5a_project_bn[0][0]']     \n",
      "                                                                                                  \n",
      " block5c_expand_conv (Conv2D)   (None, 19, 19, 816)  110976      ['block5b_add[0][0]']            \n",
      "                                                                                                  \n",
      " block5c_expand_bn (BatchNormal  (None, 19, 19, 816)  3264       ['block5c_expand_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block5c_expand_activation (Act  (None, 19, 19, 816)  0          ['block5c_expand_bn[0][0]']      \n",
      " ivation)                                                                                         \n",
      "                                                                                                  \n",
      " block5c_dwconv (DepthwiseConv2  (None, 19, 19, 816)  20400      ['block5c_expand_activation[0][0]\n",
      " D)                                                              ']                               \n",
      "                                                                                                  \n",
      " block5c_bn (BatchNormalization  (None, 19, 19, 816)  3264       ['block5c_dwconv[0][0]']         \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block5c_activation (Activation  (None, 19, 19, 816)  0          ['block5c_bn[0][0]']             \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block5c_se_squeeze (GlobalAver  (None, 816)         0           ['block5c_activation[0][0]']     \n",
      " agePooling2D)                                                                                    \n",
      "                                                                                                  \n",
      " block5c_se_reshape (Reshape)   (None, 1, 1, 816)    0           ['block5c_se_squeeze[0][0]']     \n",
      "                                                                                                  \n",
      " block5c_se_reduce (Conv2D)     (None, 1, 1, 34)     27778       ['block5c_se_reshape[0][0]']     \n",
      "                                                                                                  \n",
      " block5c_se_expand (Conv2D)     (None, 1, 1, 816)    28560       ['block5c_se_reduce[0][0]']      \n",
      "                                                                                                  \n",
      " block5c_se_excite (Multiply)   (None, 19, 19, 816)  0           ['block5c_activation[0][0]',     \n",
      "                                                                  'block5c_se_expand[0][0]']      \n",
      "                                                                                                  \n",
      " block5c_project_conv (Conv2D)  (None, 19, 19, 136)  110976      ['block5c_se_excite[0][0]']      \n",
      "                                                                                                  \n",
      " block5c_project_bn (BatchNorma  (None, 19, 19, 136)  544        ['block5c_project_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block5c_drop (Dropout)         (None, 19, 19, 136)  0           ['block5c_project_bn[0][0]']     \n",
      "                                                                                                  \n",
      " block5c_add (Add)              (None, 19, 19, 136)  0           ['block5c_drop[0][0]',           \n",
      "                                                                  'block5b_add[0][0]']            \n",
      "                                                                                                  \n",
      " block5d_expand_conv (Conv2D)   (None, 19, 19, 816)  110976      ['block5c_add[0][0]']            \n",
      "                                                                                                  \n",
      " block5d_expand_bn (BatchNormal  (None, 19, 19, 816)  3264       ['block5d_expand_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block5d_expand_activation (Act  (None, 19, 19, 816)  0          ['block5d_expand_bn[0][0]']      \n",
      " ivation)                                                                                         \n",
      "                                                                                                  \n",
      " block5d_dwconv (DepthwiseConv2  (None, 19, 19, 816)  20400      ['block5d_expand_activation[0][0]\n",
      " D)                                                              ']                               \n",
      "                                                                                                  \n",
      " block5d_bn (BatchNormalization  (None, 19, 19, 816)  3264       ['block5d_dwconv[0][0]']         \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block5d_activation (Activation  (None, 19, 19, 816)  0          ['block5d_bn[0][0]']             \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block5d_se_squeeze (GlobalAver  (None, 816)         0           ['block5d_activation[0][0]']     \n",
      " agePooling2D)                                                                                    \n",
      "                                                                                                  \n",
      " block5d_se_reshape (Reshape)   (None, 1, 1, 816)    0           ['block5d_se_squeeze[0][0]']     \n",
      "                                                                                                  \n",
      " block5d_se_reduce (Conv2D)     (None, 1, 1, 34)     27778       ['block5d_se_reshape[0][0]']     \n",
      "                                                                                                  \n",
      " block5d_se_expand (Conv2D)     (None, 1, 1, 816)    28560       ['block5d_se_reduce[0][0]']      \n",
      "                                                                                                  \n",
      " block5d_se_excite (Multiply)   (None, 19, 19, 816)  0           ['block5d_activation[0][0]',     \n",
      "                                                                  'block5d_se_expand[0][0]']      \n",
      "                                                                                                  \n",
      " block5d_project_conv (Conv2D)  (None, 19, 19, 136)  110976      ['block5d_se_excite[0][0]']      \n",
      "                                                                                                  \n",
      " block5d_project_bn (BatchNorma  (None, 19, 19, 136)  544        ['block5d_project_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block5d_drop (Dropout)         (None, 19, 19, 136)  0           ['block5d_project_bn[0][0]']     \n",
      "                                                                                                  \n",
      " block5d_add (Add)              (None, 19, 19, 136)  0           ['block5d_drop[0][0]',           \n",
      "                                                                  'block5c_add[0][0]']            \n",
      "                                                                                                  \n",
      " block5e_expand_conv (Conv2D)   (None, 19, 19, 816)  110976      ['block5d_add[0][0]']            \n",
      "                                                                                                  \n",
      " block5e_expand_bn (BatchNormal  (None, 19, 19, 816)  3264       ['block5e_expand_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block5e_expand_activation (Act  (None, 19, 19, 816)  0          ['block5e_expand_bn[0][0]']      \n",
      " ivation)                                                                                         \n",
      "                                                                                                  \n",
      " block5e_dwconv (DepthwiseConv2  (None, 19, 19, 816)  20400      ['block5e_expand_activation[0][0]\n",
      " D)                                                              ']                               \n",
      "                                                                                                  \n",
      " block5e_bn (BatchNormalization  (None, 19, 19, 816)  3264       ['block5e_dwconv[0][0]']         \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block5e_activation (Activation  (None, 19, 19, 816)  0          ['block5e_bn[0][0]']             \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block5e_se_squeeze (GlobalAver  (None, 816)         0           ['block5e_activation[0][0]']     \n",
      " agePooling2D)                                                                                    \n",
      "                                                                                                  \n",
      " block5e_se_reshape (Reshape)   (None, 1, 1, 816)    0           ['block5e_se_squeeze[0][0]']     \n",
      "                                                                                                  \n",
      " block5e_se_reduce (Conv2D)     (None, 1, 1, 34)     27778       ['block5e_se_reshape[0][0]']     \n",
      "                                                                                                  \n",
      " block5e_se_expand (Conv2D)     (None, 1, 1, 816)    28560       ['block5e_se_reduce[0][0]']      \n",
      "                                                                                                  \n",
      " block5e_se_excite (Multiply)   (None, 19, 19, 816)  0           ['block5e_activation[0][0]',     \n",
      "                                                                  'block5e_se_expand[0][0]']      \n",
      "                                                                                                  \n",
      " block5e_project_conv (Conv2D)  (None, 19, 19, 136)  110976      ['block5e_se_excite[0][0]']      \n",
      "                                                                                                  \n",
      " block5e_project_bn (BatchNorma  (None, 19, 19, 136)  544        ['block5e_project_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block5e_drop (Dropout)         (None, 19, 19, 136)  0           ['block5e_project_bn[0][0]']     \n",
      "                                                                                                  \n",
      " block5e_add (Add)              (None, 19, 19, 136)  0           ['block5e_drop[0][0]',           \n",
      "                                                                  'block5d_add[0][0]']            \n",
      "                                                                                                  \n",
      " block6a_expand_conv (Conv2D)   (None, 19, 19, 816)  110976      ['block5e_add[0][0]']            \n",
      "                                                                                                  \n",
      " block6a_expand_bn (BatchNormal  (None, 19, 19, 816)  3264       ['block6a_expand_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block6a_expand_activation (Act  (None, 19, 19, 816)  0          ['block6a_expand_bn[0][0]']      \n",
      " ivation)                                                                                         \n",
      "                                                                                                  \n",
      " block6a_dwconv_pad (ZeroPaddin  (None, 23, 23, 816)  0          ['block6a_expand_activation[0][0]\n",
      " g2D)                                                            ']                               \n",
      "                                                                                                  \n",
      " block6a_dwconv (DepthwiseConv2  (None, 10, 10, 816)  20400      ['block6a_dwconv_pad[0][0]']     \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " block6a_bn (BatchNormalization  (None, 10, 10, 816)  3264       ['block6a_dwconv[0][0]']         \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block6a_activation (Activation  (None, 10, 10, 816)  0          ['block6a_bn[0][0]']             \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block6a_se_squeeze (GlobalAver  (None, 816)         0           ['block6a_activation[0][0]']     \n",
      " agePooling2D)                                                                                    \n",
      "                                                                                                  \n",
      " block6a_se_reshape (Reshape)   (None, 1, 1, 816)    0           ['block6a_se_squeeze[0][0]']     \n",
      "                                                                                                  \n",
      " block6a_se_reduce (Conv2D)     (None, 1, 1, 34)     27778       ['block6a_se_reshape[0][0]']     \n",
      "                                                                                                  \n",
      " block6a_se_expand (Conv2D)     (None, 1, 1, 816)    28560       ['block6a_se_reduce[0][0]']      \n",
      "                                                                                                  \n",
      " block6a_se_excite (Multiply)   (None, 10, 10, 816)  0           ['block6a_activation[0][0]',     \n",
      "                                                                  'block6a_se_expand[0][0]']      \n",
      "                                                                                                  \n",
      " block6a_project_conv (Conv2D)  (None, 10, 10, 232)  189312      ['block6a_se_excite[0][0]']      \n",
      "                                                                                                  \n",
      " block6a_project_bn (BatchNorma  (None, 10, 10, 232)  928        ['block6a_project_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block6b_expand_conv (Conv2D)   (None, 10, 10, 1392  322944      ['block6a_project_bn[0][0]']     \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " block6b_expand_bn (BatchNormal  (None, 10, 10, 1392  5568       ['block6b_expand_conv[0][0]']    \n",
      " ization)                       )                                                                 \n",
      "                                                                                                  \n",
      " block6b_expand_activation (Act  (None, 10, 10, 1392  0          ['block6b_expand_bn[0][0]']      \n",
      " ivation)                       )                                                                 \n",
      "                                                                                                  \n",
      " block6b_dwconv (DepthwiseConv2  (None, 10, 10, 1392  34800      ['block6b_expand_activation[0][0]\n",
      " D)                             )                                ']                               \n",
      "                                                                                                  \n",
      " block6b_bn (BatchNormalization  (None, 10, 10, 1392  5568       ['block6b_dwconv[0][0]']         \n",
      " )                              )                                                                 \n",
      "                                                                                                  \n",
      " block6b_activation (Activation  (None, 10, 10, 1392  0          ['block6b_bn[0][0]']             \n",
      " )                              )                                                                 \n",
      "                                                                                                  \n",
      " block6b_se_squeeze (GlobalAver  (None, 1392)        0           ['block6b_activation[0][0]']     \n",
      " agePooling2D)                                                                                    \n",
      "                                                                                                  \n",
      " block6b_se_reshape (Reshape)   (None, 1, 1, 1392)   0           ['block6b_se_squeeze[0][0]']     \n",
      "                                                                                                  \n",
      " block6b_se_reduce (Conv2D)     (None, 1, 1, 58)     80794       ['block6b_se_reshape[0][0]']     \n",
      "                                                                                                  \n",
      " block6b_se_expand (Conv2D)     (None, 1, 1, 1392)   82128       ['block6b_se_reduce[0][0]']      \n",
      "                                                                                                  \n",
      " block6b_se_excite (Multiply)   (None, 10, 10, 1392  0           ['block6b_activation[0][0]',     \n",
      "                                )                                 'block6b_se_expand[0][0]']      \n",
      "                                                                                                  \n",
      " block6b_project_conv (Conv2D)  (None, 10, 10, 232)  322944      ['block6b_se_excite[0][0]']      \n",
      "                                                                                                  \n",
      " block6b_project_bn (BatchNorma  (None, 10, 10, 232)  928        ['block6b_project_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block6b_drop (Dropout)         (None, 10, 10, 232)  0           ['block6b_project_bn[0][0]']     \n",
      "                                                                                                  \n",
      " block6b_add (Add)              (None, 10, 10, 232)  0           ['block6b_drop[0][0]',           \n",
      "                                                                  'block6a_project_bn[0][0]']     \n",
      "                                                                                                  \n",
      " block6c_expand_conv (Conv2D)   (None, 10, 10, 1392  322944      ['block6b_add[0][0]']            \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " block6c_expand_bn (BatchNormal  (None, 10, 10, 1392  5568       ['block6c_expand_conv[0][0]']    \n",
      " ization)                       )                                                                 \n",
      "                                                                                                  \n",
      " block6c_expand_activation (Act  (None, 10, 10, 1392  0          ['block6c_expand_bn[0][0]']      \n",
      " ivation)                       )                                                                 \n",
      "                                                                                                  \n",
      " block6c_dwconv (DepthwiseConv2  (None, 10, 10, 1392  34800      ['block6c_expand_activation[0][0]\n",
      " D)                             )                                ']                               \n",
      "                                                                                                  \n",
      " block6c_bn (BatchNormalization  (None, 10, 10, 1392  5568       ['block6c_dwconv[0][0]']         \n",
      " )                              )                                                                 \n",
      "                                                                                                  \n",
      " block6c_activation (Activation  (None, 10, 10, 1392  0          ['block6c_bn[0][0]']             \n",
      " )                              )                                                                 \n",
      "                                                                                                  \n",
      " block6c_se_squeeze (GlobalAver  (None, 1392)        0           ['block6c_activation[0][0]']     \n",
      " agePooling2D)                                                                                    \n",
      "                                                                                                  \n",
      " block6c_se_reshape (Reshape)   (None, 1, 1, 1392)   0           ['block6c_se_squeeze[0][0]']     \n",
      "                                                                                                  \n",
      " block6c_se_reduce (Conv2D)     (None, 1, 1, 58)     80794       ['block6c_se_reshape[0][0]']     \n",
      "                                                                                                  \n",
      " block6c_se_expand (Conv2D)     (None, 1, 1, 1392)   82128       ['block6c_se_reduce[0][0]']      \n",
      "                                                                                                  \n",
      " block6c_se_excite (Multiply)   (None, 10, 10, 1392  0           ['block6c_activation[0][0]',     \n",
      "                                )                                 'block6c_se_expand[0][0]']      \n",
      "                                                                                                  \n",
      " block6c_project_conv (Conv2D)  (None, 10, 10, 232)  322944      ['block6c_se_excite[0][0]']      \n",
      "                                                                                                  \n",
      " block6c_project_bn (BatchNorma  (None, 10, 10, 232)  928        ['block6c_project_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block6c_drop (Dropout)         (None, 10, 10, 232)  0           ['block6c_project_bn[0][0]']     \n",
      "                                                                                                  \n",
      " block6c_add (Add)              (None, 10, 10, 232)  0           ['block6c_drop[0][0]',           \n",
      "                                                                  'block6b_add[0][0]']            \n",
      "                                                                                                  \n",
      " block6d_expand_conv (Conv2D)   (None, 10, 10, 1392  322944      ['block6c_add[0][0]']            \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " block6d_expand_bn (BatchNormal  (None, 10, 10, 1392  5568       ['block6d_expand_conv[0][0]']    \n",
      " ization)                       )                                                                 \n",
      "                                                                                                  \n",
      " block6d_expand_activation (Act  (None, 10, 10, 1392  0          ['block6d_expand_bn[0][0]']      \n",
      " ivation)                       )                                                                 \n",
      "                                                                                                  \n",
      " block6d_dwconv (DepthwiseConv2  (None, 10, 10, 1392  34800      ['block6d_expand_activation[0][0]\n",
      " D)                             )                                ']                               \n",
      "                                                                                                  \n",
      " block6d_bn (BatchNormalization  (None, 10, 10, 1392  5568       ['block6d_dwconv[0][0]']         \n",
      " )                              )                                                                 \n",
      "                                                                                                  \n",
      " block6d_activation (Activation  (None, 10, 10, 1392  0          ['block6d_bn[0][0]']             \n",
      " )                              )                                                                 \n",
      "                                                                                                  \n",
      " block6d_se_squeeze (GlobalAver  (None, 1392)        0           ['block6d_activation[0][0]']     \n",
      " agePooling2D)                                                                                    \n",
      "                                                                                                  \n",
      " block6d_se_reshape (Reshape)   (None, 1, 1, 1392)   0           ['block6d_se_squeeze[0][0]']     \n",
      "                                                                                                  \n",
      " block6d_se_reduce (Conv2D)     (None, 1, 1, 58)     80794       ['block6d_se_reshape[0][0]']     \n",
      "                                                                                                  \n",
      " block6d_se_expand (Conv2D)     (None, 1, 1, 1392)   82128       ['block6d_se_reduce[0][0]']      \n",
      "                                                                                                  \n",
      " block6d_se_excite (Multiply)   (None, 10, 10, 1392  0           ['block6d_activation[0][0]',     \n",
      "                                )                                 'block6d_se_expand[0][0]']      \n",
      "                                                                                                  \n",
      " block6d_project_conv (Conv2D)  (None, 10, 10, 232)  322944      ['block6d_se_excite[0][0]']      \n",
      "                                                                                                  \n",
      " block6d_project_bn (BatchNorma  (None, 10, 10, 232)  928        ['block6d_project_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block6d_drop (Dropout)         (None, 10, 10, 232)  0           ['block6d_project_bn[0][0]']     \n",
      "                                                                                                  \n",
      " block6d_add (Add)              (None, 10, 10, 232)  0           ['block6d_drop[0][0]',           \n",
      "                                                                  'block6c_add[0][0]']            \n",
      "                                                                                                  \n",
      " block6e_expand_conv (Conv2D)   (None, 10, 10, 1392  322944      ['block6d_add[0][0]']            \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " block6e_expand_bn (BatchNormal  (None, 10, 10, 1392  5568       ['block6e_expand_conv[0][0]']    \n",
      " ization)                       )                                                                 \n",
      "                                                                                                  \n",
      " block6e_expand_activation (Act  (None, 10, 10, 1392  0          ['block6e_expand_bn[0][0]']      \n",
      " ivation)                       )                                                                 \n",
      "                                                                                                  \n",
      " block6e_dwconv (DepthwiseConv2  (None, 10, 10, 1392  34800      ['block6e_expand_activation[0][0]\n",
      " D)                             )                                ']                               \n",
      "                                                                                                  \n",
      " block6e_bn (BatchNormalization  (None, 10, 10, 1392  5568       ['block6e_dwconv[0][0]']         \n",
      " )                              )                                                                 \n",
      "                                                                                                  \n",
      " block6e_activation (Activation  (None, 10, 10, 1392  0          ['block6e_bn[0][0]']             \n",
      " )                              )                                                                 \n",
      "                                                                                                  \n",
      " block6e_se_squeeze (GlobalAver  (None, 1392)        0           ['block6e_activation[0][0]']     \n",
      " agePooling2D)                                                                                    \n",
      "                                                                                                  \n",
      " block6e_se_reshape (Reshape)   (None, 1, 1, 1392)   0           ['block6e_se_squeeze[0][0]']     \n",
      "                                                                                                  \n",
      " block6e_se_reduce (Conv2D)     (None, 1, 1, 58)     80794       ['block6e_se_reshape[0][0]']     \n",
      "                                                                                                  \n",
      " block6e_se_expand (Conv2D)     (None, 1, 1, 1392)   82128       ['block6e_se_reduce[0][0]']      \n",
      "                                                                                                  \n",
      " block6e_se_excite (Multiply)   (None, 10, 10, 1392  0           ['block6e_activation[0][0]',     \n",
      "                                )                                 'block6e_se_expand[0][0]']      \n",
      "                                                                                                  \n",
      " block6e_project_conv (Conv2D)  (None, 10, 10, 232)  322944      ['block6e_se_excite[0][0]']      \n",
      "                                                                                                  \n",
      " block6e_project_bn (BatchNorma  (None, 10, 10, 232)  928        ['block6e_project_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block6e_drop (Dropout)         (None, 10, 10, 232)  0           ['block6e_project_bn[0][0]']     \n",
      "                                                                                                  \n",
      " block6e_add (Add)              (None, 10, 10, 232)  0           ['block6e_drop[0][0]',           \n",
      "                                                                  'block6d_add[0][0]']            \n",
      "                                                                                                  \n",
      " block6f_expand_conv (Conv2D)   (None, 10, 10, 1392  322944      ['block6e_add[0][0]']            \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " block6f_expand_bn (BatchNormal  (None, 10, 10, 1392  5568       ['block6f_expand_conv[0][0]']    \n",
      " ization)                       )                                                                 \n",
      "                                                                                                  \n",
      " block6f_expand_activation (Act  (None, 10, 10, 1392  0          ['block6f_expand_bn[0][0]']      \n",
      " ivation)                       )                                                                 \n",
      "                                                                                                  \n",
      " block6f_dwconv (DepthwiseConv2  (None, 10, 10, 1392  34800      ['block6f_expand_activation[0][0]\n",
      " D)                             )                                ']                               \n",
      "                                                                                                  \n",
      " block6f_bn (BatchNormalization  (None, 10, 10, 1392  5568       ['block6f_dwconv[0][0]']         \n",
      " )                              )                                                                 \n",
      "                                                                                                  \n",
      " block6f_activation (Activation  (None, 10, 10, 1392  0          ['block6f_bn[0][0]']             \n",
      " )                              )                                                                 \n",
      "                                                                                                  \n",
      " block6f_se_squeeze (GlobalAver  (None, 1392)        0           ['block6f_activation[0][0]']     \n",
      " agePooling2D)                                                                                    \n",
      "                                                                                                  \n",
      " block6f_se_reshape (Reshape)   (None, 1, 1, 1392)   0           ['block6f_se_squeeze[0][0]']     \n",
      "                                                                                                  \n",
      " block6f_se_reduce (Conv2D)     (None, 1, 1, 58)     80794       ['block6f_se_reshape[0][0]']     \n",
      "                                                                                                  \n",
      " block6f_se_expand (Conv2D)     (None, 1, 1, 1392)   82128       ['block6f_se_reduce[0][0]']      \n",
      "                                                                                                  \n",
      " block6f_se_excite (Multiply)   (None, 10, 10, 1392  0           ['block6f_activation[0][0]',     \n",
      "                                )                                 'block6f_se_expand[0][0]']      \n",
      "                                                                                                  \n",
      " block6f_project_conv (Conv2D)  (None, 10, 10, 232)  322944      ['block6f_se_excite[0][0]']      \n",
      "                                                                                                  \n",
      " block6f_project_bn (BatchNorma  (None, 10, 10, 232)  928        ['block6f_project_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block6f_drop (Dropout)         (None, 10, 10, 232)  0           ['block6f_project_bn[0][0]']     \n",
      "                                                                                                  \n",
      " block6f_add (Add)              (None, 10, 10, 232)  0           ['block6f_drop[0][0]',           \n",
      "                                                                  'block6e_add[0][0]']            \n",
      "                                                                                                  \n",
      " block7a_expand_conv (Conv2D)   (None, 10, 10, 1392  322944      ['block6f_add[0][0]']            \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " block7a_expand_bn (BatchNormal  (None, 10, 10, 1392  5568       ['block7a_expand_conv[0][0]']    \n",
      " ization)                       )                                                                 \n",
      "                                                                                                  \n",
      " block7a_expand_activation (Act  (None, 10, 10, 1392  0          ['block7a_expand_bn[0][0]']      \n",
      " ivation)                       )                                                                 \n",
      "                                                                                                  \n",
      " block7a_dwconv (DepthwiseConv2  (None, 10, 10, 1392  12528      ['block7a_expand_activation[0][0]\n",
      " D)                             )                                ']                               \n",
      "                                                                                                  \n",
      " block7a_bn (BatchNormalization  (None, 10, 10, 1392  5568       ['block7a_dwconv[0][0]']         \n",
      " )                              )                                                                 \n",
      "                                                                                                  \n",
      " block7a_activation (Activation  (None, 10, 10, 1392  0          ['block7a_bn[0][0]']             \n",
      " )                              )                                                                 \n",
      "                                                                                                  \n",
      " block7a_se_squeeze (GlobalAver  (None, 1392)        0           ['block7a_activation[0][0]']     \n",
      " agePooling2D)                                                                                    \n",
      "                                                                                                  \n",
      " block7a_se_reshape (Reshape)   (None, 1, 1, 1392)   0           ['block7a_se_squeeze[0][0]']     \n",
      "                                                                                                  \n",
      " block7a_se_reduce (Conv2D)     (None, 1, 1, 58)     80794       ['block7a_se_reshape[0][0]']     \n",
      "                                                                                                  \n",
      " block7a_se_expand (Conv2D)     (None, 1, 1, 1392)   82128       ['block7a_se_reduce[0][0]']      \n",
      "                                                                                                  \n",
      " block7a_se_excite (Multiply)   (None, 10, 10, 1392  0           ['block7a_activation[0][0]',     \n",
      "                                )                                 'block7a_se_expand[0][0]']      \n",
      "                                                                                                  \n",
      " block7a_project_conv (Conv2D)  (None, 10, 10, 384)  534528      ['block7a_se_excite[0][0]']      \n",
      "                                                                                                  \n",
      " block7a_project_bn (BatchNorma  (None, 10, 10, 384)  1536       ['block7a_project_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block7b_expand_conv (Conv2D)   (None, 10, 10, 2304  884736      ['block7a_project_bn[0][0]']     \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " block7b_expand_bn (BatchNormal  (None, 10, 10, 2304  9216       ['block7b_expand_conv[0][0]']    \n",
      " ization)                       )                                                                 \n",
      "                                                                                                  \n",
      " block7b_expand_activation (Act  (None, 10, 10, 2304  0          ['block7b_expand_bn[0][0]']      \n",
      " ivation)                       )                                                                 \n",
      "                                                                                                  \n",
      " block7b_dwconv (DepthwiseConv2  (None, 10, 10, 2304  20736      ['block7b_expand_activation[0][0]\n",
      " D)                             )                                ']                               \n",
      "                                                                                                  \n",
      " block7b_bn (BatchNormalization  (None, 10, 10, 2304  9216       ['block7b_dwconv[0][0]']         \n",
      " )                              )                                                                 \n",
      "                                                                                                  \n",
      " block7b_activation (Activation  (None, 10, 10, 2304  0          ['block7b_bn[0][0]']             \n",
      " )                              )                                                                 \n",
      "                                                                                                  \n",
      " block7b_se_squeeze (GlobalAver  (None, 2304)        0           ['block7b_activation[0][0]']     \n",
      " agePooling2D)                                                                                    \n",
      "                                                                                                  \n",
      " block7b_se_reshape (Reshape)   (None, 1, 1, 2304)   0           ['block7b_se_squeeze[0][0]']     \n",
      "                                                                                                  \n",
      " block7b_se_reduce (Conv2D)     (None, 1, 1, 96)     221280      ['block7b_se_reshape[0][0]']     \n",
      "                                                                                                  \n",
      " block7b_se_expand (Conv2D)     (None, 1, 1, 2304)   223488      ['block7b_se_reduce[0][0]']      \n",
      "                                                                                                  \n",
      " block7b_se_excite (Multiply)   (None, 10, 10, 2304  0           ['block7b_activation[0][0]',     \n",
      "                                )                                 'block7b_se_expand[0][0]']      \n",
      "                                                                                                  \n",
      " block7b_project_conv (Conv2D)  (None, 10, 10, 384)  884736      ['block7b_se_excite[0][0]']      \n",
      "                                                                                                  \n",
      " block7b_project_bn (BatchNorma  (None, 10, 10, 384)  1536       ['block7b_project_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block7b_drop (Dropout)         (None, 10, 10, 384)  0           ['block7b_project_bn[0][0]']     \n",
      "                                                                                                  \n",
      " block7b_add (Add)              (None, 10, 10, 384)  0           ['block7b_drop[0][0]',           \n",
      "                                                                  'block7a_project_bn[0][0]']     \n",
      "                                                                                                  \n",
      " top_conv (Conv2D)              (None, 10, 10, 1536  589824      ['block7b_add[0][0]']            \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " top_bn (BatchNormalization)    (None, 10, 10, 1536  6144        ['top_conv[0][0]']               \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " top_activation (Activation)    (None, 10, 10, 1536  0           ['top_bn[0][0]']                 \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " avg_pool (GlobalAveragePooling  (None, 1536)        0           ['top_activation[0][0]']         \n",
      " 2D)                                                                                              \n",
      "                                                                                                  \n",
      " softmax (Dense)                (None, 3)            4611        ['avg_pool[0][0]']               \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 10,787,784\n",
      "Trainable params: 10,700,483\n",
      "Non-trainable params: 87,301\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input = Input(shape= (300,300,2))\n",
    "model = EfficientNetB3(\n",
    "    input_tensor= input,\n",
    "    include_top= False,\n",
    "    weights= None,\n",
    "    pooling= 'avg'\n",
    ")\n",
    "\n",
    "x = model.output\n",
    "#x = Dropout(0,2)(x)\n",
    "x = Dense(3, activation= 'softmax', name= 'softmax')(x)\n",
    "\n",
    "model = Model(model.input, x)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf.keras.utils.plot_model(model, show_shapes= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "import datetime\n",
    "\n",
    "log_dir = \"logs_EffiB1/my_board/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir= log_dir, histogram_freq= 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118\n",
      "1061\n",
      "118\n",
      "1061\n",
      "118\n",
      "1061\n",
      "118\n",
      "1061\n",
      "118\n",
      "1061\n",
      "118\n",
      "1061\n",
      "118\n",
      "1061\n",
      "118\n",
      "1061\n",
      "118\n",
      "1061\n",
      "117\n",
      "1062\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "kfold = KFold(n_splits=10, shuffle= True, random_state= seed)\n",
    "\n",
    "test= []\n",
    "train= []\n",
    "test_= []\n",
    "train_= []\n",
    "\n",
    "for train, test in kfold.split(inputs, targets):\n",
    "    \n",
    "    print(len(test))\n",
    "    print(len(train))\n",
    "    \n",
    "    for i in zip(test):\n",
    "        test_.append(i)\n",
    "    for i in zip(train):\n",
    "        train_.append(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train_[0:1061]\n",
    "train = np.reshape(train, 1061)\n",
    "test = test_[0:117]\n",
    "test = np.reshape(test, 117)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1179, 300, 300, 2)\n",
      "(1179, 3)\n",
      "(1179,)\n"
     ]
    }
   ],
   "source": [
    "print(inputs.shape)\n",
    "print(targets.shape)\n",
    "print(index.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(848,) (95,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jwhyu\\anaconda3\\envs\\venv\\lib\\site-packages\\keras\\optimizer_v2\\gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------\n",
      "Training for fold 1 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jwhyu\\anaconda3\\envs\\venv\\lib\\site-packages\\keras\\engine\\functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  layer_config = serialize_layer_fn(layer)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "212/212 [==============================] - 45s 168ms/step - loss: 0.4993 - accuracy: 0.6521 - val_loss: 0.9864 - val_accuracy: 0.3368\n",
      "Epoch 2/100\n",
      "212/212 [==============================] - 34s 161ms/step - loss: 0.4141 - accuracy: 0.7288 - val_loss: 0.3936 - val_accuracy: 0.6842\n",
      "Epoch 3/100\n",
      "212/212 [==============================] - 34s 161ms/step - loss: 0.4053 - accuracy: 0.7417 - val_loss: 58.5074 - val_accuracy: 0.6421\n",
      "Epoch 4/100\n",
      "212/212 [==============================] - 33s 156ms/step - loss: 0.3406 - accuracy: 0.7877 - val_loss: 5.5401 - val_accuracy: 0.6842\n",
      "Epoch 5/100\n",
      "212/212 [==============================] - 33s 155ms/step - loss: 0.3061 - accuracy: 0.8255 - val_loss: 1.7826 - val_accuracy: 0.8000\n",
      "Epoch 6/100\n",
      "212/212 [==============================] - 33s 155ms/step - loss: 0.3096 - accuracy: 0.8054 - val_loss: 0.0970 - val_accuracy: 0.9263\n",
      "Epoch 7/100\n",
      "212/212 [==============================] - 33s 157ms/step - loss: 0.2831 - accuracy: 0.8219 - val_loss: 0.1443 - val_accuracy: 0.8842\n",
      "Epoch 8/100\n",
      "212/212 [==============================] - 33s 157ms/step - loss: 0.2741 - accuracy: 0.8231 - val_loss: 0.8340 - val_accuracy: 0.8000\n",
      "Epoch 9/100\n",
      "212/212 [==============================] - 34s 157ms/step - loss: 0.2741 - accuracy: 0.8290 - val_loss: 0.3840 - val_accuracy: 0.6947\n",
      "Epoch 10/100\n",
      "212/212 [==============================] - 34s 160ms/step - loss: 0.2789 - accuracy: 0.8349 - val_loss: 0.1538 - val_accuracy: 0.9158\n",
      "Epoch 11/100\n",
      "212/212 [==============================] - 34s 159ms/step - loss: 0.2522 - accuracy: 0.8420 - val_loss: 0.1752 - val_accuracy: 0.8737\n",
      "Epoch 12/100\n",
      "212/212 [==============================] - 34s 159ms/step - loss: 0.2679 - accuracy: 0.8420 - val_loss: 0.1278 - val_accuracy: 0.9368\n",
      "Epoch 13/100\n",
      "212/212 [==============================] - 34s 159ms/step - loss: 0.2569 - accuracy: 0.8455 - val_loss: 0.1549 - val_accuracy: 0.8947\n",
      "Epoch 14/100\n",
      "212/212 [==============================] - 34s 160ms/step - loss: 0.2230 - accuracy: 0.8703 - val_loss: 0.1107 - val_accuracy: 0.9474\n",
      "Epoch 15/100\n",
      "212/212 [==============================] - 34s 160ms/step - loss: 0.2270 - accuracy: 0.8632 - val_loss: 0.2981 - val_accuracy: 0.8526\n",
      "Epoch 16/100\n",
      "212/212 [==============================] - 34s 159ms/step - loss: 0.2370 - accuracy: 0.8585 - val_loss: 0.0994 - val_accuracy: 0.9368\n",
      "Epoch 17/100\n",
      "212/212 [==============================] - 34s 160ms/step - loss: 0.2212 - accuracy: 0.8597 - val_loss: 0.1747 - val_accuracy: 0.9053\n",
      "Epoch 18/100\n",
      "212/212 [==============================] - 34s 159ms/step - loss: 0.2128 - accuracy: 0.8644 - val_loss: 0.1339 - val_accuracy: 0.9053\n",
      "Epoch 19/100\n",
      "212/212 [==============================] - 34s 160ms/step - loss: 0.2111 - accuracy: 0.8738 - val_loss: 0.4898 - val_accuracy: 0.7789\n",
      "Epoch 20/100\n",
      "212/212 [==============================] - 34s 159ms/step - loss: 0.2204 - accuracy: 0.8597 - val_loss: 0.2182 - val_accuracy: 0.8632\n",
      "Epoch 21/100\n",
      "212/212 [==============================] - 34s 160ms/step - loss: 0.2028 - accuracy: 0.8703 - val_loss: 0.1846 - val_accuracy: 0.8842\n",
      "Epoch 22/100\n",
      "212/212 [==============================] - 34s 160ms/step - loss: 0.1845 - accuracy: 0.8880 - val_loss: 0.1682 - val_accuracy: 0.9053\n",
      "Epoch 23/100\n",
      "212/212 [==============================] - 34s 160ms/step - loss: 0.1902 - accuracy: 0.8927 - val_loss: 0.1097 - val_accuracy: 0.9368\n",
      "Epoch 24/100\n",
      "212/212 [==============================] - 34s 160ms/step - loss: 0.1995 - accuracy: 0.8644 - val_loss: 0.2327 - val_accuracy: 0.8737\n",
      "Epoch 25/100\n",
      "212/212 [==============================] - 34s 160ms/step - loss: 0.1712 - accuracy: 0.8868 - val_loss: 0.0975 - val_accuracy: 0.9368\n",
      "Epoch 26/100\n",
      "212/212 [==============================] - 34s 160ms/step - loss: 0.1479 - accuracy: 0.9139 - val_loss: 0.1526 - val_accuracy: 0.8947\n",
      "Epoch 27/100\n",
      "212/212 [==============================] - 34s 159ms/step - loss: 0.1568 - accuracy: 0.9057 - val_loss: 0.1401 - val_accuracy: 0.9158\n",
      "Epoch 28/100\n",
      "212/212 [==============================] - 34s 160ms/step - loss: 0.1458 - accuracy: 0.9116 - val_loss: 0.1511 - val_accuracy: 0.8737\n",
      "Epoch 29/100\n",
      "212/212 [==============================] - 34s 160ms/step - loss: 0.1536 - accuracy: 0.9057 - val_loss: 0.3360 - val_accuracy: 0.8000\n",
      "Epoch 30/100\n",
      "212/212 [==============================] - 34s 162ms/step - loss: 0.1731 - accuracy: 0.8962 - val_loss: 0.1532 - val_accuracy: 0.9263\n",
      "Epoch 31/100\n",
      "212/212 [==============================] - 34s 162ms/step - loss: 0.1478 - accuracy: 0.9045 - val_loss: 0.1440 - val_accuracy: 0.8632\n",
      "Epoch 32/100\n",
      "212/212 [==============================] - 35s 164ms/step - loss: 0.1555 - accuracy: 0.9163 - val_loss: 0.1202 - val_accuracy: 0.9263\n",
      "Epoch 33/100\n",
      "212/212 [==============================] - 34s 162ms/step - loss: 0.1789 - accuracy: 0.8950 - val_loss: 0.1068 - val_accuracy: 0.9474\n",
      "Epoch 34/100\n",
      "212/212 [==============================] - 34s 161ms/step - loss: 0.1363 - accuracy: 0.9281 - val_loss: 0.2770 - val_accuracy: 0.9263\n",
      "Epoch 35/100\n",
      "212/212 [==============================] - 34s 161ms/step - loss: 0.1540 - accuracy: 0.9151 - val_loss: 0.1503 - val_accuracy: 0.8842\n",
      "Epoch 36/100\n",
      "212/212 [==============================] - 34s 161ms/step - loss: 0.1210 - accuracy: 0.9292 - val_loss: 0.1903 - val_accuracy: 0.8842\n",
      "Epoch 37/100\n",
      "212/212 [==============================] - 34s 161ms/step - loss: 0.1231 - accuracy: 0.9410 - val_loss: 0.1467 - val_accuracy: 0.9474\n",
      "Epoch 38/100\n",
      "212/212 [==============================] - 34s 160ms/step - loss: 0.1416 - accuracy: 0.9281 - val_loss: 0.1858 - val_accuracy: 0.8632\n",
      "Epoch 39/100\n",
      "212/212 [==============================] - 34s 162ms/step - loss: 0.1380 - accuracy: 0.9304 - val_loss: 0.1530 - val_accuracy: 0.9263\n",
      "Epoch 40/100\n",
      "212/212 [==============================] - 34s 160ms/step - loss: 0.1392 - accuracy: 0.9292 - val_loss: 0.1530 - val_accuracy: 0.9053\n",
      "Epoch 41/100\n",
      "212/212 [==============================] - 34s 157ms/step - loss: 0.1128 - accuracy: 0.9458 - val_loss: 0.1649 - val_accuracy: 0.9053\n",
      "Epoch 42/100\n",
      "212/212 [==============================] - 34s 162ms/step - loss: 0.1237 - accuracy: 0.9387 - val_loss: 0.1406 - val_accuracy: 0.9263\n",
      "Epoch 43/100\n",
      "212/212 [==============================] - 34s 161ms/step - loss: 0.1169 - accuracy: 0.9422 - val_loss: 0.1539 - val_accuracy: 0.9263\n",
      "Epoch 44/100\n",
      "212/212 [==============================] - 34s 161ms/step - loss: 0.0945 - accuracy: 0.9493 - val_loss: 0.1179 - val_accuracy: 0.9053\n",
      "Epoch 45/100\n",
      "212/212 [==============================] - 34s 162ms/step - loss: 0.0853 - accuracy: 0.9670 - val_loss: 0.0975 - val_accuracy: 0.9684\n",
      "Epoch 46/100\n",
      "212/212 [==============================] - 34s 161ms/step - loss: 0.0713 - accuracy: 0.9717 - val_loss: 0.1677 - val_accuracy: 0.8947\n",
      "Epoch 47/100\n",
      "212/212 [==============================] - 35s 166ms/step - loss: 0.1207 - accuracy: 0.9422 - val_loss: 0.1673 - val_accuracy: 0.9368\n",
      "Epoch 48/100\n",
      "212/212 [==============================] - 34s 161ms/step - loss: 0.1067 - accuracy: 0.9493 - val_loss: 0.1557 - val_accuracy: 0.8737\n",
      "Epoch 49/100\n",
      "212/212 [==============================] - 34s 160ms/step - loss: 0.0781 - accuracy: 0.9658 - val_loss: 0.1517 - val_accuracy: 0.9158\n",
      "Epoch 50/100\n",
      "212/212 [==============================] - 34s 160ms/step - loss: 0.0744 - accuracy: 0.9658 - val_loss: 0.1289 - val_accuracy: 0.9158\n",
      "Epoch 51/100\n",
      "212/212 [==============================] - 34s 160ms/step - loss: 0.0797 - accuracy: 0.9646 - val_loss: 0.1417 - val_accuracy: 0.9158\n",
      "Epoch 52/100\n",
      "212/212 [==============================] - 34s 160ms/step - loss: 0.0967 - accuracy: 0.9599 - val_loss: 0.1384 - val_accuracy: 0.8947\n",
      "Epoch 53/100\n",
      "212/212 [==============================] - 34s 160ms/step - loss: 0.0939 - accuracy: 0.9599 - val_loss: 0.1329 - val_accuracy: 0.9368\n",
      "Epoch 54/100\n",
      "212/212 [==============================] - 34s 159ms/step - loss: 0.1232 - accuracy: 0.9387 - val_loss: 0.1376 - val_accuracy: 0.9263\n",
      "Epoch 55/100\n",
      "212/212 [==============================] - 34s 159ms/step - loss: 0.0914 - accuracy: 0.9634 - val_loss: 0.1122 - val_accuracy: 0.9368\n",
      "Epoch 56/100\n",
      "212/212 [==============================] - 34s 160ms/step - loss: 0.0865 - accuracy: 0.9575 - val_loss: 0.1070 - val_accuracy: 0.9368\n",
      "Epoch 57/100\n",
      "212/212 [==============================] - 34s 160ms/step - loss: 0.0898 - accuracy: 0.9587 - val_loss: 0.1242 - val_accuracy: 0.9158\n",
      "Epoch 58/100\n",
      "212/212 [==============================] - 34s 160ms/step - loss: 0.0712 - accuracy: 0.9717 - val_loss: 0.1113 - val_accuracy: 0.9579\n",
      "Epoch 59/100\n",
      "212/212 [==============================] - 34s 160ms/step - loss: 0.0713 - accuracy: 0.9717 - val_loss: 0.1486 - val_accuracy: 0.9158\n",
      "Epoch 60/100\n",
      "212/212 [==============================] - 34s 160ms/step - loss: 0.0955 - accuracy: 0.9599 - val_loss: 0.1059 - val_accuracy: 0.9474\n",
      "Epoch 61/100\n",
      "212/212 [==============================] - 34s 160ms/step - loss: 0.0852 - accuracy: 0.9540 - val_loss: 0.1299 - val_accuracy: 0.9263\n",
      "Epoch 62/100\n",
      "212/212 [==============================] - 34s 159ms/step - loss: 0.0656 - accuracy: 0.9752 - val_loss: 0.1609 - val_accuracy: 0.9053\n",
      "Epoch 63/100\n",
      "212/212 [==============================] - 34s 160ms/step - loss: 0.0942 - accuracy: 0.9575 - val_loss: 0.1463 - val_accuracy: 0.9368\n",
      "Epoch 64/100\n",
      "212/212 [==============================] - 34s 159ms/step - loss: 0.0774 - accuracy: 0.9682 - val_loss: 0.1178 - val_accuracy: 0.9263\n",
      "Epoch 65/100\n",
      "212/212 [==============================] - 34s 159ms/step - loss: 0.0852 - accuracy: 0.9717 - val_loss: 0.1451 - val_accuracy: 0.9158\n",
      "Epoch 66/100\n",
      "212/212 [==============================] - 34s 160ms/step - loss: 0.0689 - accuracy: 0.9764 - val_loss: 0.1197 - val_accuracy: 0.9263\n",
      "Epoch 67/100\n",
      "212/212 [==============================] - 34s 160ms/step - loss: 0.0794 - accuracy: 0.9693 - val_loss: 0.1232 - val_accuracy: 0.9368\n",
      "Epoch 68/100\n",
      "212/212 [==============================] - 34s 159ms/step - loss: 0.0802 - accuracy: 0.9693 - val_loss: 0.1043 - val_accuracy: 0.9368\n",
      "Epoch 69/100\n",
      "212/212 [==============================] - 34s 160ms/step - loss: 0.0534 - accuracy: 0.9811 - val_loss: 0.1143 - val_accuracy: 0.9263\n",
      "Epoch 70/100\n",
      "212/212 [==============================] - 34s 160ms/step - loss: 0.0688 - accuracy: 0.9646 - val_loss: 0.1344 - val_accuracy: 0.9158\n",
      "Epoch 71/100\n",
      "212/212 [==============================] - 34s 160ms/step - loss: 0.0645 - accuracy: 0.9717 - val_loss: 0.0972 - val_accuracy: 0.9474\n",
      "Epoch 72/100\n",
      "212/212 [==============================] - 34s 160ms/step - loss: 0.0598 - accuracy: 0.9729 - val_loss: 0.1460 - val_accuracy: 0.9158\n",
      "Epoch 73/100\n",
      "212/212 [==============================] - 35s 164ms/step - loss: 0.0855 - accuracy: 0.9611 - val_loss: 0.1557 - val_accuracy: 0.9263\n",
      "Epoch 74/100\n",
      "212/212 [==============================] - 35s 163ms/step - loss: 0.0919 - accuracy: 0.9552 - val_loss: 0.1043 - val_accuracy: 0.9368\n",
      "Epoch 75/100\n",
      "212/212 [==============================] - 35s 162ms/step - loss: 0.0576 - accuracy: 0.9776 - val_loss: 0.1059 - val_accuracy: 0.9474\n",
      "Epoch 76/100\n",
      "212/212 [==============================] - 34s 162ms/step - loss: 0.0539 - accuracy: 0.9811 - val_loss: 0.1257 - val_accuracy: 0.9263\n",
      "Epoch 77/100\n",
      "212/212 [==============================] - 34s 161ms/step - loss: 0.0717 - accuracy: 0.9658 - val_loss: 0.1088 - val_accuracy: 0.9368\n",
      "Epoch 78/100\n",
      "212/212 [==============================] - 34s 160ms/step - loss: 0.0694 - accuracy: 0.9646 - val_loss: 0.1054 - val_accuracy: 0.9474\n",
      "Epoch 79/100\n",
      "212/212 [==============================] - 34s 161ms/step - loss: 0.0718 - accuracy: 0.9670 - val_loss: 0.1188 - val_accuracy: 0.9474\n",
      "Epoch 80/100\n",
      "212/212 [==============================] - 34s 161ms/step - loss: 0.0868 - accuracy: 0.9575 - val_loss: 0.1131 - val_accuracy: 0.9368\n",
      "Epoch 81/100\n",
      "212/212 [==============================] - 35s 163ms/step - loss: 0.0554 - accuracy: 0.9717 - val_loss: 0.1104 - val_accuracy: 0.9474\n",
      "Epoch 82/100\n",
      "212/212 [==============================] - 34s 161ms/step - loss: 0.0680 - accuracy: 0.9693 - val_loss: 0.1022 - val_accuracy: 0.9474\n",
      "Epoch 83/100\n",
      "212/212 [==============================] - 34s 161ms/step - loss: 0.0894 - accuracy: 0.9623 - val_loss: 0.0946 - val_accuracy: 0.9263\n",
      "Epoch 84/100\n",
      "212/212 [==============================] - 34s 162ms/step - loss: 0.0728 - accuracy: 0.9611 - val_loss: 0.1057 - val_accuracy: 0.9053\n",
      "Epoch 85/100\n",
      "212/212 [==============================] - 35s 162ms/step - loss: 0.0840 - accuracy: 0.9611 - val_loss: 0.0945 - val_accuracy: 0.9158\n",
      "Epoch 86/100\n",
      "212/212 [==============================] - 35s 162ms/step - loss: 0.0587 - accuracy: 0.9705 - val_loss: 0.1143 - val_accuracy: 0.9263\n",
      "Epoch 87/100\n",
      "212/212 [==============================] - 35s 163ms/step - loss: 0.0532 - accuracy: 0.9764 - val_loss: 0.1079 - val_accuracy: 0.9368\n",
      "Epoch 88/100\n",
      "212/212 [==============================] - 35s 163ms/step - loss: 0.0794 - accuracy: 0.9646 - val_loss: 0.1434 - val_accuracy: 0.9263\n",
      "Epoch 89/100\n",
      "212/212 [==============================] - 35s 163ms/step - loss: 0.0573 - accuracy: 0.9717 - val_loss: 0.1041 - val_accuracy: 0.9474\n",
      "Epoch 90/100\n",
      "212/212 [==============================] - 35s 163ms/step - loss: 0.0575 - accuracy: 0.9788 - val_loss: 0.1120 - val_accuracy: 0.9368\n",
      "Epoch 91/100\n",
      "212/212 [==============================] - 35s 163ms/step - loss: 0.0725 - accuracy: 0.9623 - val_loss: 0.0996 - val_accuracy: 0.9368\n",
      "Epoch 92/100\n",
      "212/212 [==============================] - 34s 162ms/step - loss: 0.0812 - accuracy: 0.9658 - val_loss: 0.1019 - val_accuracy: 0.9474\n",
      "Epoch 93/100\n",
      "212/212 [==============================] - 34s 161ms/step - loss: 0.0783 - accuracy: 0.9611 - val_loss: 0.0967 - val_accuracy: 0.9474\n",
      "Epoch 94/100\n",
      "212/212 [==============================] - 34s 161ms/step - loss: 0.0863 - accuracy: 0.9517 - val_loss: 0.1231 - val_accuracy: 0.9053\n",
      "Epoch 95/100\n",
      "212/212 [==============================] - 34s 161ms/step - loss: 0.0725 - accuracy: 0.9741 - val_loss: 0.1044 - val_accuracy: 0.9368\n",
      "Epoch 96/100\n",
      "212/212 [==============================] - 34s 161ms/step - loss: 0.0652 - accuracy: 0.9670 - val_loss: 0.1131 - val_accuracy: 0.9263\n",
      "Epoch 97/100\n",
      "212/212 [==============================] - 34s 161ms/step - loss: 0.0503 - accuracy: 0.9776 - val_loss: 0.1051 - val_accuracy: 0.9474\n",
      "Epoch 98/100\n",
      "212/212 [==============================] - 34s 162ms/step - loss: 0.0816 - accuracy: 0.9552 - val_loss: 0.1100 - val_accuracy: 0.9263\n",
      "Epoch 99/100\n",
      "212/212 [==============================] - 35s 163ms/step - loss: 0.0394 - accuracy: 0.9917 - val_loss: 0.0985 - val_accuracy: 0.9368\n",
      "Epoch 100/100\n",
      "212/212 [==============================] - 34s 161ms/step - loss: 0.0529 - accuracy: 0.9800 - val_loss: 0.1046 - val_accuracy: 0.9368\n",
      "Score for fold 1: loss of 0.10457567125558853; accuracy of 93.68420839309692%\n",
      "(848,) (95,)\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 2 ...\n",
      "Epoch 1/100\n",
      "212/212 [==============================] - 42s 168ms/step - loss: 0.4738 - accuracy: 0.6592 - val_loss: 2.4729 - val_accuracy: 0.3368\n",
      "Epoch 2/100\n",
      "212/212 [==============================] - 34s 161ms/step - loss: 0.3974 - accuracy: 0.7264 - val_loss: 0.3631 - val_accuracy: 0.7158\n",
      "Epoch 3/100\n",
      "212/212 [==============================] - 35s 162ms/step - loss: 0.3641 - accuracy: 0.7771 - val_loss: 0.5603 - val_accuracy: 0.8105\n",
      "Epoch 4/100\n",
      "212/212 [==============================] - 35s 163ms/step - loss: 0.3399 - accuracy: 0.7830 - val_loss: 1.3212 - val_accuracy: 0.8737\n",
      "Epoch 5/100\n",
      "212/212 [==============================] - 34s 161ms/step - loss: 0.3317 - accuracy: 0.7830 - val_loss: 1.0042 - val_accuracy: 0.7263\n",
      "Epoch 6/100\n",
      "212/212 [==============================] - 34s 161ms/step - loss: 0.2907 - accuracy: 0.8337 - val_loss: 0.2536 - val_accuracy: 0.9053\n",
      "Epoch 7/100\n",
      "212/212 [==============================] - 34s 160ms/step - loss: 0.3004 - accuracy: 0.8090 - val_loss: 0.3387 - val_accuracy: 0.8316\n",
      "Epoch 8/100\n",
      "212/212 [==============================] - 34s 161ms/step - loss: 0.2594 - accuracy: 0.8585 - val_loss: 0.2053 - val_accuracy: 0.9158\n",
      "Epoch 9/100\n",
      "212/212 [==============================] - 34s 159ms/step - loss: 0.2623 - accuracy: 0.8361 - val_loss: 0.2354 - val_accuracy: 0.8737\n",
      "Epoch 10/100\n",
      "212/212 [==============================] - 34s 159ms/step - loss: 0.2630 - accuracy: 0.8325 - val_loss: 0.2273 - val_accuracy: 0.8632\n",
      "Epoch 11/100\n",
      "212/212 [==============================] - 34s 160ms/step - loss: 0.2280 - accuracy: 0.8644 - val_loss: 0.1964 - val_accuracy: 0.8737\n",
      "Epoch 12/100\n",
      "212/212 [==============================] - 34s 159ms/step - loss: 0.2275 - accuracy: 0.8538 - val_loss: 0.2674 - val_accuracy: 0.8211\n",
      "Epoch 13/100\n",
      "212/212 [==============================] - 34s 160ms/step - loss: 0.2797 - accuracy: 0.8302 - val_loss: 0.5436 - val_accuracy: 0.8000\n",
      "Epoch 14/100\n",
      "212/212 [==============================] - 34s 160ms/step - loss: 0.2361 - accuracy: 0.8561 - val_loss: 0.1559 - val_accuracy: 0.9474\n",
      "Epoch 15/100\n",
      "212/212 [==============================] - 34s 159ms/step - loss: 0.2300 - accuracy: 0.8561 - val_loss: 0.1407 - val_accuracy: 0.9368\n",
      "Epoch 16/100\n",
      "212/212 [==============================] - 34s 159ms/step - loss: 0.2172 - accuracy: 0.8762 - val_loss: 0.2130 - val_accuracy: 0.8632\n",
      "Epoch 17/100\n",
      "212/212 [==============================] - 34s 160ms/step - loss: 0.2023 - accuracy: 0.8703 - val_loss: 0.3504 - val_accuracy: 0.8000\n",
      "Epoch 18/100\n",
      "212/212 [==============================] - 34s 160ms/step - loss: 0.2135 - accuracy: 0.8691 - val_loss: 0.1628 - val_accuracy: 0.9158\n",
      "Epoch 19/100\n",
      "212/212 [==============================] - 34s 159ms/step - loss: 0.1928 - accuracy: 0.8856 - val_loss: 0.1436 - val_accuracy: 0.9263\n",
      "Epoch 20/100\n",
      "212/212 [==============================] - 34s 160ms/step - loss: 0.1676 - accuracy: 0.8998 - val_loss: 0.1827 - val_accuracy: 0.9053\n",
      "Epoch 21/100\n",
      "212/212 [==============================] - 34s 159ms/step - loss: 0.1756 - accuracy: 0.8939 - val_loss: 0.2381 - val_accuracy: 0.8737\n",
      "Epoch 22/100\n",
      "212/212 [==============================] - 34s 159ms/step - loss: 0.1806 - accuracy: 0.8950 - val_loss: 0.1705 - val_accuracy: 0.9158\n",
      "Epoch 23/100\n",
      "212/212 [==============================] - 34s 159ms/step - loss: 0.1611 - accuracy: 0.8974 - val_loss: 0.1676 - val_accuracy: 0.9158\n",
      "Epoch 24/100\n",
      "212/212 [==============================] - 34s 160ms/step - loss: 0.1640 - accuracy: 0.9057 - val_loss: 0.4290 - val_accuracy: 0.7263\n",
      "Epoch 25/100\n",
      "212/212 [==============================] - 34s 160ms/step - loss: 0.1874 - accuracy: 0.8903 - val_loss: 0.1736 - val_accuracy: 0.9053\n",
      "Epoch 26/100\n",
      "212/212 [==============================] - 34s 160ms/step - loss: 0.1633 - accuracy: 0.8998 - val_loss: 0.2504 - val_accuracy: 0.8526\n",
      "Epoch 27/100\n",
      "212/212 [==============================] - 34s 160ms/step - loss: 0.1892 - accuracy: 0.8809 - val_loss: 0.1489 - val_accuracy: 0.9368\n",
      "Epoch 28/100\n",
      "212/212 [==============================] - 34s 160ms/step - loss: 0.1164 - accuracy: 0.9410 - val_loss: 0.1416 - val_accuracy: 0.9579\n",
      "Epoch 29/100\n",
      "212/212 [==============================] - 34s 160ms/step - loss: 0.1668 - accuracy: 0.9033 - val_loss: 0.1810 - val_accuracy: 0.9368\n",
      "Epoch 30/100\n",
      "212/212 [==============================] - 34s 159ms/step - loss: 0.1548 - accuracy: 0.9021 - val_loss: 0.1044 - val_accuracy: 0.9684\n",
      "Epoch 31/100\n",
      "212/212 [==============================] - 34s 160ms/step - loss: 0.1683 - accuracy: 0.9021 - val_loss: 0.1909 - val_accuracy: 0.8947\n",
      "Epoch 32/100\n",
      "212/212 [==============================] - 34s 160ms/step - loss: 0.1381 - accuracy: 0.9210 - val_loss: 0.1499 - val_accuracy: 0.9474\n",
      "Epoch 33/100\n",
      "212/212 [==============================] - 34s 160ms/step - loss: 0.1156 - accuracy: 0.9387 - val_loss: 0.1611 - val_accuracy: 0.9263\n",
      "Epoch 34/100\n",
      "212/212 [==============================] - 34s 160ms/step - loss: 0.1195 - accuracy: 0.9422 - val_loss: 0.2258 - val_accuracy: 0.8842\n",
      "Epoch 35/100\n",
      "212/212 [==============================] - 34s 160ms/step - loss: 0.1220 - accuracy: 0.9387 - val_loss: 0.2369 - val_accuracy: 0.9263\n",
      "Epoch 36/100\n",
      "212/212 [==============================] - 34s 160ms/step - loss: 0.1185 - accuracy: 0.9446 - val_loss: 0.1751 - val_accuracy: 0.9263\n",
      "Epoch 37/100\n",
      "212/212 [==============================] - 34s 160ms/step - loss: 0.1186 - accuracy: 0.9399 - val_loss: 0.1518 - val_accuracy: 0.9158\n",
      "Epoch 38/100\n",
      "212/212 [==============================] - 34s 161ms/step - loss: 0.1269 - accuracy: 0.9410 - val_loss: 0.1806 - val_accuracy: 0.9263\n",
      "Epoch 39/100\n",
      "212/212 [==============================] - 34s 160ms/step - loss: 0.1388 - accuracy: 0.9281 - val_loss: 0.1698 - val_accuracy: 0.9053\n",
      "Epoch 40/100\n",
      "212/212 [==============================] - 34s 160ms/step - loss: 0.0994 - accuracy: 0.9552 - val_loss: 0.1762 - val_accuracy: 0.9263\n",
      "Epoch 41/100\n",
      "212/212 [==============================] - 34s 160ms/step - loss: 0.1038 - accuracy: 0.9505 - val_loss: 0.1495 - val_accuracy: 0.9368\n",
      "Epoch 42/100\n",
      "212/212 [==============================] - 34s 160ms/step - loss: 0.1216 - accuracy: 0.9481 - val_loss: 0.1709 - val_accuracy: 0.9158\n",
      "Epoch 43/100\n",
      "212/212 [==============================] - 34s 160ms/step - loss: 0.1151 - accuracy: 0.9469 - val_loss: 0.2512 - val_accuracy: 0.8947\n",
      "Epoch 44/100\n",
      "212/212 [==============================] - 34s 160ms/step - loss: 0.1374 - accuracy: 0.9387 - val_loss: 0.1988 - val_accuracy: 0.9263\n",
      "Epoch 45/100\n",
      "212/212 [==============================] - 34s 160ms/step - loss: 0.0914 - accuracy: 0.9599 - val_loss: 0.2242 - val_accuracy: 0.9053\n",
      "Epoch 46/100\n",
      "212/212 [==============================] - 34s 160ms/step - loss: 0.1065 - accuracy: 0.9422 - val_loss: 0.2003 - val_accuracy: 0.9368\n",
      "Epoch 47/100\n",
      "212/212 [==============================] - 34s 159ms/step - loss: 0.1178 - accuracy: 0.9469 - val_loss: 0.1684 - val_accuracy: 0.9368\n",
      "Epoch 48/100\n",
      "212/212 [==============================] - 34s 160ms/step - loss: 0.0756 - accuracy: 0.9658 - val_loss: 0.2381 - val_accuracy: 0.9158\n",
      "Epoch 49/100\n",
      "212/212 [==============================] - 34s 159ms/step - loss: 0.0847 - accuracy: 0.9587 - val_loss: 0.1698 - val_accuracy: 0.9158\n",
      "Epoch 50/100\n",
      "212/212 [==============================] - 34s 160ms/step - loss: 0.1152 - accuracy: 0.9481 - val_loss: 0.2048 - val_accuracy: 0.9158\n",
      "Epoch 51/100\n",
      "212/212 [==============================] - 34s 160ms/step - loss: 0.1085 - accuracy: 0.9517 - val_loss: 0.1730 - val_accuracy: 0.9263\n",
      "Epoch 52/100\n",
      "212/212 [==============================] - 34s 160ms/step - loss: 0.0903 - accuracy: 0.9599 - val_loss: 0.1850 - val_accuracy: 0.9263\n",
      "Epoch 53/100\n",
      "212/212 [==============================] - 34s 160ms/step - loss: 0.0768 - accuracy: 0.9705 - val_loss: 0.1479 - val_accuracy: 0.9263\n",
      "Epoch 54/100\n",
      "212/212 [==============================] - 34s 160ms/step - loss: 0.0824 - accuracy: 0.9646 - val_loss: 0.1778 - val_accuracy: 0.9263\n",
      "Epoch 55/100\n",
      "212/212 [==============================] - 34s 160ms/step - loss: 0.0867 - accuracy: 0.9611 - val_loss: 0.1854 - val_accuracy: 0.9158\n",
      "Epoch 56/100\n",
      "212/212 [==============================] - 34s 160ms/step - loss: 0.0952 - accuracy: 0.9575 - val_loss: 0.1957 - val_accuracy: 0.9263\n",
      "Epoch 57/100\n",
      "212/212 [==============================] - 34s 161ms/step - loss: 0.0832 - accuracy: 0.9517 - val_loss: 0.1825 - val_accuracy: 0.9263\n",
      "Epoch 58/100\n",
      "212/212 [==============================] - 34s 162ms/step - loss: 0.0785 - accuracy: 0.9658 - val_loss: 0.2274 - val_accuracy: 0.9158\n",
      "Epoch 59/100\n",
      "212/212 [==============================] - 35s 162ms/step - loss: 0.0654 - accuracy: 0.9705 - val_loss: 0.1622 - val_accuracy: 0.9263\n",
      "Epoch 60/100\n",
      "212/212 [==============================] - 34s 161ms/step - loss: 0.0811 - accuracy: 0.9670 - val_loss: 0.1414 - val_accuracy: 0.9263\n",
      "Epoch 61/100\n",
      "212/212 [==============================] - 34s 161ms/step - loss: 0.0699 - accuracy: 0.9717 - val_loss: 0.1687 - val_accuracy: 0.9263\n",
      "Epoch 62/100\n",
      "212/212 [==============================] - 34s 161ms/step - loss: 0.0516 - accuracy: 0.9788 - val_loss: 0.1515 - val_accuracy: 0.9263\n",
      "Epoch 63/100\n",
      "212/212 [==============================] - 34s 161ms/step - loss: 0.0536 - accuracy: 0.9788 - val_loss: 0.1537 - val_accuracy: 0.9263\n",
      "Epoch 64/100\n",
      "212/212 [==============================] - 34s 162ms/step - loss: 0.0845 - accuracy: 0.9658 - val_loss: 0.1763 - val_accuracy: 0.9368\n",
      "Epoch 65/100\n",
      "212/212 [==============================] - 34s 161ms/step - loss: 0.0830 - accuracy: 0.9670 - val_loss: 0.1612 - val_accuracy: 0.9263\n",
      "Epoch 66/100\n",
      "212/212 [==============================] - 34s 162ms/step - loss: 0.0894 - accuracy: 0.9587 - val_loss: 0.1730 - val_accuracy: 0.9263\n",
      "Epoch 67/100\n",
      "212/212 [==============================] - 34s 161ms/step - loss: 0.0773 - accuracy: 0.9634 - val_loss: 0.2519 - val_accuracy: 0.8842\n",
      "Epoch 68/100\n",
      "212/212 [==============================] - 34s 161ms/step - loss: 0.0729 - accuracy: 0.9705 - val_loss: 0.2024 - val_accuracy: 0.9053\n",
      "Epoch 69/100\n",
      "212/212 [==============================] - 34s 161ms/step - loss: 0.0795 - accuracy: 0.9646 - val_loss: 0.1690 - val_accuracy: 0.9263\n",
      "Epoch 70/100\n",
      "212/212 [==============================] - 34s 160ms/step - loss: 0.0775 - accuracy: 0.9658 - val_loss: 0.1867 - val_accuracy: 0.9263\n",
      "Epoch 71/100\n",
      "212/212 [==============================] - 34s 161ms/step - loss: 0.0752 - accuracy: 0.9634 - val_loss: 0.1516 - val_accuracy: 0.9368\n",
      "Epoch 72/100\n",
      "212/212 [==============================] - 34s 161ms/step - loss: 0.0531 - accuracy: 0.9800 - val_loss: 0.2002 - val_accuracy: 0.9053\n",
      "Epoch 73/100\n",
      "212/212 [==============================] - 34s 161ms/step - loss: 0.0266 - accuracy: 0.9953 - val_loss: 0.1821 - val_accuracy: 0.9263\n",
      "Epoch 74/100\n",
      "212/212 [==============================] - 34s 160ms/step - loss: 0.0628 - accuracy: 0.9752 - val_loss: 0.1356 - val_accuracy: 0.9474\n",
      "Epoch 75/100\n",
      "212/212 [==============================] - 34s 160ms/step - loss: 0.0472 - accuracy: 0.9858 - val_loss: 0.1586 - val_accuracy: 0.9474\n",
      "Epoch 76/100\n",
      "212/212 [==============================] - 35s 164ms/step - loss: 0.0667 - accuracy: 0.9776 - val_loss: 0.1803 - val_accuracy: 0.9158\n",
      "Epoch 77/100\n",
      "212/212 [==============================] - 35s 162ms/step - loss: 0.0813 - accuracy: 0.9646 - val_loss: 0.1654 - val_accuracy: 0.9368\n",
      "Epoch 78/100\n",
      "212/212 [==============================] - 35s 163ms/step - loss: 0.0765 - accuracy: 0.9646 - val_loss: 0.1985 - val_accuracy: 0.9053\n",
      "Epoch 79/100\n",
      "212/212 [==============================] - 34s 162ms/step - loss: 0.0696 - accuracy: 0.9705 - val_loss: 0.1816 - val_accuracy: 0.9368\n",
      "Epoch 80/100\n",
      "212/212 [==============================] - 34s 161ms/step - loss: 0.0566 - accuracy: 0.9729 - val_loss: 0.2311 - val_accuracy: 0.8947\n",
      "Epoch 81/100\n",
      "212/212 [==============================] - 34s 161ms/step - loss: 0.0710 - accuracy: 0.9670 - val_loss: 0.2090 - val_accuracy: 0.9263\n",
      "Epoch 82/100\n",
      "212/212 [==============================] - 34s 161ms/step - loss: 0.0516 - accuracy: 0.9811 - val_loss: 0.1920 - val_accuracy: 0.9158\n",
      "Epoch 83/100\n",
      "212/212 [==============================] - 34s 162ms/step - loss: 0.0598 - accuracy: 0.9752 - val_loss: 0.1608 - val_accuracy: 0.9474\n",
      "Epoch 84/100\n",
      "212/212 [==============================] - 34s 161ms/step - loss: 0.0547 - accuracy: 0.9729 - val_loss: 0.1762 - val_accuracy: 0.9263\n",
      "Epoch 85/100\n",
      "212/212 [==============================] - 35s 164ms/step - loss: 0.0874 - accuracy: 0.9587 - val_loss: 0.1961 - val_accuracy: 0.9158\n",
      "Epoch 86/100\n",
      "212/212 [==============================] - 35s 164ms/step - loss: 0.0648 - accuracy: 0.9705 - val_loss: 0.1670 - val_accuracy: 0.9263\n",
      "Epoch 87/100\n",
      "212/212 [==============================] - 35s 166ms/step - loss: 0.0516 - accuracy: 0.9741 - val_loss: 0.1769 - val_accuracy: 0.9368\n",
      "Epoch 88/100\n",
      "212/212 [==============================] - 35s 165ms/step - loss: 0.0753 - accuracy: 0.9717 - val_loss: 0.1991 - val_accuracy: 0.9263\n",
      "Epoch 89/100\n",
      "212/212 [==============================] - 35s 164ms/step - loss: 0.0656 - accuracy: 0.9717 - val_loss: 0.1956 - val_accuracy: 0.9263\n",
      "Epoch 90/100\n",
      "212/212 [==============================] - 35s 162ms/step - loss: 0.0894 - accuracy: 0.9493 - val_loss: 0.1870 - val_accuracy: 0.9263\n",
      "Epoch 91/100\n",
      "212/212 [==============================] - 35s 163ms/step - loss: 0.0683 - accuracy: 0.9705 - val_loss: 0.1917 - val_accuracy: 0.9263\n",
      "Epoch 92/100\n",
      "212/212 [==============================] - 35s 163ms/step - loss: 0.0332 - accuracy: 0.9906 - val_loss: 0.1989 - val_accuracy: 0.9263\n",
      "Epoch 93/100\n",
      "212/212 [==============================] - 35s 164ms/step - loss: 0.0727 - accuracy: 0.9611 - val_loss: 0.1757 - val_accuracy: 0.9263\n",
      "Epoch 94/100\n",
      "212/212 [==============================] - 35s 166ms/step - loss: 0.0638 - accuracy: 0.9717 - val_loss: 0.1728 - val_accuracy: 0.9474\n",
      "Epoch 95/100\n",
      "212/212 [==============================] - 35s 162ms/step - loss: 0.0655 - accuracy: 0.9729 - val_loss: 0.1952 - val_accuracy: 0.9263\n",
      "Epoch 96/100\n",
      "212/212 [==============================] - 35s 163ms/step - loss: 0.0713 - accuracy: 0.9658 - val_loss: 0.3059 - val_accuracy: 0.8947\n",
      "Epoch 97/100\n",
      "212/212 [==============================] - 35s 162ms/step - loss: 0.0618 - accuracy: 0.9752 - val_loss: 0.2039 - val_accuracy: 0.8947\n",
      "Epoch 98/100\n",
      "212/212 [==============================] - 35s 165ms/step - loss: 0.0773 - accuracy: 0.9646 - val_loss: 0.1878 - val_accuracy: 0.9474\n",
      "Epoch 99/100\n",
      "212/212 [==============================] - 35s 162ms/step - loss: 0.0511 - accuracy: 0.9788 - val_loss: 0.1801 - val_accuracy: 0.9263\n",
      "Epoch 100/100\n",
      "212/212 [==============================] - 35s 163ms/step - loss: 0.0790 - accuracy: 0.9634 - val_loss: 0.1631 - val_accuracy: 0.9368\n",
      "Score for fold 2: loss of 0.163054957985878; accuracy of 93.68420839309692%\n",
      "(848,) (95,)\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 3 ...\n",
      "Epoch 1/100\n",
      "212/212 [==============================] - 41s 165ms/step - loss: 0.4789 - accuracy: 0.6486 - val_loss: 1.1639 - val_accuracy: 0.3368\n",
      "Epoch 2/100\n",
      "212/212 [==============================] - 34s 160ms/step - loss: 0.3727 - accuracy: 0.7559 - val_loss: 0.2863 - val_accuracy: 0.8947\n",
      "Epoch 3/100\n",
      "212/212 [==============================] - 34s 159ms/step - loss: 0.3384 - accuracy: 0.7960 - val_loss: 0.7988 - val_accuracy: 0.6316\n",
      "Epoch 4/100\n",
      "212/212 [==============================] - 34s 159ms/step - loss: 0.3230 - accuracy: 0.8007 - val_loss: 0.7581 - val_accuracy: 0.7368\n",
      "Epoch 5/100\n",
      "212/212 [==============================] - 34s 159ms/step - loss: 0.3222 - accuracy: 0.8172 - val_loss: 0.4518 - val_accuracy: 0.8842\n",
      "Epoch 6/100\n",
      "212/212 [==============================] - 34s 158ms/step - loss: 0.2949 - accuracy: 0.8125 - val_loss: 9.2850 - val_accuracy: 0.5474\n",
      "Epoch 7/100\n",
      "212/212 [==============================] - 34s 158ms/step - loss: 0.2676 - accuracy: 0.8467 - val_loss: 0.3919 - val_accuracy: 0.7895\n",
      "Epoch 8/100\n",
      "212/212 [==============================] - 34s 159ms/step - loss: 0.2818 - accuracy: 0.8314 - val_loss: 0.6726 - val_accuracy: 0.8000\n",
      "Epoch 9/100\n",
      "212/212 [==============================] - 34s 159ms/step - loss: 0.2713 - accuracy: 0.8432 - val_loss: 0.2894 - val_accuracy: 0.8947\n",
      "Epoch 10/100\n",
      "212/212 [==============================] - 34s 160ms/step - loss: 0.2640 - accuracy: 0.8290 - val_loss: 0.3905 - val_accuracy: 0.7895\n",
      "Epoch 11/100\n",
      "212/212 [==============================] - 34s 159ms/step - loss: 0.2488 - accuracy: 0.8550 - val_loss: 0.1063 - val_accuracy: 0.9368\n",
      "Epoch 12/100\n",
      "212/212 [==============================] - 34s 159ms/step - loss: 0.2534 - accuracy: 0.8491 - val_loss: 0.4954 - val_accuracy: 0.7263\n",
      "Epoch 13/100\n",
      "212/212 [==============================] - 34s 159ms/step - loss: 0.2471 - accuracy: 0.8573 - val_loss: 0.3449 - val_accuracy: 0.7684\n",
      "Epoch 14/100\n",
      "212/212 [==============================] - 34s 159ms/step - loss: 0.2341 - accuracy: 0.8538 - val_loss: 0.0992 - val_accuracy: 0.9579\n",
      "Epoch 15/100\n",
      "212/212 [==============================] - 34s 161ms/step - loss: 0.2085 - accuracy: 0.8644 - val_loss: 0.1023 - val_accuracy: 0.9474\n",
      "Epoch 16/100\n",
      "212/212 [==============================] - 34s 160ms/step - loss: 0.2157 - accuracy: 0.8797 - val_loss: 0.2533 - val_accuracy: 0.8632\n",
      "Epoch 17/100\n",
      "212/212 [==============================] - 34s 159ms/step - loss: 0.2122 - accuracy: 0.8715 - val_loss: 0.1898 - val_accuracy: 0.8737\n",
      "Epoch 18/100\n",
      "212/212 [==============================] - 34s 159ms/step - loss: 0.2385 - accuracy: 0.8679 - val_loss: 0.1222 - val_accuracy: 0.9263\n",
      "Epoch 19/100\n",
      "212/212 [==============================] - 34s 159ms/step - loss: 0.2123 - accuracy: 0.8691 - val_loss: 0.1286 - val_accuracy: 0.9474\n",
      "Epoch 20/100\n",
      "212/212 [==============================] - 34s 160ms/step - loss: 0.1768 - accuracy: 0.8927 - val_loss: 0.1235 - val_accuracy: 0.9368\n",
      "Epoch 21/100\n",
      "212/212 [==============================] - 34s 160ms/step - loss: 0.1818 - accuracy: 0.8856 - val_loss: 0.3043 - val_accuracy: 0.8105\n",
      "Epoch 22/100\n",
      "212/212 [==============================] - 34s 160ms/step - loss: 0.1976 - accuracy: 0.8738 - val_loss: 0.1880 - val_accuracy: 0.8737\n",
      "Epoch 23/100\n",
      "212/212 [==============================] - 34s 160ms/step - loss: 0.1951 - accuracy: 0.8797 - val_loss: 0.1441 - val_accuracy: 0.8947\n",
      "Epoch 24/100\n",
      "212/212 [==============================] - 34s 160ms/step - loss: 0.1738 - accuracy: 0.8785 - val_loss: 0.1271 - val_accuracy: 0.9053\n",
      "Epoch 25/100\n",
      "212/212 [==============================] - 34s 160ms/step - loss: 0.1600 - accuracy: 0.9021 - val_loss: 0.0979 - val_accuracy: 0.9474\n",
      "Epoch 26/100\n",
      "212/212 [==============================] - 34s 160ms/step - loss: 0.1928 - accuracy: 0.8738 - val_loss: 0.2461 - val_accuracy: 0.8421\n",
      "Epoch 27/100\n",
      "212/212 [==============================] - 34s 161ms/step - loss: 0.1653 - accuracy: 0.8892 - val_loss: 0.1337 - val_accuracy: 0.9158\n",
      "Epoch 28/100\n",
      "212/212 [==============================] - 34s 160ms/step - loss: 0.1541 - accuracy: 0.9104 - val_loss: 0.1158 - val_accuracy: 0.9474\n",
      "Epoch 29/100\n",
      "212/212 [==============================] - 34s 160ms/step - loss: 0.1446 - accuracy: 0.9116 - val_loss: 0.1743 - val_accuracy: 0.8842\n",
      "Epoch 30/100\n",
      "212/212 [==============================] - 34s 160ms/step - loss: 0.1555 - accuracy: 0.9104 - val_loss: 0.1774 - val_accuracy: 0.8737\n",
      "Epoch 31/100\n",
      "212/212 [==============================] - 34s 160ms/step - loss: 0.1715 - accuracy: 0.9057 - val_loss: 0.2327 - val_accuracy: 0.8211\n",
      "Epoch 32/100\n",
      "212/212 [==============================] - 34s 160ms/step - loss: 0.1474 - accuracy: 0.9104 - val_loss: 0.1187 - val_accuracy: 0.9158\n",
      "Epoch 33/100\n",
      "212/212 [==============================] - 34s 160ms/step - loss: 0.1328 - accuracy: 0.9269 - val_loss: 0.1136 - val_accuracy: 0.9474\n",
      "Epoch 34/100\n",
      "212/212 [==============================] - 34s 160ms/step - loss: 0.1527 - accuracy: 0.9092 - val_loss: 0.2093 - val_accuracy: 0.8421\n",
      "Epoch 35/100\n",
      "212/212 [==============================] - 34s 160ms/step - loss: 0.1382 - accuracy: 0.9257 - val_loss: 0.1911 - val_accuracy: 0.8421\n",
      "Epoch 36/100\n",
      "212/212 [==============================] - 34s 160ms/step - loss: 0.1293 - accuracy: 0.9175 - val_loss: 0.1774 - val_accuracy: 0.8737\n",
      "Epoch 37/100\n",
      "212/212 [==============================] - 34s 160ms/step - loss: 0.1244 - accuracy: 0.9316 - val_loss: 0.1847 - val_accuracy: 0.8632\n",
      "Epoch 38/100\n",
      "212/212 [==============================] - 34s 160ms/step - loss: 0.1120 - accuracy: 0.9387 - val_loss: 0.0907 - val_accuracy: 0.9579\n",
      "Epoch 39/100\n",
      "212/212 [==============================] - 34s 160ms/step - loss: 0.1459 - accuracy: 0.9163 - val_loss: 0.0881 - val_accuracy: 0.9684\n",
      "Epoch 40/100\n",
      "212/212 [==============================] - 34s 160ms/step - loss: 0.1242 - accuracy: 0.9304 - val_loss: 0.1045 - val_accuracy: 0.9684\n",
      "Epoch 41/100\n",
      "212/212 [==============================] - 34s 160ms/step - loss: 0.1162 - accuracy: 0.9375 - val_loss: 0.1020 - val_accuracy: 0.9474\n",
      "Epoch 42/100\n",
      "212/212 [==============================] - 34s 160ms/step - loss: 0.1240 - accuracy: 0.9340 - val_loss: 0.1148 - val_accuracy: 0.9158\n",
      "Epoch 43/100\n",
      "212/212 [==============================] - 34s 160ms/step - loss: 0.0941 - accuracy: 0.9599 - val_loss: 0.1274 - val_accuracy: 0.9263\n",
      "Epoch 44/100\n",
      "212/212 [==============================] - 34s 160ms/step - loss: 0.1399 - accuracy: 0.9304 - val_loss: 0.1132 - val_accuracy: 0.9263\n",
      "Epoch 45/100\n",
      "212/212 [==============================] - 34s 160ms/step - loss: 0.0896 - accuracy: 0.9552 - val_loss: 0.1025 - val_accuracy: 0.9474\n",
      "Epoch 46/100\n",
      "212/212 [==============================] - 34s 160ms/step - loss: 0.1028 - accuracy: 0.9517 - val_loss: 0.1482 - val_accuracy: 0.8947\n",
      "Epoch 47/100\n",
      "212/212 [==============================] - 34s 160ms/step - loss: 0.0851 - accuracy: 0.9587 - val_loss: 0.3185 - val_accuracy: 0.8211\n",
      "Epoch 48/100\n",
      "212/212 [==============================] - 34s 160ms/step - loss: 0.1190 - accuracy: 0.9328 - val_loss: 0.1398 - val_accuracy: 0.9158\n",
      "Epoch 49/100\n",
      "212/212 [==============================] - 34s 161ms/step - loss: 0.0986 - accuracy: 0.9575 - val_loss: 0.1560 - val_accuracy: 0.9158\n",
      "Epoch 50/100\n",
      "212/212 [==============================] - 34s 160ms/step - loss: 0.1163 - accuracy: 0.9410 - val_loss: 0.1002 - val_accuracy: 0.9474\n",
      "Epoch 51/100\n",
      "212/212 [==============================] - 34s 160ms/step - loss: 0.0845 - accuracy: 0.9564 - val_loss: 0.1003 - val_accuracy: 0.9158\n",
      "Epoch 52/100\n",
      "212/212 [==============================] - 34s 160ms/step - loss: 0.0904 - accuracy: 0.9646 - val_loss: 0.1482 - val_accuracy: 0.9368\n",
      "Epoch 53/100\n",
      "212/212 [==============================] - 34s 160ms/step - loss: 0.0568 - accuracy: 0.9729 - val_loss: 0.1380 - val_accuracy: 0.9053\n",
      "Epoch 54/100\n",
      "212/212 [==============================] - 34s 160ms/step - loss: 0.0799 - accuracy: 0.9611 - val_loss: 0.1314 - val_accuracy: 0.9158\n",
      "Epoch 55/100\n",
      "212/212 [==============================] - 34s 159ms/step - loss: 0.0629 - accuracy: 0.9741 - val_loss: 0.1796 - val_accuracy: 0.8842\n",
      "Epoch 56/100\n",
      "212/212 [==============================] - 34s 160ms/step - loss: 0.1222 - accuracy: 0.9410 - val_loss: 0.1804 - val_accuracy: 0.8632\n",
      "Epoch 57/100\n",
      "212/212 [==============================] - 34s 160ms/step - loss: 0.0878 - accuracy: 0.9587 - val_loss: 0.1619 - val_accuracy: 0.9053\n",
      "Epoch 58/100\n",
      "212/212 [==============================] - 34s 160ms/step - loss: 0.0975 - accuracy: 0.9528 - val_loss: 0.1290 - val_accuracy: 0.9053\n",
      "Epoch 59/100\n",
      "212/212 [==============================] - 34s 160ms/step - loss: 0.0624 - accuracy: 0.9752 - val_loss: 0.1572 - val_accuracy: 0.8947\n",
      "Epoch 60/100\n",
      "212/212 [==============================] - 34s 160ms/step - loss: 0.0867 - accuracy: 0.9564 - val_loss: 0.1349 - val_accuracy: 0.8947\n",
      "Epoch 61/100\n",
      "212/212 [==============================] - 34s 160ms/step - loss: 0.0554 - accuracy: 0.9776 - val_loss: 0.1330 - val_accuracy: 0.9158\n",
      "Epoch 62/100\n",
      "212/212 [==============================] - 34s 160ms/step - loss: 0.1070 - accuracy: 0.9540 - val_loss: 0.1113 - val_accuracy: 0.9263\n",
      "Epoch 63/100\n",
      "212/212 [==============================] - 34s 160ms/step - loss: 0.0815 - accuracy: 0.9646 - val_loss: 0.1188 - val_accuracy: 0.9263\n",
      "Epoch 64/100\n",
      "212/212 [==============================] - 34s 159ms/step - loss: 0.0777 - accuracy: 0.9717 - val_loss: 0.1444 - val_accuracy: 0.9158\n",
      "Epoch 65/100\n",
      "212/212 [==============================] - 34s 160ms/step - loss: 0.0469 - accuracy: 0.9882 - val_loss: 0.1214 - val_accuracy: 0.9368\n",
      "Epoch 66/100\n",
      "212/212 [==============================] - 34s 160ms/step - loss: 0.0694 - accuracy: 0.9658 - val_loss: 0.1318 - val_accuracy: 0.9263\n",
      "Epoch 67/100\n",
      "212/212 [==============================] - 34s 160ms/step - loss: 0.0820 - accuracy: 0.9693 - val_loss: 0.1473 - val_accuracy: 0.9053\n",
      "Epoch 68/100\n",
      "212/212 [==============================] - 34s 160ms/step - loss: 0.0668 - accuracy: 0.9693 - val_loss: 0.1313 - val_accuracy: 0.9368\n",
      "Epoch 69/100\n",
      "212/212 [==============================] - 34s 160ms/step - loss: 0.0940 - accuracy: 0.9505 - val_loss: 0.3028 - val_accuracy: 0.8316\n",
      "Epoch 70/100\n",
      "212/212 [==============================] - 34s 160ms/step - loss: 0.0729 - accuracy: 0.9634 - val_loss: 0.1224 - val_accuracy: 0.9053\n",
      "Epoch 71/100\n",
      "212/212 [==============================] - 34s 160ms/step - loss: 0.0670 - accuracy: 0.9682 - val_loss: 0.1394 - val_accuracy: 0.9158\n",
      "Epoch 72/100\n",
      "212/212 [==============================] - 34s 160ms/step - loss: 0.0440 - accuracy: 0.9882 - val_loss: 0.1271 - val_accuracy: 0.9474\n",
      "Epoch 73/100\n",
      "212/212 [==============================] - 34s 160ms/step - loss: 0.0811 - accuracy: 0.9646 - val_loss: 0.1369 - val_accuracy: 0.9053\n",
      "Epoch 74/100\n",
      "212/212 [==============================] - 34s 160ms/step - loss: 0.0923 - accuracy: 0.9575 - val_loss: 0.1271 - val_accuracy: 0.9158\n",
      "Epoch 75/100\n",
      "212/212 [==============================] - 34s 160ms/step - loss: 0.0551 - accuracy: 0.9776 - val_loss: 0.2059 - val_accuracy: 0.8737\n",
      "Epoch 76/100\n",
      "212/212 [==============================] - 34s 160ms/step - loss: 0.0579 - accuracy: 0.9729 - val_loss: 0.1378 - val_accuracy: 0.8947\n",
      "Epoch 77/100\n",
      "212/212 [==============================] - 34s 160ms/step - loss: 0.0993 - accuracy: 0.9481 - val_loss: 0.1382 - val_accuracy: 0.9263\n",
      "Epoch 78/100\n",
      "212/212 [==============================] - 34s 160ms/step - loss: 0.0743 - accuracy: 0.9670 - val_loss: 0.1244 - val_accuracy: 0.9368\n",
      "Epoch 79/100\n",
      "212/212 [==============================] - 34s 159ms/step - loss: 0.0662 - accuracy: 0.9741 - val_loss: 0.1202 - val_accuracy: 0.9158\n",
      "Epoch 80/100\n",
      "212/212 [==============================] - 34s 160ms/step - loss: 0.0689 - accuracy: 0.9693 - val_loss: 0.1253 - val_accuracy: 0.9053\n",
      "Epoch 81/100\n",
      "212/212 [==============================] - 34s 160ms/step - loss: 0.0633 - accuracy: 0.9741 - val_loss: 0.1229 - val_accuracy: 0.8947\n",
      "Epoch 82/100\n",
      "212/212 [==============================] - 34s 160ms/step - loss: 0.0572 - accuracy: 0.9776 - val_loss: 0.1934 - val_accuracy: 0.8737\n",
      "Epoch 83/100\n",
      "212/212 [==============================] - 34s 160ms/step - loss: 0.0777 - accuracy: 0.9670 - val_loss: 0.1528 - val_accuracy: 0.9263\n",
      "Epoch 84/100\n",
      "212/212 [==============================] - 34s 161ms/step - loss: 0.0591 - accuracy: 0.9741 - val_loss: 0.1266 - val_accuracy: 0.9053\n",
      "Epoch 85/100\n",
      "212/212 [==============================] - 34s 160ms/step - loss: 0.0584 - accuracy: 0.9705 - val_loss: 0.1232 - val_accuracy: 0.9053\n",
      "Epoch 86/100\n",
      "212/212 [==============================] - 34s 160ms/step - loss: 0.0489 - accuracy: 0.9811 - val_loss: 0.1234 - val_accuracy: 0.9263\n",
      "Epoch 87/100\n",
      "212/212 [==============================] - 34s 160ms/step - loss: 0.0741 - accuracy: 0.9658 - val_loss: 0.1418 - val_accuracy: 0.9263\n",
      "Epoch 88/100\n",
      "212/212 [==============================] - 34s 160ms/step - loss: 0.0917 - accuracy: 0.9587 - val_loss: 0.1155 - val_accuracy: 0.9474\n",
      "Epoch 89/100\n",
      "212/212 [==============================] - 34s 160ms/step - loss: 0.0646 - accuracy: 0.9705 - val_loss: 0.1340 - val_accuracy: 0.9368\n",
      "Epoch 90/100\n",
      "212/212 [==============================] - 34s 160ms/step - loss: 0.0536 - accuracy: 0.9752 - val_loss: 0.1258 - val_accuracy: 0.9158\n",
      "Epoch 91/100\n",
      "212/212 [==============================] - 34s 160ms/step - loss: 0.0840 - accuracy: 0.9564 - val_loss: 0.1783 - val_accuracy: 0.8842\n",
      "Epoch 92/100\n",
      "212/212 [==============================] - 34s 160ms/step - loss: 0.0712 - accuracy: 0.9658 - val_loss: 0.1190 - val_accuracy: 0.9474\n",
      "Epoch 93/100\n",
      "212/212 [==============================] - 34s 160ms/step - loss: 0.0738 - accuracy: 0.9634 - val_loss: 0.1372 - val_accuracy: 0.8947\n",
      "Epoch 94/100\n",
      "212/212 [==============================] - 34s 160ms/step - loss: 0.0472 - accuracy: 0.9870 - val_loss: 0.1304 - val_accuracy: 0.9263\n",
      "Epoch 95/100\n",
      "212/212 [==============================] - 34s 160ms/step - loss: 0.0626 - accuracy: 0.9741 - val_loss: 0.1296 - val_accuracy: 0.9158\n",
      "Epoch 96/100\n",
      "212/212 [==============================] - 34s 160ms/step - loss: 0.0646 - accuracy: 0.9717 - val_loss: 0.1238 - val_accuracy: 0.9158\n",
      "Epoch 97/100\n",
      "212/212 [==============================] - 34s 160ms/step - loss: 0.0807 - accuracy: 0.9670 - val_loss: 0.1358 - val_accuracy: 0.9263\n",
      "Epoch 98/100\n",
      "212/212 [==============================] - 34s 160ms/step - loss: 0.0562 - accuracy: 0.9800 - val_loss: 0.1241 - val_accuracy: 0.9474\n",
      "Epoch 99/100\n",
      "212/212 [==============================] - 34s 160ms/step - loss: 0.0752 - accuracy: 0.9634 - val_loss: 0.1213 - val_accuracy: 0.9474\n",
      "Epoch 100/100\n",
      "212/212 [==============================] - 34s 160ms/step - loss: 0.0582 - accuracy: 0.9741 - val_loss: 0.1239 - val_accuracy: 0.9368\n",
      "Score for fold 3: loss of 0.12388116121292114; accuracy of 93.68420839309692%\n",
      "(849,) (94,)\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 4 ...\n",
      "Epoch 1/100\n",
      "213/213 [==============================] - 47s 187ms/step - loss: 0.4906 - accuracy: 0.6655 - val_loss: 1.2963 - val_accuracy: 0.3298\n",
      "Epoch 2/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.4065 - accuracy: 0.7385 - val_loss: 0.2224 - val_accuracy: 0.8830\n",
      "Epoch 3/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.3881 - accuracy: 0.7715 - val_loss: 0.2561 - val_accuracy: 0.7979\n",
      "Epoch 4/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.3374 - accuracy: 0.7892 - val_loss: 0.1985 - val_accuracy: 0.9149\n",
      "Epoch 5/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.3202 - accuracy: 0.8021 - val_loss: 0.2071 - val_accuracy: 0.8723\n",
      "Epoch 6/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.2965 - accuracy: 0.8174 - val_loss: 0.2250 - val_accuracy: 0.8617\n",
      "Epoch 7/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.2505 - accuracy: 0.8622 - val_loss: 0.1451 - val_accuracy: 0.9149\n",
      "Epoch 8/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.2942 - accuracy: 0.8186 - val_loss: 0.4152 - val_accuracy: 0.7660\n",
      "Epoch 9/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.2596 - accuracy: 0.8445 - val_loss: 0.2282 - val_accuracy: 0.8085\n",
      "Epoch 10/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.2504 - accuracy: 0.8610 - val_loss: 0.1812 - val_accuracy: 0.8511\n",
      "Epoch 11/100\n",
      "213/213 [==============================] - 37s 174ms/step - loss: 0.2618 - accuracy: 0.8492 - val_loss: 0.1222 - val_accuracy: 0.9149\n",
      "Epoch 12/100\n",
      "213/213 [==============================] - 37s 174ms/step - loss: 0.2622 - accuracy: 0.8445 - val_loss: 0.1090 - val_accuracy: 0.9468\n",
      "Epoch 13/100\n",
      "213/213 [==============================] - 37s 174ms/step - loss: 0.2587 - accuracy: 0.8316 - val_loss: 0.1001 - val_accuracy: 0.9681\n",
      "Epoch 14/100\n",
      "213/213 [==============================] - 37s 174ms/step - loss: 0.2413 - accuracy: 0.8539 - val_loss: 0.0809 - val_accuracy: 0.9681\n",
      "Epoch 15/100\n",
      "213/213 [==============================] - 37s 174ms/step - loss: 0.2485 - accuracy: 0.8339 - val_loss: 0.1994 - val_accuracy: 0.8511\n",
      "Epoch 16/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.1940 - accuracy: 0.8916 - val_loss: 0.1026 - val_accuracy: 0.9681\n",
      "Epoch 17/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.2290 - accuracy: 0.8704 - val_loss: 0.2484 - val_accuracy: 0.8085\n",
      "Epoch 18/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.2270 - accuracy: 0.8598 - val_loss: 0.1172 - val_accuracy: 0.9043\n",
      "Epoch 19/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.2208 - accuracy: 0.8575 - val_loss: 0.1005 - val_accuracy: 0.9362\n",
      "Epoch 20/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.2443 - accuracy: 0.8433 - val_loss: 0.5124 - val_accuracy: 0.7234\n",
      "Epoch 21/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.1801 - accuracy: 0.8975 - val_loss: 0.0986 - val_accuracy: 0.9362\n",
      "Epoch 22/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.1631 - accuracy: 0.8987 - val_loss: 0.2542 - val_accuracy: 0.8617\n",
      "Epoch 23/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.2242 - accuracy: 0.8587 - val_loss: 0.1172 - val_accuracy: 0.9362\n",
      "Epoch 24/100\n",
      "213/213 [==============================] - 37s 174ms/step - loss: 0.1899 - accuracy: 0.8846 - val_loss: 0.1907 - val_accuracy: 0.8617\n",
      "Epoch 25/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.1984 - accuracy: 0.8787 - val_loss: 0.2768 - val_accuracy: 0.7766\n",
      "Epoch 26/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.1494 - accuracy: 0.9128 - val_loss: 0.1268 - val_accuracy: 0.9362\n",
      "Epoch 27/100\n",
      "213/213 [==============================] - 37s 174ms/step - loss: 0.1537 - accuracy: 0.9128 - val_loss: 0.1222 - val_accuracy: 0.9255\n",
      "Epoch 28/100\n",
      "213/213 [==============================] - 37s 174ms/step - loss: 0.1618 - accuracy: 0.9081 - val_loss: 0.0922 - val_accuracy: 0.9574\n",
      "Epoch 29/100\n",
      "213/213 [==============================] - 37s 174ms/step - loss: 0.1484 - accuracy: 0.9176 - val_loss: 0.1034 - val_accuracy: 0.9574\n",
      "Epoch 30/100\n",
      "213/213 [==============================] - 37s 174ms/step - loss: 0.1451 - accuracy: 0.9211 - val_loss: 0.2880 - val_accuracy: 0.7979\n",
      "Epoch 31/100\n",
      "213/213 [==============================] - 37s 174ms/step - loss: 0.1560 - accuracy: 0.9069 - val_loss: 0.1248 - val_accuracy: 0.9149\n",
      "Epoch 32/100\n",
      "213/213 [==============================] - 37s 174ms/step - loss: 0.1157 - accuracy: 0.9482 - val_loss: 0.0814 - val_accuracy: 0.9574\n",
      "Epoch 33/100\n",
      "213/213 [==============================] - 37s 174ms/step - loss: 0.1364 - accuracy: 0.9270 - val_loss: 0.4701 - val_accuracy: 0.7660\n",
      "Epoch 34/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.1443 - accuracy: 0.9176 - val_loss: 0.2264 - val_accuracy: 0.8511\n",
      "Epoch 35/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.1332 - accuracy: 0.9364 - val_loss: 0.2010 - val_accuracy: 0.8511\n",
      "Epoch 36/100\n",
      "213/213 [==============================] - 37s 174ms/step - loss: 0.1267 - accuracy: 0.9305 - val_loss: 0.0918 - val_accuracy: 0.9362\n",
      "Epoch 37/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.1051 - accuracy: 0.9470 - val_loss: 0.0896 - val_accuracy: 0.9468\n",
      "Epoch 38/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.1088 - accuracy: 0.9435 - val_loss: 0.0907 - val_accuracy: 0.9574\n",
      "Epoch 39/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.1050 - accuracy: 0.9458 - val_loss: 0.1077 - val_accuracy: 0.9255\n",
      "Epoch 40/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.1150 - accuracy: 0.9482 - val_loss: 0.2700 - val_accuracy: 0.7766\n",
      "Epoch 41/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.1156 - accuracy: 0.9411 - val_loss: 0.1065 - val_accuracy: 0.9574\n",
      "Epoch 42/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0565 - accuracy: 0.9776 - val_loss: 0.0801 - val_accuracy: 0.9574\n",
      "Epoch 43/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.1001 - accuracy: 0.9505 - val_loss: 0.0983 - val_accuracy: 0.9468\n",
      "Epoch 44/100\n",
      "213/213 [==============================] - 37s 174ms/step - loss: 0.0920 - accuracy: 0.9588 - val_loss: 0.1373 - val_accuracy: 0.8830\n",
      "Epoch 45/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0720 - accuracy: 0.9682 - val_loss: 0.0919 - val_accuracy: 0.9574\n",
      "Epoch 46/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0755 - accuracy: 0.9682 - val_loss: 0.1459 - val_accuracy: 0.9255\n",
      "Epoch 47/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0772 - accuracy: 0.9658 - val_loss: 0.1261 - val_accuracy: 0.9149\n",
      "Epoch 48/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.1150 - accuracy: 0.9435 - val_loss: 0.0812 - val_accuracy: 0.9574\n",
      "Epoch 49/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.1088 - accuracy: 0.9458 - val_loss: 0.1529 - val_accuracy: 0.8830\n",
      "Epoch 50/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0737 - accuracy: 0.9741 - val_loss: 0.0876 - val_accuracy: 0.9574\n",
      "Epoch 51/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0716 - accuracy: 0.9776 - val_loss: 0.1643 - val_accuracy: 0.8830\n",
      "Epoch 52/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0818 - accuracy: 0.9623 - val_loss: 0.1569 - val_accuracy: 0.9043\n",
      "Epoch 53/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0924 - accuracy: 0.9564 - val_loss: 0.1034 - val_accuracy: 0.9362\n",
      "Epoch 54/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0651 - accuracy: 0.9764 - val_loss: 0.1402 - val_accuracy: 0.9255\n",
      "Epoch 55/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0619 - accuracy: 0.9706 - val_loss: 0.1017 - val_accuracy: 0.9255\n",
      "Epoch 56/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0761 - accuracy: 0.9658 - val_loss: 0.0896 - val_accuracy: 0.9362\n",
      "Epoch 57/100\n",
      "213/213 [==============================] - 37s 174ms/step - loss: 0.0647 - accuracy: 0.9694 - val_loss: 0.1053 - val_accuracy: 0.9468\n",
      "Epoch 58/100\n",
      "213/213 [==============================] - 37s 174ms/step - loss: 0.0829 - accuracy: 0.9670 - val_loss: 0.1122 - val_accuracy: 0.9255\n",
      "Epoch 59/100\n",
      "213/213 [==============================] - 37s 174ms/step - loss: 0.0960 - accuracy: 0.9505 - val_loss: 0.0806 - val_accuracy: 0.9468\n",
      "Epoch 60/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0905 - accuracy: 0.9517 - val_loss: 0.1046 - val_accuracy: 0.9149\n",
      "Epoch 61/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0779 - accuracy: 0.9647 - val_loss: 0.0790 - val_accuracy: 0.9574\n",
      "Epoch 62/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0675 - accuracy: 0.9741 - val_loss: 0.0861 - val_accuracy: 0.9681\n",
      "Epoch 63/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0623 - accuracy: 0.9706 - val_loss: 0.0777 - val_accuracy: 0.9468\n",
      "Epoch 64/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0638 - accuracy: 0.9682 - val_loss: 0.0753 - val_accuracy: 0.9574\n",
      "Epoch 65/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0585 - accuracy: 0.9764 - val_loss: 0.1491 - val_accuracy: 0.8830\n",
      "Epoch 66/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0655 - accuracy: 0.9717 - val_loss: 0.0935 - val_accuracy: 0.9574\n",
      "Epoch 67/100\n",
      "213/213 [==============================] - 37s 174ms/step - loss: 0.0804 - accuracy: 0.9658 - val_loss: 0.1172 - val_accuracy: 0.8936\n",
      "Epoch 68/100\n",
      "213/213 [==============================] - 37s 175ms/step - loss: 0.0696 - accuracy: 0.9753 - val_loss: 0.0872 - val_accuracy: 0.9574\n",
      "Epoch 69/100\n",
      "213/213 [==============================] - 38s 177ms/step - loss: 0.0920 - accuracy: 0.9600 - val_loss: 0.1284 - val_accuracy: 0.9149\n",
      "Epoch 70/100\n",
      "213/213 [==============================] - 38s 176ms/step - loss: 0.0765 - accuracy: 0.9635 - val_loss: 0.0991 - val_accuracy: 0.9255\n",
      "Epoch 71/100\n",
      "213/213 [==============================] - 38s 177ms/step - loss: 0.0959 - accuracy: 0.9658 - val_loss: 0.1151 - val_accuracy: 0.9362\n",
      "Epoch 72/100\n",
      "213/213 [==============================] - 38s 177ms/step - loss: 0.0849 - accuracy: 0.9576 - val_loss: 0.0809 - val_accuracy: 0.9362\n",
      "Epoch 73/100\n",
      "213/213 [==============================] - 37s 175ms/step - loss: 0.0757 - accuracy: 0.9635 - val_loss: 0.1148 - val_accuracy: 0.9149\n",
      "Epoch 74/100\n",
      "213/213 [==============================] - 37s 175ms/step - loss: 0.0900 - accuracy: 0.9611 - val_loss: 0.1038 - val_accuracy: 0.9468\n",
      "Epoch 75/100\n",
      "213/213 [==============================] - 37s 174ms/step - loss: 0.0502 - accuracy: 0.9835 - val_loss: 0.0846 - val_accuracy: 0.9574\n",
      "Epoch 76/100\n",
      "213/213 [==============================] - 37s 174ms/step - loss: 0.0445 - accuracy: 0.9847 - val_loss: 0.0875 - val_accuracy: 0.9468\n",
      "Epoch 77/100\n",
      "213/213 [==============================] - 37s 175ms/step - loss: 0.0572 - accuracy: 0.9741 - val_loss: 0.1066 - val_accuracy: 0.9468\n",
      "Epoch 78/100\n",
      "213/213 [==============================] - 37s 174ms/step - loss: 0.0542 - accuracy: 0.9847 - val_loss: 0.1040 - val_accuracy: 0.9149\n",
      "Epoch 79/100\n",
      "213/213 [==============================] - 37s 175ms/step - loss: 0.0612 - accuracy: 0.9729 - val_loss: 0.0623 - val_accuracy: 0.9681\n",
      "Epoch 80/100\n",
      "213/213 [==============================] - 38s 175ms/step - loss: 0.0767 - accuracy: 0.9658 - val_loss: 0.1248 - val_accuracy: 0.9043\n",
      "Epoch 81/100\n",
      "213/213 [==============================] - 38s 175ms/step - loss: 0.0461 - accuracy: 0.9882 - val_loss: 0.0986 - val_accuracy: 0.9362\n",
      "Epoch 82/100\n",
      "213/213 [==============================] - 37s 175ms/step - loss: 0.0426 - accuracy: 0.9859 - val_loss: 0.1005 - val_accuracy: 0.9043\n",
      "Epoch 83/100\n",
      "213/213 [==============================] - 37s 175ms/step - loss: 0.0481 - accuracy: 0.9776 - val_loss: 0.1105 - val_accuracy: 0.9255\n",
      "Epoch 84/100\n",
      "213/213 [==============================] - 37s 175ms/step - loss: 0.0703 - accuracy: 0.9694 - val_loss: 0.0945 - val_accuracy: 0.9574\n",
      "Epoch 85/100\n",
      "213/213 [==============================] - 38s 176ms/step - loss: 0.0756 - accuracy: 0.9670 - val_loss: 0.0984 - val_accuracy: 0.9362\n",
      "Epoch 86/100\n",
      "213/213 [==============================] - 38s 176ms/step - loss: 0.0594 - accuracy: 0.9764 - val_loss: 0.0984 - val_accuracy: 0.9255\n",
      "Epoch 87/100\n",
      "213/213 [==============================] - 38s 175ms/step - loss: 0.0694 - accuracy: 0.9670 - val_loss: 0.1297 - val_accuracy: 0.9255\n",
      "Epoch 88/100\n",
      "213/213 [==============================] - 37s 175ms/step - loss: 0.0329 - accuracy: 0.9894 - val_loss: 0.0940 - val_accuracy: 0.9362\n",
      "Epoch 89/100\n",
      "213/213 [==============================] - 37s 175ms/step - loss: 0.0811 - accuracy: 0.9623 - val_loss: 0.1413 - val_accuracy: 0.9255\n",
      "Epoch 90/100\n",
      "213/213 [==============================] - 37s 175ms/step - loss: 0.0431 - accuracy: 0.9835 - val_loss: 0.1046 - val_accuracy: 0.9255\n",
      "Epoch 91/100\n",
      "213/213 [==============================] - 37s 175ms/step - loss: 0.0541 - accuracy: 0.9753 - val_loss: 0.0804 - val_accuracy: 0.9468\n",
      "Epoch 92/100\n",
      "213/213 [==============================] - 37s 175ms/step - loss: 0.0876 - accuracy: 0.9505 - val_loss: 0.1230 - val_accuracy: 0.9255\n",
      "Epoch 93/100\n",
      "213/213 [==============================] - 38s 176ms/step - loss: 0.0708 - accuracy: 0.9717 - val_loss: 0.1097 - val_accuracy: 0.9362\n",
      "Epoch 94/100\n",
      "213/213 [==============================] - 37s 175ms/step - loss: 0.0612 - accuracy: 0.9706 - val_loss: 0.1411 - val_accuracy: 0.9043\n",
      "Epoch 95/100\n",
      "213/213 [==============================] - 37s 175ms/step - loss: 0.0698 - accuracy: 0.9670 - val_loss: 0.1112 - val_accuracy: 0.9255\n",
      "Epoch 96/100\n",
      "213/213 [==============================] - 37s 175ms/step - loss: 0.0474 - accuracy: 0.9823 - val_loss: 0.1024 - val_accuracy: 0.9255\n",
      "Epoch 97/100\n",
      "213/213 [==============================] - 37s 175ms/step - loss: 0.0426 - accuracy: 0.9823 - val_loss: 0.0967 - val_accuracy: 0.9468\n",
      "Epoch 98/100\n",
      "213/213 [==============================] - 38s 175ms/step - loss: 0.0710 - accuracy: 0.9717 - val_loss: 0.1006 - val_accuracy: 0.9468\n",
      "Epoch 99/100\n",
      "213/213 [==============================] - 38s 176ms/step - loss: 0.0674 - accuracy: 0.9670 - val_loss: 0.1017 - val_accuracy: 0.9468\n",
      "Epoch 100/100\n",
      "213/213 [==============================] - 38s 176ms/step - loss: 0.0512 - accuracy: 0.9788 - val_loss: 0.1073 - val_accuracy: 0.9468\n",
      "Score for fold 4: loss of 0.10732637345790863; accuracy of 94.68085169792175%\n",
      "(849,) (94,)\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 5 ...\n",
      "Epoch 1/100\n",
      "213/213 [==============================] - 46s 181ms/step - loss: 0.4939 - accuracy: 0.6466 - val_loss: 1.1806 - val_accuracy: 0.3298\n",
      "Epoch 2/100\n",
      "213/213 [==============================] - 37s 175ms/step - loss: 0.4450 - accuracy: 0.6926 - val_loss: 0.5158 - val_accuracy: 0.6277\n",
      "Epoch 3/100\n",
      "213/213 [==============================] - 37s 175ms/step - loss: 0.3430 - accuracy: 0.7845 - val_loss: 1.2686 - val_accuracy: 0.7128\n",
      "Epoch 4/100\n",
      "213/213 [==============================] - 37s 174ms/step - loss: 0.3382 - accuracy: 0.7962 - val_loss: 1.2817 - val_accuracy: 0.7766\n",
      "Epoch 5/100\n",
      "213/213 [==============================] - 37s 174ms/step - loss: 0.3251 - accuracy: 0.7939 - val_loss: 2.0365 - val_accuracy: 0.6702\n",
      "Epoch 6/100\n",
      "213/213 [==============================] - 37s 174ms/step - loss: 0.3186 - accuracy: 0.8021 - val_loss: 0.1957 - val_accuracy: 0.8511\n",
      "Epoch 7/100\n",
      "213/213 [==============================] - 37s 175ms/step - loss: 0.2840 - accuracy: 0.8233 - val_loss: 0.3623 - val_accuracy: 0.7340\n",
      "Epoch 8/100\n",
      "213/213 [==============================] - 37s 175ms/step - loss: 0.2611 - accuracy: 0.8445 - val_loss: 0.3218 - val_accuracy: 0.7979\n",
      "Epoch 9/100\n",
      "213/213 [==============================] - 37s 175ms/step - loss: 0.2651 - accuracy: 0.8304 - val_loss: 0.1574 - val_accuracy: 0.8936\n",
      "Epoch 10/100\n",
      "213/213 [==============================] - 38s 176ms/step - loss: 0.2637 - accuracy: 0.8481 - val_loss: 0.2445 - val_accuracy: 0.8617\n",
      "Epoch 11/100\n",
      "213/213 [==============================] - 37s 175ms/step - loss: 0.2382 - accuracy: 0.8622 - val_loss: 0.1859 - val_accuracy: 0.8511\n",
      "Epoch 12/100\n",
      "213/213 [==============================] - 37s 175ms/step - loss: 0.2379 - accuracy: 0.8587 - val_loss: 0.8430 - val_accuracy: 0.6702\n",
      "Epoch 13/100\n",
      "213/213 [==============================] - 38s 176ms/step - loss: 0.2727 - accuracy: 0.8304 - val_loss: 0.5595 - val_accuracy: 0.7021\n",
      "Epoch 14/100\n",
      "213/213 [==============================] - 37s 175ms/step - loss: 0.2437 - accuracy: 0.8563 - val_loss: 0.2024 - val_accuracy: 0.8936\n",
      "Epoch 15/100\n",
      "213/213 [==============================] - 37s 174ms/step - loss: 0.2322 - accuracy: 0.8528 - val_loss: 0.4551 - val_accuracy: 0.7979\n",
      "Epoch 16/100\n",
      "213/213 [==============================] - 37s 175ms/step - loss: 0.2300 - accuracy: 0.8575 - val_loss: 0.5024 - val_accuracy: 0.6383\n",
      "Epoch 17/100\n",
      "213/213 [==============================] - 37s 174ms/step - loss: 0.1983 - accuracy: 0.8740 - val_loss: 0.2855 - val_accuracy: 0.8404\n",
      "Epoch 18/100\n",
      "213/213 [==============================] - 37s 174ms/step - loss: 0.2048 - accuracy: 0.8763 - val_loss: 0.2102 - val_accuracy: 0.8830\n",
      "Epoch 19/100\n",
      "213/213 [==============================] - 37s 175ms/step - loss: 0.2075 - accuracy: 0.8763 - val_loss: 0.2776 - val_accuracy: 0.7979\n",
      "Epoch 20/100\n",
      "213/213 [==============================] - 37s 175ms/step - loss: 0.2128 - accuracy: 0.8704 - val_loss: 0.1682 - val_accuracy: 0.8936\n",
      "Epoch 21/100\n",
      "213/213 [==============================] - 37s 174ms/step - loss: 0.2130 - accuracy: 0.8728 - val_loss: 0.2501 - val_accuracy: 0.8511\n",
      "Epoch 22/100\n",
      "213/213 [==============================] - 37s 174ms/step - loss: 0.2024 - accuracy: 0.8645 - val_loss: 1.0056 - val_accuracy: 0.7021\n",
      "Epoch 23/100\n",
      "213/213 [==============================] - 37s 175ms/step - loss: 0.1897 - accuracy: 0.8810 - val_loss: 0.4557 - val_accuracy: 0.7979\n",
      "Epoch 24/100\n",
      "213/213 [==============================] - 37s 174ms/step - loss: 0.1709 - accuracy: 0.8787 - val_loss: 0.1853 - val_accuracy: 0.9149\n",
      "Epoch 25/100\n",
      "213/213 [==============================] - 37s 175ms/step - loss: 0.1637 - accuracy: 0.8987 - val_loss: 0.2832 - val_accuracy: 0.8723\n",
      "Epoch 26/100\n",
      "213/213 [==============================] - 37s 175ms/step - loss: 0.1635 - accuracy: 0.9011 - val_loss: 0.2558 - val_accuracy: 0.8723\n",
      "Epoch 27/100\n",
      "213/213 [==============================] - 38s 177ms/step - loss: 0.1611 - accuracy: 0.9046 - val_loss: 0.2170 - val_accuracy: 0.8404\n",
      "Epoch 28/100\n",
      "213/213 [==============================] - 38s 177ms/step - loss: 0.1578 - accuracy: 0.9128 - val_loss: 0.5018 - val_accuracy: 0.8298\n",
      "Epoch 29/100\n",
      "213/213 [==============================] - 38s 176ms/step - loss: 0.1566 - accuracy: 0.9058 - val_loss: 0.5250 - val_accuracy: 0.7340\n",
      "Epoch 30/100\n",
      "213/213 [==============================] - 38s 176ms/step - loss: 0.1319 - accuracy: 0.9246 - val_loss: 0.4211 - val_accuracy: 0.7872\n",
      "Epoch 31/100\n",
      "213/213 [==============================] - 37s 175ms/step - loss: 0.1637 - accuracy: 0.8916 - val_loss: 0.2287 - val_accuracy: 0.9043\n",
      "Epoch 32/100\n",
      "213/213 [==============================] - 38s 176ms/step - loss: 0.1516 - accuracy: 0.9034 - val_loss: 0.4380 - val_accuracy: 0.7234\n",
      "Epoch 33/100\n",
      "213/213 [==============================] - 37s 175ms/step - loss: 0.1155 - accuracy: 0.9329 - val_loss: 0.2154 - val_accuracy: 0.8830\n",
      "Epoch 34/100\n",
      "213/213 [==============================] - 38s 177ms/step - loss: 0.1387 - accuracy: 0.9187 - val_loss: 0.2436 - val_accuracy: 0.8830\n",
      "Epoch 35/100\n",
      "213/213 [==============================] - 37s 175ms/step - loss: 0.1392 - accuracy: 0.9164 - val_loss: 0.2599 - val_accuracy: 0.8404\n",
      "Epoch 36/100\n",
      "213/213 [==============================] - 38s 178ms/step - loss: 0.1296 - accuracy: 0.9293 - val_loss: 0.1884 - val_accuracy: 0.8617\n",
      "Epoch 37/100\n",
      "213/213 [==============================] - 38s 176ms/step - loss: 0.1419 - accuracy: 0.9128 - val_loss: 0.3698 - val_accuracy: 0.7979\n",
      "Epoch 38/100\n",
      "213/213 [==============================] - 37s 174ms/step - loss: 0.1322 - accuracy: 0.9329 - val_loss: 0.1952 - val_accuracy: 0.8830\n",
      "Epoch 39/100\n",
      "213/213 [==============================] - 37s 174ms/step - loss: 0.1055 - accuracy: 0.9446 - val_loss: 0.2436 - val_accuracy: 0.8617\n",
      "Epoch 40/100\n",
      "213/213 [==============================] - 37s 174ms/step - loss: 0.0948 - accuracy: 0.9623 - val_loss: 0.2024 - val_accuracy: 0.9149\n",
      "Epoch 41/100\n",
      "213/213 [==============================] - 37s 174ms/step - loss: 0.1100 - accuracy: 0.9517 - val_loss: 0.2133 - val_accuracy: 0.9043\n",
      "Epoch 42/100\n",
      "213/213 [==============================] - 37s 174ms/step - loss: 0.1194 - accuracy: 0.9329 - val_loss: 0.2497 - val_accuracy: 0.8617\n",
      "Epoch 43/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.1106 - accuracy: 0.9505 - val_loss: 0.1791 - val_accuracy: 0.8936\n",
      "Epoch 44/100\n",
      "213/213 [==============================] - 37s 174ms/step - loss: 0.1352 - accuracy: 0.9293 - val_loss: 0.2343 - val_accuracy: 0.8617\n",
      "Epoch 45/100\n",
      "213/213 [==============================] - 37s 174ms/step - loss: 0.1232 - accuracy: 0.9329 - val_loss: 0.1604 - val_accuracy: 0.9043\n",
      "Epoch 46/100\n",
      "213/213 [==============================] - 37s 174ms/step - loss: 0.0832 - accuracy: 0.9635 - val_loss: 0.2167 - val_accuracy: 0.8936\n",
      "Epoch 47/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0677 - accuracy: 0.9753 - val_loss: 0.1261 - val_accuracy: 0.9149\n",
      "Epoch 48/100\n",
      "213/213 [==============================] - 37s 175ms/step - loss: 0.1104 - accuracy: 0.9458 - val_loss: 0.1500 - val_accuracy: 0.8936\n",
      "Epoch 49/100\n",
      "213/213 [==============================] - 37s 175ms/step - loss: 0.0758 - accuracy: 0.9647 - val_loss: 0.1388 - val_accuracy: 0.9149\n",
      "Epoch 50/100\n",
      "213/213 [==============================] - 37s 174ms/step - loss: 0.0832 - accuracy: 0.9635 - val_loss: 0.1667 - val_accuracy: 0.9149\n",
      "Epoch 51/100\n",
      "213/213 [==============================] - 37s 174ms/step - loss: 0.0661 - accuracy: 0.9764 - val_loss: 0.2295 - val_accuracy: 0.8936\n",
      "Epoch 52/100\n",
      "213/213 [==============================] - 37s 174ms/step - loss: 0.0963 - accuracy: 0.9529 - val_loss: 0.2210 - val_accuracy: 0.8936\n",
      "Epoch 53/100\n",
      "213/213 [==============================] - 37s 174ms/step - loss: 0.0814 - accuracy: 0.9576 - val_loss: 0.1556 - val_accuracy: 0.9255\n",
      "Epoch 54/100\n",
      "213/213 [==============================] - 37s 174ms/step - loss: 0.0798 - accuracy: 0.9682 - val_loss: 0.1178 - val_accuracy: 0.9255\n",
      "Epoch 55/100\n",
      "213/213 [==============================] - 37s 175ms/step - loss: 0.0879 - accuracy: 0.9635 - val_loss: 0.1362 - val_accuracy: 0.9043\n",
      "Epoch 56/100\n",
      "213/213 [==============================] - 37s 174ms/step - loss: 0.1335 - accuracy: 0.9270 - val_loss: 0.1276 - val_accuracy: 0.9362\n",
      "Epoch 57/100\n",
      "213/213 [==============================] - 37s 174ms/step - loss: 0.0853 - accuracy: 0.9623 - val_loss: 0.2369 - val_accuracy: 0.8723\n",
      "Epoch 58/100\n",
      "213/213 [==============================] - 37s 174ms/step - loss: 0.0722 - accuracy: 0.9706 - val_loss: 0.1533 - val_accuracy: 0.8936\n",
      "Epoch 59/100\n",
      "213/213 [==============================] - 37s 174ms/step - loss: 0.0842 - accuracy: 0.9635 - val_loss: 0.1721 - val_accuracy: 0.9149\n",
      "Epoch 60/100\n",
      "213/213 [==============================] - 37s 175ms/step - loss: 0.0972 - accuracy: 0.9494 - val_loss: 0.1319 - val_accuracy: 0.9362\n",
      "Epoch 61/100\n",
      "213/213 [==============================] - 37s 174ms/step - loss: 0.0928 - accuracy: 0.9552 - val_loss: 0.2196 - val_accuracy: 0.8830\n",
      "Epoch 62/100\n",
      "213/213 [==============================] - 37s 174ms/step - loss: 0.0634 - accuracy: 0.9741 - val_loss: 0.2246 - val_accuracy: 0.8830\n",
      "Epoch 63/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0742 - accuracy: 0.9670 - val_loss: 0.1758 - val_accuracy: 0.9043\n",
      "Epoch 64/100\n",
      "213/213 [==============================] - 37s 174ms/step - loss: 0.0478 - accuracy: 0.9812 - val_loss: 0.2570 - val_accuracy: 0.8617\n",
      "Epoch 65/100\n",
      "213/213 [==============================] - 37s 174ms/step - loss: 0.0822 - accuracy: 0.9635 - val_loss: 0.1468 - val_accuracy: 0.8830\n",
      "Epoch 66/100\n",
      "213/213 [==============================] - 37s 174ms/step - loss: 0.0849 - accuracy: 0.9552 - val_loss: 0.1379 - val_accuracy: 0.9043\n",
      "Epoch 67/100\n",
      "213/213 [==============================] - 37s 174ms/step - loss: 0.0567 - accuracy: 0.9764 - val_loss: 0.1528 - val_accuracy: 0.9043\n",
      "Epoch 68/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0447 - accuracy: 0.9859 - val_loss: 0.1810 - val_accuracy: 0.8936\n",
      "Epoch 69/100\n",
      "213/213 [==============================] - 37s 174ms/step - loss: 0.0672 - accuracy: 0.9717 - val_loss: 0.2172 - val_accuracy: 0.8830\n",
      "Epoch 70/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0631 - accuracy: 0.9670 - val_loss: 0.1771 - val_accuracy: 0.8936\n",
      "Epoch 71/100\n",
      "213/213 [==============================] - 37s 174ms/step - loss: 0.0547 - accuracy: 0.9823 - val_loss: 0.2169 - val_accuracy: 0.8723\n",
      "Epoch 72/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0644 - accuracy: 0.9741 - val_loss: 0.2343 - val_accuracy: 0.8830\n",
      "Epoch 73/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0732 - accuracy: 0.9623 - val_loss: 0.3369 - val_accuracy: 0.7979\n",
      "Epoch 74/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0593 - accuracy: 0.9764 - val_loss: 0.1508 - val_accuracy: 0.9043\n",
      "Epoch 75/100\n",
      "213/213 [==============================] - 37s 174ms/step - loss: 0.0561 - accuracy: 0.9776 - val_loss: 0.3188 - val_accuracy: 0.8617\n",
      "Epoch 76/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0505 - accuracy: 0.9800 - val_loss: 0.1605 - val_accuracy: 0.8936\n",
      "Epoch 77/100\n",
      "213/213 [==============================] - 37s 174ms/step - loss: 0.0644 - accuracy: 0.9717 - val_loss: 0.1533 - val_accuracy: 0.9255\n",
      "Epoch 78/100\n",
      "213/213 [==============================] - 37s 174ms/step - loss: 0.0682 - accuracy: 0.9658 - val_loss: 0.1553 - val_accuracy: 0.9149\n",
      "Epoch 79/100\n",
      "213/213 [==============================] - 37s 174ms/step - loss: 0.0762 - accuracy: 0.9600 - val_loss: 0.1291 - val_accuracy: 0.9255\n",
      "Epoch 80/100\n",
      "213/213 [==============================] - 37s 174ms/step - loss: 0.0506 - accuracy: 0.9776 - val_loss: 0.1548 - val_accuracy: 0.9255\n",
      "Epoch 81/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0731 - accuracy: 0.9682 - val_loss: 0.2340 - val_accuracy: 0.8404\n",
      "Epoch 82/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0587 - accuracy: 0.9764 - val_loss: 0.3072 - val_accuracy: 0.8298\n",
      "Epoch 83/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0510 - accuracy: 0.9753 - val_loss: 0.1594 - val_accuracy: 0.9149\n",
      "Epoch 84/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0672 - accuracy: 0.9611 - val_loss: 0.1652 - val_accuracy: 0.9043\n",
      "Epoch 85/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0834 - accuracy: 0.9564 - val_loss: 0.1568 - val_accuracy: 0.9255\n",
      "Epoch 86/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0651 - accuracy: 0.9812 - val_loss: 0.2518 - val_accuracy: 0.8723\n",
      "Epoch 87/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0715 - accuracy: 0.9682 - val_loss: 0.1634 - val_accuracy: 0.9149\n",
      "Epoch 88/100\n",
      "213/213 [==============================] - 38s 176ms/step - loss: 0.0392 - accuracy: 0.9859 - val_loss: 0.1678 - val_accuracy: 0.9255\n",
      "Epoch 89/100\n",
      "213/213 [==============================] - 37s 175ms/step - loss: 0.0696 - accuracy: 0.9776 - val_loss: 0.1426 - val_accuracy: 0.9255\n",
      "Epoch 90/100\n",
      "213/213 [==============================] - 37s 175ms/step - loss: 0.0551 - accuracy: 0.9741 - val_loss: 0.2001 - val_accuracy: 0.8830\n",
      "Epoch 91/100\n",
      "213/213 [==============================] - 37s 175ms/step - loss: 0.0683 - accuracy: 0.9670 - val_loss: 0.1574 - val_accuracy: 0.9043\n",
      "Epoch 92/100\n",
      "213/213 [==============================] - 37s 174ms/step - loss: 0.0690 - accuracy: 0.9658 - val_loss: 0.1660 - val_accuracy: 0.8723\n",
      "Epoch 93/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0588 - accuracy: 0.9741 - val_loss: 0.1652 - val_accuracy: 0.8936\n",
      "Epoch 94/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0606 - accuracy: 0.9741 - val_loss: 0.1512 - val_accuracy: 0.9043\n",
      "Epoch 95/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0709 - accuracy: 0.9682 - val_loss: 0.1713 - val_accuracy: 0.8936\n",
      "Epoch 96/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0438 - accuracy: 0.9870 - val_loss: 0.2010 - val_accuracy: 0.8617\n",
      "Epoch 97/100\n",
      "213/213 [==============================] - 37s 175ms/step - loss: 0.0504 - accuracy: 0.9835 - val_loss: 0.1589 - val_accuracy: 0.8936\n",
      "Epoch 98/100\n",
      "213/213 [==============================] - 37s 175ms/step - loss: 0.0902 - accuracy: 0.9552 - val_loss: 0.1685 - val_accuracy: 0.8936\n",
      "Epoch 99/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0588 - accuracy: 0.9753 - val_loss: 0.1692 - val_accuracy: 0.9255\n",
      "Epoch 100/100\n",
      "213/213 [==============================] - 37s 174ms/step - loss: 0.0502 - accuracy: 0.9788 - val_loss: 0.2894 - val_accuracy: 0.8404\n",
      "Score for fold 5: loss of 0.2894236743450165; accuracy of 84.04255509376526%\n",
      "(849,) (94,)\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 6 ...\n",
      "Epoch 1/100\n",
      "213/213 [==============================] - 46s 181ms/step - loss: 0.4804 - accuracy: 0.6455 - val_loss: 1.7463 - val_accuracy: 0.3298\n",
      "Epoch 2/100\n",
      "213/213 [==============================] - 38s 177ms/step - loss: 0.4076 - accuracy: 0.7291 - val_loss: 0.3189 - val_accuracy: 0.7553\n",
      "Epoch 3/100\n",
      "213/213 [==============================] - 37s 176ms/step - loss: 0.3739 - accuracy: 0.7609 - val_loss: 0.2842 - val_accuracy: 0.7447\n",
      "Epoch 4/100\n",
      "213/213 [==============================] - 37s 175ms/step - loss: 0.3071 - accuracy: 0.8092 - val_loss: 1.6937 - val_accuracy: 0.7766\n",
      "Epoch 5/100\n",
      "213/213 [==============================] - 38s 178ms/step - loss: 0.3398 - accuracy: 0.8080 - val_loss: 0.4282 - val_accuracy: 0.7234\n",
      "Epoch 6/100\n",
      "213/213 [==============================] - 37s 175ms/step - loss: 0.3018 - accuracy: 0.8351 - val_loss: 0.2749 - val_accuracy: 0.8191\n",
      "Epoch 7/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.3101 - accuracy: 0.8163 - val_loss: 0.1141 - val_accuracy: 0.9255\n",
      "Epoch 8/100\n",
      "213/213 [==============================] - 38s 178ms/step - loss: 0.2641 - accuracy: 0.8363 - val_loss: 1.8940 - val_accuracy: 0.6383\n",
      "Epoch 9/100\n",
      "213/213 [==============================] - 38s 177ms/step - loss: 0.2829 - accuracy: 0.8375 - val_loss: 0.1761 - val_accuracy: 0.9043\n",
      "Epoch 10/100\n",
      "213/213 [==============================] - 37s 175ms/step - loss: 0.2428 - accuracy: 0.8587 - val_loss: 0.2539 - val_accuracy: 0.8511\n",
      "Epoch 11/100\n",
      "213/213 [==============================] - 38s 176ms/step - loss: 0.2651 - accuracy: 0.8327 - val_loss: 0.2054 - val_accuracy: 0.9043\n",
      "Epoch 12/100\n",
      "213/213 [==============================] - 38s 178ms/step - loss: 0.2300 - accuracy: 0.8669 - val_loss: 0.1913 - val_accuracy: 0.8936\n",
      "Epoch 13/100\n",
      "213/213 [==============================] - 38s 177ms/step - loss: 0.2284 - accuracy: 0.8681 - val_loss: 0.6151 - val_accuracy: 0.7553\n",
      "Epoch 14/100\n",
      "213/213 [==============================] - 38s 178ms/step - loss: 0.2302 - accuracy: 0.8563 - val_loss: 0.1241 - val_accuracy: 0.9255\n",
      "Epoch 15/100\n",
      "213/213 [==============================] - 37s 175ms/step - loss: 0.2337 - accuracy: 0.8563 - val_loss: 0.1171 - val_accuracy: 0.9149\n",
      "Epoch 16/100\n",
      "213/213 [==============================] - 37s 175ms/step - loss: 0.2311 - accuracy: 0.8575 - val_loss: 0.3328 - val_accuracy: 0.8404\n",
      "Epoch 17/100\n",
      "213/213 [==============================] - 37s 175ms/step - loss: 0.2093 - accuracy: 0.8751 - val_loss: 0.2821 - val_accuracy: 0.8511\n",
      "Epoch 18/100\n",
      "213/213 [==============================] - 38s 175ms/step - loss: 0.2179 - accuracy: 0.8598 - val_loss: 0.2977 - val_accuracy: 0.8617\n",
      "Epoch 19/100\n",
      "213/213 [==============================] - 37s 175ms/step - loss: 0.1820 - accuracy: 0.8963 - val_loss: 0.2512 - val_accuracy: 0.8085\n",
      "Epoch 20/100\n",
      "213/213 [==============================] - 37s 175ms/step - loss: 0.1814 - accuracy: 0.8775 - val_loss: 0.1173 - val_accuracy: 0.9255\n",
      "Epoch 21/100\n",
      "213/213 [==============================] - 38s 175ms/step - loss: 0.1887 - accuracy: 0.8916 - val_loss: 0.3726 - val_accuracy: 0.7979\n",
      "Epoch 22/100\n",
      "213/213 [==============================] - 38s 177ms/step - loss: 0.2026 - accuracy: 0.8657 - val_loss: 0.1803 - val_accuracy: 0.8936\n",
      "Epoch 23/100\n",
      "213/213 [==============================] - 39s 181ms/step - loss: 0.1731 - accuracy: 0.8928 - val_loss: 0.1661 - val_accuracy: 0.8936\n",
      "Epoch 24/100\n",
      "213/213 [==============================] - 38s 178ms/step - loss: 0.1683 - accuracy: 0.8975 - val_loss: 0.1446 - val_accuracy: 0.8617\n",
      "Epoch 25/100\n",
      "213/213 [==============================] - 38s 179ms/step - loss: 0.1510 - accuracy: 0.9199 - val_loss: 0.2049 - val_accuracy: 0.8936\n",
      "Epoch 26/100\n",
      "213/213 [==============================] - 38s 179ms/step - loss: 0.1495 - accuracy: 0.9223 - val_loss: 0.1611 - val_accuracy: 0.8936\n",
      "Epoch 27/100\n",
      "213/213 [==============================] - 38s 178ms/step - loss: 0.1762 - accuracy: 0.9128 - val_loss: 0.1302 - val_accuracy: 0.9043\n",
      "Epoch 28/100\n",
      "213/213 [==============================] - 38s 179ms/step - loss: 0.1654 - accuracy: 0.8963 - val_loss: 0.1696 - val_accuracy: 0.8830\n",
      "Epoch 29/100\n",
      "213/213 [==============================] - 38s 179ms/step - loss: 0.1518 - accuracy: 0.9093 - val_loss: 0.4358 - val_accuracy: 0.7660\n",
      "Epoch 30/100\n",
      "213/213 [==============================] - 39s 180ms/step - loss: 0.1122 - accuracy: 0.9411 - val_loss: 0.1897 - val_accuracy: 0.8936\n",
      "Epoch 31/100\n",
      "213/213 [==============================] - 38s 177ms/step - loss: 0.1515 - accuracy: 0.9152 - val_loss: 0.2094 - val_accuracy: 0.8723\n",
      "Epoch 32/100\n",
      "213/213 [==============================] - 38s 177ms/step - loss: 0.1342 - accuracy: 0.9246 - val_loss: 0.1453 - val_accuracy: 0.9255\n",
      "Epoch 33/100\n",
      "213/213 [==============================] - 38s 177ms/step - loss: 0.1170 - accuracy: 0.9411 - val_loss: 0.3072 - val_accuracy: 0.7979\n",
      "Epoch 34/100\n",
      "213/213 [==============================] - 38s 176ms/step - loss: 0.1255 - accuracy: 0.9329 - val_loss: 0.1506 - val_accuracy: 0.8936\n",
      "Epoch 35/100\n",
      "213/213 [==============================] - 38s 176ms/step - loss: 0.1244 - accuracy: 0.9423 - val_loss: 0.1434 - val_accuracy: 0.9149\n",
      "Epoch 36/100\n",
      "213/213 [==============================] - 38s 177ms/step - loss: 0.1028 - accuracy: 0.9576 - val_loss: 0.1999 - val_accuracy: 0.8830\n",
      "Epoch 37/100\n",
      "213/213 [==============================] - 38s 176ms/step - loss: 0.0963 - accuracy: 0.9600 - val_loss: 0.2655 - val_accuracy: 0.8511\n",
      "Epoch 38/100\n",
      "213/213 [==============================] - 37s 174ms/step - loss: 0.1125 - accuracy: 0.9505 - val_loss: 0.1881 - val_accuracy: 0.8936\n",
      "Epoch 39/100\n",
      "213/213 [==============================] - 36s 169ms/step - loss: 0.1167 - accuracy: 0.9482 - val_loss: 0.5302 - val_accuracy: 0.7766\n",
      "Epoch 40/100\n",
      "213/213 [==============================] - 36s 168ms/step - loss: 0.1007 - accuracy: 0.9505 - val_loss: 0.1931 - val_accuracy: 0.8936\n",
      "Epoch 41/100\n",
      "213/213 [==============================] - 36s 170ms/step - loss: 0.0929 - accuracy: 0.9517 - val_loss: 0.1938 - val_accuracy: 0.8723\n",
      "Epoch 42/100\n",
      "213/213 [==============================] - 36s 170ms/step - loss: 0.0914 - accuracy: 0.9635 - val_loss: 0.2183 - val_accuracy: 0.8830\n",
      "Epoch 43/100\n",
      "213/213 [==============================] - 36s 168ms/step - loss: 0.0847 - accuracy: 0.9611 - val_loss: 0.1447 - val_accuracy: 0.9149\n",
      "Epoch 44/100\n",
      "213/213 [==============================] - 36s 167ms/step - loss: 0.0960 - accuracy: 0.9600 - val_loss: 0.1826 - val_accuracy: 0.8830\n",
      "Epoch 45/100\n",
      "213/213 [==============================] - 36s 167ms/step - loss: 0.0839 - accuracy: 0.9647 - val_loss: 0.2335 - val_accuracy: 0.8617\n",
      "Epoch 46/100\n",
      "213/213 [==============================] - 36s 168ms/step - loss: 0.0911 - accuracy: 0.9647 - val_loss: 0.2255 - val_accuracy: 0.8511\n",
      "Epoch 47/100\n",
      "213/213 [==============================] - 36s 171ms/step - loss: 0.1115 - accuracy: 0.9435 - val_loss: 0.1763 - val_accuracy: 0.8723\n",
      "Epoch 48/100\n",
      "213/213 [==============================] - 36s 170ms/step - loss: 0.1219 - accuracy: 0.9376 - val_loss: 0.2011 - val_accuracy: 0.8723\n",
      "Epoch 49/100\n",
      "213/213 [==============================] - 36s 168ms/step - loss: 0.0754 - accuracy: 0.9670 - val_loss: 0.1614 - val_accuracy: 0.8830\n",
      "Epoch 50/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0789 - accuracy: 0.9717 - val_loss: 0.2145 - val_accuracy: 0.8723\n",
      "Epoch 51/100\n",
      "213/213 [==============================] - 36s 170ms/step - loss: 0.0747 - accuracy: 0.9717 - val_loss: 0.2414 - val_accuracy: 0.8511\n",
      "Epoch 52/100\n",
      "213/213 [==============================] - 36s 170ms/step - loss: 0.0850 - accuracy: 0.9623 - val_loss: 0.2202 - val_accuracy: 0.8511\n",
      "Epoch 53/100\n",
      "213/213 [==============================] - 36s 168ms/step - loss: 0.0805 - accuracy: 0.9623 - val_loss: 0.3486 - val_accuracy: 0.8191\n",
      "Epoch 54/100\n",
      "213/213 [==============================] - 36s 169ms/step - loss: 0.0599 - accuracy: 0.9764 - val_loss: 0.2807 - val_accuracy: 0.8191\n",
      "Epoch 55/100\n",
      "213/213 [==============================] - 36s 168ms/step - loss: 0.0583 - accuracy: 0.9788 - val_loss: 0.1829 - val_accuracy: 0.8617\n",
      "Epoch 56/100\n",
      "213/213 [==============================] - 36s 167ms/step - loss: 0.0752 - accuracy: 0.9694 - val_loss: 0.1900 - val_accuracy: 0.8511\n",
      "Epoch 57/100\n",
      "213/213 [==============================] - 36s 167ms/step - loss: 0.0816 - accuracy: 0.9600 - val_loss: 0.2011 - val_accuracy: 0.8830\n",
      "Epoch 58/100\n",
      "213/213 [==============================] - 36s 168ms/step - loss: 0.0613 - accuracy: 0.9753 - val_loss: 0.1739 - val_accuracy: 0.9043\n",
      "Epoch 59/100\n",
      "213/213 [==============================] - 36s 167ms/step - loss: 0.0842 - accuracy: 0.9623 - val_loss: 0.2724 - val_accuracy: 0.8085\n",
      "Epoch 60/100\n",
      "213/213 [==============================] - 36s 167ms/step - loss: 0.0806 - accuracy: 0.9611 - val_loss: 0.1577 - val_accuracy: 0.9043\n",
      "Epoch 61/100\n",
      "213/213 [==============================] - 36s 167ms/step - loss: 0.0740 - accuracy: 0.9623 - val_loss: 0.2121 - val_accuracy: 0.8617\n",
      "Epoch 62/100\n",
      "213/213 [==============================] - 36s 168ms/step - loss: 0.0780 - accuracy: 0.9600 - val_loss: 0.2922 - val_accuracy: 0.8298\n",
      "Epoch 63/100\n",
      "213/213 [==============================] - 36s 167ms/step - loss: 0.0883 - accuracy: 0.9541 - val_loss: 0.1747 - val_accuracy: 0.8830\n",
      "Epoch 64/100\n",
      "213/213 [==============================] - 36s 168ms/step - loss: 0.0634 - accuracy: 0.9729 - val_loss: 0.2349 - val_accuracy: 0.8404\n",
      "Epoch 65/100\n",
      "213/213 [==============================] - 36s 167ms/step - loss: 0.0882 - accuracy: 0.9600 - val_loss: 0.1570 - val_accuracy: 0.8723\n",
      "Epoch 66/100\n",
      "213/213 [==============================] - 36s 168ms/step - loss: 0.0747 - accuracy: 0.9647 - val_loss: 0.1897 - val_accuracy: 0.8830\n",
      "Epoch 67/100\n",
      "213/213 [==============================] - 36s 168ms/step - loss: 0.0597 - accuracy: 0.9753 - val_loss: 0.2925 - val_accuracy: 0.8191\n",
      "Epoch 68/100\n",
      "213/213 [==============================] - 36s 168ms/step - loss: 0.0643 - accuracy: 0.9682 - val_loss: 0.2244 - val_accuracy: 0.8723\n",
      "Epoch 69/100\n",
      "213/213 [==============================] - 36s 168ms/step - loss: 0.0660 - accuracy: 0.9764 - val_loss: 0.2031 - val_accuracy: 0.8617\n",
      "Epoch 70/100\n",
      "213/213 [==============================] - 36s 168ms/step - loss: 0.0580 - accuracy: 0.9776 - val_loss: 0.1852 - val_accuracy: 0.8723\n",
      "Epoch 71/100\n",
      "213/213 [==============================] - 36s 167ms/step - loss: 0.0832 - accuracy: 0.9541 - val_loss: 0.1504 - val_accuracy: 0.9043\n",
      "Epoch 72/100\n",
      "213/213 [==============================] - 36s 168ms/step - loss: 0.0638 - accuracy: 0.9682 - val_loss: 0.1801 - val_accuracy: 0.8830\n",
      "Epoch 73/100\n",
      "213/213 [==============================] - 36s 168ms/step - loss: 0.0511 - accuracy: 0.9788 - val_loss: 0.1786 - val_accuracy: 0.8723\n",
      "Epoch 74/100\n",
      "213/213 [==============================] - 36s 168ms/step - loss: 0.0700 - accuracy: 0.9682 - val_loss: 0.2039 - val_accuracy: 0.8723\n",
      "Epoch 75/100\n",
      "213/213 [==============================] - 36s 168ms/step - loss: 0.0569 - accuracy: 0.9741 - val_loss: 0.2098 - val_accuracy: 0.8830\n",
      "Epoch 76/100\n",
      "213/213 [==============================] - 36s 168ms/step - loss: 0.0387 - accuracy: 0.9882 - val_loss: 0.1898 - val_accuracy: 0.8723\n",
      "Epoch 77/100\n",
      "213/213 [==============================] - 36s 167ms/step - loss: 0.0772 - accuracy: 0.9658 - val_loss: 0.2180 - val_accuracy: 0.8617\n",
      "Epoch 78/100\n",
      "213/213 [==============================] - 36s 169ms/step - loss: 0.0450 - accuracy: 0.9812 - val_loss: 0.2511 - val_accuracy: 0.8191\n",
      "Epoch 79/100\n",
      "213/213 [==============================] - 36s 169ms/step - loss: 0.0870 - accuracy: 0.9576 - val_loss: 0.1881 - val_accuracy: 0.8830\n",
      "Epoch 80/100\n",
      "213/213 [==============================] - 36s 168ms/step - loss: 0.0538 - accuracy: 0.9753 - val_loss: 0.2138 - val_accuracy: 0.8617\n",
      "Epoch 81/100\n",
      "213/213 [==============================] - 36s 168ms/step - loss: 0.0738 - accuracy: 0.9635 - val_loss: 0.2355 - val_accuracy: 0.8404\n",
      "Epoch 82/100\n",
      "213/213 [==============================] - 36s 167ms/step - loss: 0.0957 - accuracy: 0.9423 - val_loss: 0.1994 - val_accuracy: 0.8723\n",
      "Epoch 83/100\n",
      "213/213 [==============================] - 36s 168ms/step - loss: 0.0772 - accuracy: 0.9623 - val_loss: 0.1783 - val_accuracy: 0.8936\n",
      "Epoch 84/100\n",
      "213/213 [==============================] - 36s 168ms/step - loss: 0.0794 - accuracy: 0.9694 - val_loss: 0.1997 - val_accuracy: 0.8617\n",
      "Epoch 85/100\n",
      "213/213 [==============================] - 36s 168ms/step - loss: 0.0606 - accuracy: 0.9764 - val_loss: 0.1791 - val_accuracy: 0.8723\n",
      "Epoch 86/100\n",
      "213/213 [==============================] - 36s 167ms/step - loss: 0.0487 - accuracy: 0.9812 - val_loss: 0.2015 - val_accuracy: 0.8404\n",
      "Epoch 87/100\n",
      "213/213 [==============================] - 36s 167ms/step - loss: 0.0707 - accuracy: 0.9682 - val_loss: 0.1733 - val_accuracy: 0.8617\n",
      "Epoch 88/100\n",
      "213/213 [==============================] - 36s 168ms/step - loss: 0.0524 - accuracy: 0.9764 - val_loss: 0.2104 - val_accuracy: 0.8617\n",
      "Epoch 89/100\n",
      "213/213 [==============================] - 36s 168ms/step - loss: 0.0745 - accuracy: 0.9623 - val_loss: 0.1941 - val_accuracy: 0.8723\n",
      "Epoch 90/100\n",
      "213/213 [==============================] - 37s 171ms/step - loss: 0.0421 - accuracy: 0.9870 - val_loss: 0.1751 - val_accuracy: 0.8830\n",
      "Epoch 91/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0513 - accuracy: 0.9776 - val_loss: 0.1860 - val_accuracy: 0.8830\n",
      "Epoch 92/100\n",
      "213/213 [==============================] - 37s 174ms/step - loss: 0.0645 - accuracy: 0.9658 - val_loss: 0.1639 - val_accuracy: 0.8830\n",
      "Epoch 93/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0542 - accuracy: 0.9788 - val_loss: 0.2535 - val_accuracy: 0.8511\n",
      "Epoch 94/100\n",
      "213/213 [==============================] - 37s 174ms/step - loss: 0.0698 - accuracy: 0.9682 - val_loss: 0.2352 - val_accuracy: 0.8404\n",
      "Epoch 95/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0545 - accuracy: 0.9729 - val_loss: 0.1825 - val_accuracy: 0.8830\n",
      "Epoch 96/100\n",
      "213/213 [==============================] - 37s 174ms/step - loss: 0.0535 - accuracy: 0.9823 - val_loss: 0.2100 - val_accuracy: 0.8617\n",
      "Epoch 97/100\n",
      "213/213 [==============================] - 37s 174ms/step - loss: 0.0629 - accuracy: 0.9694 - val_loss: 0.1658 - val_accuracy: 0.8936\n",
      "Epoch 98/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0591 - accuracy: 0.9717 - val_loss: 0.1792 - val_accuracy: 0.8830\n",
      "Epoch 99/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0654 - accuracy: 0.9670 - val_loss: 0.1753 - val_accuracy: 0.8830\n",
      "Epoch 100/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0750 - accuracy: 0.9588 - val_loss: 0.1774 - val_accuracy: 0.8830\n",
      "Score for fold 6: loss of 0.17741024494171143; accuracy of 88.29787373542786%\n",
      "(849,) (94,)\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 7 ...\n",
      "Epoch 1/100\n",
      "213/213 [==============================] - 45s 179ms/step - loss: 0.5020 - accuracy: 0.6584 - val_loss: 1.6131 - val_accuracy: 0.3298\n",
      "Epoch 2/100\n",
      "213/213 [==============================] - 37s 174ms/step - loss: 0.4116 - accuracy: 0.7208 - val_loss: 0.3688 - val_accuracy: 0.7128\n",
      "Epoch 3/100\n",
      "213/213 [==============================] - 37s 174ms/step - loss: 0.3566 - accuracy: 0.7845 - val_loss: 0.8310 - val_accuracy: 0.8617\n",
      "Epoch 4/100\n",
      "213/213 [==============================] - 37s 174ms/step - loss: 0.3417 - accuracy: 0.8009 - val_loss: 0.1417 - val_accuracy: 0.9574\n",
      "Epoch 5/100\n",
      "213/213 [==============================] - 37s 174ms/step - loss: 0.3317 - accuracy: 0.7868 - val_loss: 0.3646 - val_accuracy: 0.7234\n",
      "Epoch 6/100\n",
      "213/213 [==============================] - 37s 174ms/step - loss: 0.3252 - accuracy: 0.7892 - val_loss: 0.2115 - val_accuracy: 0.9149\n",
      "Epoch 7/100\n",
      "213/213 [==============================] - 38s 177ms/step - loss: 0.2659 - accuracy: 0.8492 - val_loss: 0.1226 - val_accuracy: 0.9468\n",
      "Epoch 8/100\n",
      "213/213 [==============================] - 38s 179ms/step - loss: 0.2928 - accuracy: 0.8292 - val_loss: 0.1562 - val_accuracy: 0.9043\n",
      "Epoch 9/100\n",
      "213/213 [==============================] - 39s 180ms/step - loss: 0.2594 - accuracy: 0.8445 - val_loss: 0.3079 - val_accuracy: 0.8191\n",
      "Epoch 10/100\n",
      "213/213 [==============================] - 38s 178ms/step - loss: 0.2839 - accuracy: 0.8257 - val_loss: 0.1523 - val_accuracy: 0.8936\n",
      "Epoch 11/100\n",
      "213/213 [==============================] - 40s 185ms/step - loss: 0.2346 - accuracy: 0.8598 - val_loss: 0.1457 - val_accuracy: 0.9149\n",
      "Epoch 12/100\n",
      "213/213 [==============================] - 40s 187ms/step - loss: 0.2360 - accuracy: 0.8634 - val_loss: 0.3526 - val_accuracy: 0.7234\n",
      "Epoch 13/100\n",
      "213/213 [==============================] - 38s 177ms/step - loss: 0.2427 - accuracy: 0.8445 - val_loss: 0.3629 - val_accuracy: 0.7447\n",
      "Epoch 14/100\n",
      "213/213 [==============================] - 38s 176ms/step - loss: 0.2339 - accuracy: 0.8587 - val_loss: 0.1118 - val_accuracy: 0.9255\n",
      "Epoch 15/100\n",
      "213/213 [==============================] - 37s 175ms/step - loss: 0.2110 - accuracy: 0.8740 - val_loss: 0.0996 - val_accuracy: 0.9468\n",
      "Epoch 16/100\n",
      "213/213 [==============================] - 37s 175ms/step - loss: 0.2130 - accuracy: 0.8693 - val_loss: 0.6883 - val_accuracy: 0.6809\n",
      "Epoch 17/100\n",
      "213/213 [==============================] - 38s 177ms/step - loss: 0.2275 - accuracy: 0.8528 - val_loss: 0.1251 - val_accuracy: 0.9362\n",
      "Epoch 18/100\n",
      "213/213 [==============================] - 38s 177ms/step - loss: 0.1965 - accuracy: 0.8751 - val_loss: 0.1208 - val_accuracy: 0.9255\n",
      "Epoch 19/100\n",
      "213/213 [==============================] - 38s 177ms/step - loss: 0.2080 - accuracy: 0.8693 - val_loss: 0.2043 - val_accuracy: 0.8830\n",
      "Epoch 20/100\n",
      "213/213 [==============================] - 38s 178ms/step - loss: 0.1977 - accuracy: 0.8799 - val_loss: 0.1816 - val_accuracy: 0.8723\n",
      "Epoch 21/100\n",
      "213/213 [==============================] - 38s 177ms/step - loss: 0.1965 - accuracy: 0.8622 - val_loss: 0.1475 - val_accuracy: 0.9043\n",
      "Epoch 22/100\n",
      "213/213 [==============================] - 37s 175ms/step - loss: 0.1833 - accuracy: 0.8834 - val_loss: 0.2041 - val_accuracy: 0.8511\n",
      "Epoch 23/100\n",
      "213/213 [==============================] - 37s 175ms/step - loss: 0.1785 - accuracy: 0.8975 - val_loss: 0.1502 - val_accuracy: 0.9149\n",
      "Epoch 24/100\n",
      "213/213 [==============================] - 37s 174ms/step - loss: 0.1902 - accuracy: 0.8810 - val_loss: 0.2927 - val_accuracy: 0.8085\n",
      "Epoch 25/100\n",
      "213/213 [==============================] - 38s 176ms/step - loss: 0.1826 - accuracy: 0.8775 - val_loss: 0.1015 - val_accuracy: 0.9681\n",
      "Epoch 26/100\n",
      "213/213 [==============================] - 37s 175ms/step - loss: 0.1825 - accuracy: 0.8763 - val_loss: 0.1594 - val_accuracy: 0.9043\n",
      "Epoch 27/100\n",
      "213/213 [==============================] - 37s 175ms/step - loss: 0.1592 - accuracy: 0.8893 - val_loss: 0.1696 - val_accuracy: 0.8617\n",
      "Epoch 28/100\n",
      "213/213 [==============================] - 37s 175ms/step - loss: 0.1627 - accuracy: 0.9022 - val_loss: 0.1293 - val_accuracy: 0.9362\n",
      "Epoch 29/100\n",
      "213/213 [==============================] - 37s 174ms/step - loss: 0.1596 - accuracy: 0.9022 - val_loss: 0.1835 - val_accuracy: 0.9149\n",
      "Epoch 30/100\n",
      "213/213 [==============================] - 37s 174ms/step - loss: 0.1444 - accuracy: 0.9140 - val_loss: 0.1598 - val_accuracy: 0.8830\n",
      "Epoch 31/100\n",
      "213/213 [==============================] - 37s 174ms/step - loss: 0.1282 - accuracy: 0.9081 - val_loss: 0.2442 - val_accuracy: 0.8298\n",
      "Epoch 32/100\n",
      "213/213 [==============================] - 37s 174ms/step - loss: 0.1334 - accuracy: 0.9093 - val_loss: 0.1148 - val_accuracy: 0.9255\n",
      "Epoch 33/100\n",
      "213/213 [==============================] - 37s 174ms/step - loss: 0.1842 - accuracy: 0.8834 - val_loss: 0.1293 - val_accuracy: 0.9255\n",
      "Epoch 34/100\n",
      "213/213 [==============================] - 37s 175ms/step - loss: 0.1550 - accuracy: 0.9140 - val_loss: 0.1254 - val_accuracy: 0.9362\n",
      "Epoch 35/100\n",
      "213/213 [==============================] - 37s 175ms/step - loss: 0.1397 - accuracy: 0.9246 - val_loss: 0.1063 - val_accuracy: 0.9468\n",
      "Epoch 36/100\n",
      "213/213 [==============================] - 37s 174ms/step - loss: 0.1524 - accuracy: 0.9046 - val_loss: 0.1779 - val_accuracy: 0.8830\n",
      "Epoch 37/100\n",
      "213/213 [==============================] - 37s 174ms/step - loss: 0.1557 - accuracy: 0.9046 - val_loss: 0.1584 - val_accuracy: 0.9043\n",
      "Epoch 38/100\n",
      "213/213 [==============================] - 37s 174ms/step - loss: 0.1293 - accuracy: 0.9246 - val_loss: 0.1449 - val_accuracy: 0.9043\n",
      "Epoch 39/100\n",
      "213/213 [==============================] - 37s 174ms/step - loss: 0.1289 - accuracy: 0.9234 - val_loss: 0.0962 - val_accuracy: 0.9574\n",
      "Epoch 40/100\n",
      "213/213 [==============================] - 37s 174ms/step - loss: 0.1123 - accuracy: 0.9388 - val_loss: 0.0998 - val_accuracy: 0.9468\n",
      "Epoch 41/100\n",
      "213/213 [==============================] - 37s 174ms/step - loss: 0.1144 - accuracy: 0.9364 - val_loss: 0.1345 - val_accuracy: 0.9362\n",
      "Epoch 42/100\n",
      "213/213 [==============================] - 37s 174ms/step - loss: 0.1090 - accuracy: 0.9494 - val_loss: 0.0969 - val_accuracy: 0.9574\n",
      "Epoch 43/100\n",
      "213/213 [==============================] - 37s 174ms/step - loss: 0.1115 - accuracy: 0.9458 - val_loss: 0.1658 - val_accuracy: 0.9149\n",
      "Epoch 44/100\n",
      "213/213 [==============================] - 37s 174ms/step - loss: 0.1134 - accuracy: 0.9446 - val_loss: 0.1362 - val_accuracy: 0.9149\n",
      "Epoch 45/100\n",
      "213/213 [==============================] - 37s 175ms/step - loss: 0.1243 - accuracy: 0.9340 - val_loss: 0.1033 - val_accuracy: 0.9468\n",
      "Epoch 46/100\n",
      "213/213 [==============================] - 37s 174ms/step - loss: 0.1338 - accuracy: 0.9234 - val_loss: 0.1198 - val_accuracy: 0.9362\n",
      "Epoch 47/100\n",
      "213/213 [==============================] - 37s 174ms/step - loss: 0.1079 - accuracy: 0.9529 - val_loss: 0.2662 - val_accuracy: 0.8191\n",
      "Epoch 48/100\n",
      "213/213 [==============================] - 37s 174ms/step - loss: 0.0896 - accuracy: 0.9611 - val_loss: 0.1056 - val_accuracy: 0.9468\n",
      "Epoch 49/100\n",
      "213/213 [==============================] - 37s 174ms/step - loss: 0.0773 - accuracy: 0.9647 - val_loss: 0.1238 - val_accuracy: 0.9043\n",
      "Epoch 50/100\n",
      "213/213 [==============================] - 37s 174ms/step - loss: 0.0821 - accuracy: 0.9670 - val_loss: 0.1187 - val_accuracy: 0.9362\n",
      "Epoch 51/100\n",
      "213/213 [==============================] - 37s 174ms/step - loss: 0.1157 - accuracy: 0.9446 - val_loss: 0.1460 - val_accuracy: 0.9149\n",
      "Epoch 52/100\n",
      "213/213 [==============================] - 37s 174ms/step - loss: 0.0975 - accuracy: 0.9647 - val_loss: 0.1145 - val_accuracy: 0.9468\n",
      "Epoch 53/100\n",
      "213/213 [==============================] - 37s 174ms/step - loss: 0.1201 - accuracy: 0.9340 - val_loss: 0.1519 - val_accuracy: 0.9149\n",
      "Epoch 54/100\n",
      "213/213 [==============================] - 37s 175ms/step - loss: 0.0840 - accuracy: 0.9611 - val_loss: 0.1114 - val_accuracy: 0.9362\n",
      "Epoch 55/100\n",
      "213/213 [==============================] - 37s 174ms/step - loss: 0.0855 - accuracy: 0.9647 - val_loss: 0.1168 - val_accuracy: 0.9468\n",
      "Epoch 56/100\n",
      "213/213 [==============================] - 37s 174ms/step - loss: 0.0843 - accuracy: 0.9623 - val_loss: 0.1125 - val_accuracy: 0.9468\n",
      "Epoch 57/100\n",
      "213/213 [==============================] - 37s 174ms/step - loss: 0.0783 - accuracy: 0.9658 - val_loss: 0.1298 - val_accuracy: 0.9362\n",
      "Epoch 58/100\n",
      "213/213 [==============================] - 37s 174ms/step - loss: 0.0919 - accuracy: 0.9517 - val_loss: 0.1113 - val_accuracy: 0.9362\n",
      "Epoch 59/100\n",
      "213/213 [==============================] - 37s 174ms/step - loss: 0.0883 - accuracy: 0.9552 - val_loss: 0.0976 - val_accuracy: 0.9574\n",
      "Epoch 60/100\n",
      "213/213 [==============================] - 37s 174ms/step - loss: 0.0908 - accuracy: 0.9635 - val_loss: 0.1029 - val_accuracy: 0.9468\n",
      "Epoch 61/100\n",
      "213/213 [==============================] - 37s 175ms/step - loss: 0.0508 - accuracy: 0.9823 - val_loss: 0.1183 - val_accuracy: 0.8936\n",
      "Epoch 62/100\n",
      "213/213 [==============================] - 37s 174ms/step - loss: 0.0796 - accuracy: 0.9635 - val_loss: 0.1310 - val_accuracy: 0.9255\n",
      "Epoch 63/100\n",
      "213/213 [==============================] - 36s 170ms/step - loss: 0.0654 - accuracy: 0.9776 - val_loss: 0.0994 - val_accuracy: 0.9468\n",
      "Epoch 64/100\n",
      "213/213 [==============================] - 36s 167ms/step - loss: 0.0636 - accuracy: 0.9706 - val_loss: 0.1420 - val_accuracy: 0.9043\n",
      "Epoch 65/100\n",
      "213/213 [==============================] - 36s 168ms/step - loss: 0.0655 - accuracy: 0.9682 - val_loss: 0.1283 - val_accuracy: 0.9149\n",
      "Epoch 66/100\n",
      "213/213 [==============================] - 36s 167ms/step - loss: 0.0868 - accuracy: 0.9552 - val_loss: 0.0930 - val_accuracy: 0.9468\n",
      "Epoch 67/100\n",
      "213/213 [==============================] - 36s 168ms/step - loss: 0.0554 - accuracy: 0.9812 - val_loss: 0.1064 - val_accuracy: 0.9468\n",
      "Epoch 68/100\n",
      "213/213 [==============================] - 36s 167ms/step - loss: 0.0553 - accuracy: 0.9764 - val_loss: 0.1417 - val_accuracy: 0.9255\n",
      "Epoch 69/100\n",
      "213/213 [==============================] - 36s 167ms/step - loss: 0.0414 - accuracy: 0.9835 - val_loss: 0.0968 - val_accuracy: 0.9468\n",
      "Epoch 70/100\n",
      "213/213 [==============================] - 36s 168ms/step - loss: 0.0824 - accuracy: 0.9611 - val_loss: 0.0959 - val_accuracy: 0.9468\n",
      "Epoch 71/100\n",
      "213/213 [==============================] - 36s 168ms/step - loss: 0.0811 - accuracy: 0.9694 - val_loss: 0.1271 - val_accuracy: 0.9149\n",
      "Epoch 72/100\n",
      "213/213 [==============================] - 36s 168ms/step - loss: 0.0835 - accuracy: 0.9552 - val_loss: 0.1384 - val_accuracy: 0.9149\n",
      "Epoch 73/100\n",
      "213/213 [==============================] - 36s 167ms/step - loss: 0.0759 - accuracy: 0.9717 - val_loss: 0.0970 - val_accuracy: 0.9362\n",
      "Epoch 74/100\n",
      "213/213 [==============================] - 36s 168ms/step - loss: 0.0684 - accuracy: 0.9717 - val_loss: 0.1260 - val_accuracy: 0.9362\n",
      "Epoch 75/100\n",
      "213/213 [==============================] - 36s 168ms/step - loss: 0.0567 - accuracy: 0.9800 - val_loss: 0.1391 - val_accuracy: 0.9043\n",
      "Epoch 76/100\n",
      "213/213 [==============================] - 36s 168ms/step - loss: 0.0412 - accuracy: 0.9870 - val_loss: 0.1082 - val_accuracy: 0.9255\n",
      "Epoch 77/100\n",
      "213/213 [==============================] - 36s 170ms/step - loss: 0.0777 - accuracy: 0.9682 - val_loss: 0.0998 - val_accuracy: 0.9362\n",
      "Epoch 78/100\n",
      "213/213 [==============================] - 36s 168ms/step - loss: 0.0717 - accuracy: 0.9729 - val_loss: 0.1021 - val_accuracy: 0.9468\n",
      "Epoch 79/100\n",
      "213/213 [==============================] - 36s 167ms/step - loss: 0.0697 - accuracy: 0.9717 - val_loss: 0.1141 - val_accuracy: 0.9362\n",
      "Epoch 80/100\n",
      "213/213 [==============================] - 36s 167ms/step - loss: 0.0690 - accuracy: 0.9706 - val_loss: 0.0992 - val_accuracy: 0.9468\n",
      "Epoch 81/100\n",
      "213/213 [==============================] - 36s 167ms/step - loss: 0.0433 - accuracy: 0.9870 - val_loss: 0.1057 - val_accuracy: 0.9468\n",
      "Epoch 82/100\n",
      "213/213 [==============================] - 36s 167ms/step - loss: 0.0798 - accuracy: 0.9706 - val_loss: 0.1045 - val_accuracy: 0.9362\n",
      "Epoch 83/100\n",
      "213/213 [==============================] - 36s 167ms/step - loss: 0.0641 - accuracy: 0.9694 - val_loss: 0.0999 - val_accuracy: 0.9362\n",
      "Epoch 84/100\n",
      "213/213 [==============================] - 36s 167ms/step - loss: 0.0514 - accuracy: 0.9800 - val_loss: 0.1088 - val_accuracy: 0.9362\n",
      "Epoch 85/100\n",
      "213/213 [==============================] - 36s 167ms/step - loss: 0.0597 - accuracy: 0.9764 - val_loss: 0.1192 - val_accuracy: 0.9149\n",
      "Epoch 86/100\n",
      "213/213 [==============================] - 36s 167ms/step - loss: 0.0627 - accuracy: 0.9694 - val_loss: 0.1001 - val_accuracy: 0.9362\n",
      "Epoch 87/100\n",
      "213/213 [==============================] - 36s 167ms/step - loss: 0.0696 - accuracy: 0.9670 - val_loss: 0.1435 - val_accuracy: 0.9255\n",
      "Epoch 88/100\n",
      "213/213 [==============================] - 36s 167ms/step - loss: 0.0686 - accuracy: 0.9694 - val_loss: 0.0973 - val_accuracy: 0.9362\n",
      "Epoch 89/100\n",
      "213/213 [==============================] - 36s 167ms/step - loss: 0.0483 - accuracy: 0.9776 - val_loss: 0.1090 - val_accuracy: 0.9255\n",
      "Epoch 90/100\n",
      "213/213 [==============================] - 36s 166ms/step - loss: 0.0757 - accuracy: 0.9600 - val_loss: 0.1058 - val_accuracy: 0.9149\n",
      "Epoch 91/100\n",
      "213/213 [==============================] - 36s 166ms/step - loss: 0.0323 - accuracy: 0.9918 - val_loss: 0.1063 - val_accuracy: 0.9149\n",
      "Epoch 92/100\n",
      "213/213 [==============================] - 36s 166ms/step - loss: 0.0714 - accuracy: 0.9694 - val_loss: 0.1138 - val_accuracy: 0.9149\n",
      "Epoch 93/100\n",
      "213/213 [==============================] - 36s 166ms/step - loss: 0.0848 - accuracy: 0.9600 - val_loss: 0.1187 - val_accuracy: 0.9255\n",
      "Epoch 94/100\n",
      "213/213 [==============================] - 36s 166ms/step - loss: 0.0513 - accuracy: 0.9788 - val_loss: 0.1068 - val_accuracy: 0.9362\n",
      "Epoch 95/100\n",
      "213/213 [==============================] - 36s 166ms/step - loss: 0.0915 - accuracy: 0.9576 - val_loss: 0.1021 - val_accuracy: 0.9362\n",
      "Epoch 96/100\n",
      "213/213 [==============================] - 36s 167ms/step - loss: 0.0519 - accuracy: 0.9764 - val_loss: 0.1099 - val_accuracy: 0.9255\n",
      "Epoch 97/100\n",
      "213/213 [==============================] - 36s 166ms/step - loss: 0.0510 - accuracy: 0.9776 - val_loss: 0.1018 - val_accuracy: 0.9362\n",
      "Epoch 98/100\n",
      "213/213 [==============================] - 36s 167ms/step - loss: 0.0707 - accuracy: 0.9694 - val_loss: 0.1149 - val_accuracy: 0.9149\n",
      "Epoch 99/100\n",
      "213/213 [==============================] - 36s 167ms/step - loss: 0.0545 - accuracy: 0.9741 - val_loss: 0.1048 - val_accuracy: 0.9149\n",
      "Epoch 100/100\n",
      "213/213 [==============================] - 36s 166ms/step - loss: 0.0595 - accuracy: 0.9729 - val_loss: 0.1054 - val_accuracy: 0.9362\n",
      "Score for fold 7: loss of 0.10538401454687119; accuracy of 93.6170220375061%\n",
      "(849,) (94,)\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 8 ...\n",
      "Epoch 1/100\n",
      "213/213 [==============================] - 43s 173ms/step - loss: 0.4823 - accuracy: 0.6702 - val_loss: 1.6416 - val_accuracy: 0.3404\n",
      "Epoch 2/100\n",
      "213/213 [==============================] - 36s 166ms/step - loss: 0.4019 - accuracy: 0.7256 - val_loss: 0.2397 - val_accuracy: 0.8830\n",
      "Epoch 3/100\n",
      "213/213 [==============================] - 35s 166ms/step - loss: 0.3360 - accuracy: 0.8092 - val_loss: 5.1757 - val_accuracy: 0.4681\n",
      "Epoch 4/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.3031 - accuracy: 0.8210 - val_loss: 0.3260 - val_accuracy: 0.7872\n",
      "Epoch 5/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.3301 - accuracy: 0.7939 - val_loss: 6.3804 - val_accuracy: 0.4894\n",
      "Epoch 6/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.3182 - accuracy: 0.8104 - val_loss: 0.1464 - val_accuracy: 0.9149\n",
      "Epoch 7/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.2675 - accuracy: 0.8598 - val_loss: 0.1525 - val_accuracy: 0.9255\n",
      "Epoch 8/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.2622 - accuracy: 0.8563 - val_loss: 0.1728 - val_accuracy: 0.9043\n",
      "Epoch 9/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.2618 - accuracy: 0.8551 - val_loss: 0.2043 - val_accuracy: 0.8830\n",
      "Epoch 10/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.2684 - accuracy: 0.8269 - val_loss: 0.2447 - val_accuracy: 0.8404\n",
      "Epoch 11/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.2592 - accuracy: 0.8363 - val_loss: 0.1203 - val_accuracy: 0.9362\n",
      "Epoch 12/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.2597 - accuracy: 0.8375 - val_loss: 0.1678 - val_accuracy: 0.9149\n",
      "Epoch 13/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.2239 - accuracy: 0.8575 - val_loss: 0.2850 - val_accuracy: 0.7979\n",
      "Epoch 14/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.2265 - accuracy: 0.8433 - val_loss: 0.1210 - val_accuracy: 0.9468\n",
      "Epoch 15/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.2410 - accuracy: 0.8410 - val_loss: 0.1995 - val_accuracy: 0.8511\n",
      "Epoch 16/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.2167 - accuracy: 0.8751 - val_loss: 0.1146 - val_accuracy: 0.9574\n",
      "Epoch 17/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.2184 - accuracy: 0.8681 - val_loss: 0.1284 - val_accuracy: 0.9149\n",
      "Epoch 18/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.2185 - accuracy: 0.8681 - val_loss: 0.1466 - val_accuracy: 0.9043\n",
      "Epoch 19/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.2057 - accuracy: 0.8728 - val_loss: 0.1571 - val_accuracy: 0.8936\n",
      "Epoch 20/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.1847 - accuracy: 0.8822 - val_loss: 0.3664 - val_accuracy: 0.7872\n",
      "Epoch 21/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.2088 - accuracy: 0.8763 - val_loss: 0.3001 - val_accuracy: 0.7872\n",
      "Epoch 22/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.1909 - accuracy: 0.8857 - val_loss: 0.1280 - val_accuracy: 0.9255\n",
      "Epoch 23/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.1668 - accuracy: 0.8963 - val_loss: 0.1824 - val_accuracy: 0.8936\n",
      "Epoch 24/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.1683 - accuracy: 0.8975 - val_loss: 0.1341 - val_accuracy: 0.9468\n",
      "Epoch 25/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.1726 - accuracy: 0.8963 - val_loss: 0.1202 - val_accuracy: 0.9149\n",
      "Epoch 26/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.1599 - accuracy: 0.9140 - val_loss: 0.1306 - val_accuracy: 0.9255\n",
      "Epoch 27/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.1269 - accuracy: 0.9305 - val_loss: 0.1066 - val_accuracy: 0.9362\n",
      "Epoch 28/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.1397 - accuracy: 0.9246 - val_loss: 0.1506 - val_accuracy: 0.9043\n",
      "Epoch 29/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.1595 - accuracy: 0.9128 - val_loss: 0.2164 - val_accuracy: 0.8723\n",
      "Epoch 30/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.1447 - accuracy: 0.9176 - val_loss: 0.1307 - val_accuracy: 0.9574\n",
      "Epoch 31/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.1183 - accuracy: 0.9399 - val_loss: 0.1140 - val_accuracy: 0.9362\n",
      "Epoch 32/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.1354 - accuracy: 0.9270 - val_loss: 0.1900 - val_accuracy: 0.8723\n",
      "Epoch 33/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.1420 - accuracy: 0.9164 - val_loss: 0.1126 - val_accuracy: 0.9574\n",
      "Epoch 34/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.1202 - accuracy: 0.9423 - val_loss: 0.1252 - val_accuracy: 0.9574\n",
      "Epoch 35/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.1308 - accuracy: 0.9376 - val_loss: 0.1585 - val_accuracy: 0.9149\n",
      "Epoch 36/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.1162 - accuracy: 0.9470 - val_loss: 0.1400 - val_accuracy: 0.9362\n",
      "Epoch 37/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.1375 - accuracy: 0.9282 - val_loss: 0.1204 - val_accuracy: 0.9149\n",
      "Epoch 38/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0948 - accuracy: 0.9517 - val_loss: 0.2215 - val_accuracy: 0.8723\n",
      "Epoch 39/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.1238 - accuracy: 0.9423 - val_loss: 0.2900 - val_accuracy: 0.8085\n",
      "Epoch 40/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.1354 - accuracy: 0.9340 - val_loss: 0.1319 - val_accuracy: 0.9043\n",
      "Epoch 41/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.1029 - accuracy: 0.9482 - val_loss: 0.1244 - val_accuracy: 0.9149\n",
      "Epoch 42/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0914 - accuracy: 0.9600 - val_loss: 0.1624 - val_accuracy: 0.8723\n",
      "Epoch 43/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.1201 - accuracy: 0.9317 - val_loss: 0.1541 - val_accuracy: 0.9255\n",
      "Epoch 44/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0925 - accuracy: 0.9600 - val_loss: 0.1141 - val_accuracy: 0.9149\n",
      "Epoch 45/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.1026 - accuracy: 0.9482 - val_loss: 0.1196 - val_accuracy: 0.9043\n",
      "Epoch 46/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.1010 - accuracy: 0.9470 - val_loss: 0.1491 - val_accuracy: 0.8830\n",
      "Epoch 47/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0741 - accuracy: 0.9717 - val_loss: 0.1826 - val_accuracy: 0.8298\n",
      "Epoch 48/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0786 - accuracy: 0.9717 - val_loss: 0.1166 - val_accuracy: 0.9255\n",
      "Epoch 49/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0910 - accuracy: 0.9600 - val_loss: 0.1122 - val_accuracy: 0.9362\n",
      "Epoch 50/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.1052 - accuracy: 0.9517 - val_loss: 0.1520 - val_accuracy: 0.8936\n",
      "Epoch 51/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0833 - accuracy: 0.9682 - val_loss: 0.2163 - val_accuracy: 0.8191\n",
      "Epoch 52/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0799 - accuracy: 0.9658 - val_loss: 0.0992 - val_accuracy: 0.9362\n",
      "Epoch 53/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.1052 - accuracy: 0.9446 - val_loss: 0.1533 - val_accuracy: 0.8936\n",
      "Epoch 54/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0938 - accuracy: 0.9541 - val_loss: 0.1140 - val_accuracy: 0.9255\n",
      "Epoch 55/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.1010 - accuracy: 0.9517 - val_loss: 0.1258 - val_accuracy: 0.9149\n",
      "Epoch 56/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0636 - accuracy: 0.9753 - val_loss: 0.3273 - val_accuracy: 0.7660\n",
      "Epoch 57/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0781 - accuracy: 0.9682 - val_loss: 0.1851 - val_accuracy: 0.8404\n",
      "Epoch 58/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0791 - accuracy: 0.9670 - val_loss: 0.1008 - val_accuracy: 0.9468\n",
      "Epoch 59/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0594 - accuracy: 0.9788 - val_loss: 0.1725 - val_accuracy: 0.8511\n",
      "Epoch 60/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0844 - accuracy: 0.9611 - val_loss: 0.1446 - val_accuracy: 0.9043\n",
      "Epoch 61/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0659 - accuracy: 0.9682 - val_loss: 0.1015 - val_accuracy: 0.9255\n",
      "Epoch 62/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0735 - accuracy: 0.9658 - val_loss: 0.1050 - val_accuracy: 0.9255\n",
      "Epoch 63/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0704 - accuracy: 0.9647 - val_loss: 0.2035 - val_accuracy: 0.8511\n",
      "Epoch 64/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0701 - accuracy: 0.9694 - val_loss: 0.1162 - val_accuracy: 0.9362\n",
      "Epoch 65/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0882 - accuracy: 0.9611 - val_loss: 0.1558 - val_accuracy: 0.9043\n",
      "Epoch 66/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0893 - accuracy: 0.9576 - val_loss: 0.1283 - val_accuracy: 0.9149\n",
      "Epoch 67/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0842 - accuracy: 0.9600 - val_loss: 0.1129 - val_accuracy: 0.9255\n",
      "Epoch 68/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0521 - accuracy: 0.9812 - val_loss: 0.1205 - val_accuracy: 0.9043\n",
      "Epoch 69/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0634 - accuracy: 0.9764 - val_loss: 0.2292 - val_accuracy: 0.8404\n",
      "Epoch 70/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0565 - accuracy: 0.9776 - val_loss: 0.1346 - val_accuracy: 0.9043\n",
      "Epoch 71/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0770 - accuracy: 0.9647 - val_loss: 0.1568 - val_accuracy: 0.8830\n",
      "Epoch 72/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0722 - accuracy: 0.9729 - val_loss: 0.3680 - val_accuracy: 0.7766\n",
      "Epoch 73/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0667 - accuracy: 0.9694 - val_loss: 0.1554 - val_accuracy: 0.8830\n",
      "Epoch 74/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0879 - accuracy: 0.9576 - val_loss: 0.1004 - val_accuracy: 0.9255\n",
      "Epoch 75/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0793 - accuracy: 0.9658 - val_loss: 0.1136 - val_accuracy: 0.9149\n",
      "Epoch 76/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0692 - accuracy: 0.9788 - val_loss: 0.1442 - val_accuracy: 0.8936\n",
      "Epoch 77/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0816 - accuracy: 0.9600 - val_loss: 0.1129 - val_accuracy: 0.9362\n",
      "Epoch 78/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0500 - accuracy: 0.9812 - val_loss: 0.1094 - val_accuracy: 0.9362\n",
      "Epoch 79/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0653 - accuracy: 0.9706 - val_loss: 0.1194 - val_accuracy: 0.9149\n",
      "Epoch 80/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0589 - accuracy: 0.9776 - val_loss: 0.1268 - val_accuracy: 0.9255\n",
      "Epoch 81/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0724 - accuracy: 0.9658 - val_loss: 0.0977 - val_accuracy: 0.9255\n",
      "Epoch 82/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0520 - accuracy: 0.9764 - val_loss: 0.1172 - val_accuracy: 0.9362\n",
      "Epoch 83/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0850 - accuracy: 0.9682 - val_loss: 0.2167 - val_accuracy: 0.8298\n",
      "Epoch 84/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0682 - accuracy: 0.9635 - val_loss: 0.1225 - val_accuracy: 0.9574\n",
      "Epoch 85/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0718 - accuracy: 0.9658 - val_loss: 0.1264 - val_accuracy: 0.9255\n",
      "Epoch 86/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0393 - accuracy: 0.9882 - val_loss: 0.1571 - val_accuracy: 0.8936\n",
      "Epoch 87/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0621 - accuracy: 0.9729 - val_loss: 0.1206 - val_accuracy: 0.9255\n",
      "Epoch 88/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0671 - accuracy: 0.9658 - val_loss: 0.1164 - val_accuracy: 0.9468\n",
      "Epoch 89/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0663 - accuracy: 0.9658 - val_loss: 0.1118 - val_accuracy: 0.9468\n",
      "Epoch 90/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0602 - accuracy: 0.9729 - val_loss: 0.1137 - val_accuracy: 0.9468\n",
      "Epoch 91/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0531 - accuracy: 0.9764 - val_loss: 0.1554 - val_accuracy: 0.8936\n",
      "Epoch 92/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0553 - accuracy: 0.9788 - val_loss: 0.2836 - val_accuracy: 0.7872\n",
      "Epoch 93/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0889 - accuracy: 0.9482 - val_loss: 0.1139 - val_accuracy: 0.9362\n",
      "Epoch 94/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0926 - accuracy: 0.9482 - val_loss: 0.1097 - val_accuracy: 0.9362\n",
      "Epoch 95/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0706 - accuracy: 0.9611 - val_loss: 0.1255 - val_accuracy: 0.9149\n",
      "Epoch 96/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0795 - accuracy: 0.9658 - val_loss: 0.1270 - val_accuracy: 0.9149\n",
      "Epoch 97/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0718 - accuracy: 0.9706 - val_loss: 0.1418 - val_accuracy: 0.9043\n",
      "Epoch 98/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0648 - accuracy: 0.9635 - val_loss: 0.1145 - val_accuracy: 0.9255\n",
      "Epoch 99/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0734 - accuracy: 0.9717 - val_loss: 0.1488 - val_accuracy: 0.8936\n",
      "Epoch 100/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0642 - accuracy: 0.9729 - val_loss: 0.1224 - val_accuracy: 0.9149\n",
      "Score for fold 8: loss of 0.12238933891057968; accuracy of 91.4893627166748%\n",
      "(849,) (94,)\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 9 ...\n",
      "Epoch 1/100\n",
      "213/213 [==============================] - 45s 181ms/step - loss: 0.4774 - accuracy: 0.6572 - val_loss: 1.4116 - val_accuracy: 0.3404\n",
      "Epoch 2/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.4267 - accuracy: 0.7173 - val_loss: 0.2445 - val_accuracy: 0.7979\n",
      "Epoch 3/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.3689 - accuracy: 0.7562 - val_loss: 0.2575 - val_accuracy: 0.9043\n",
      "Epoch 4/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.3141 - accuracy: 0.8115 - val_loss: 0.5459 - val_accuracy: 0.7340\n",
      "Epoch 5/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.2971 - accuracy: 0.8151 - val_loss: 1.0209 - val_accuracy: 0.6596\n",
      "Epoch 6/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.2858 - accuracy: 0.8351 - val_loss: 0.3829 - val_accuracy: 0.8723\n",
      "Epoch 7/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.2833 - accuracy: 0.8292 - val_loss: 0.1444 - val_accuracy: 0.8936\n",
      "Epoch 8/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.2557 - accuracy: 0.8422 - val_loss: 0.2694 - val_accuracy: 0.7872\n",
      "Epoch 9/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.2870 - accuracy: 0.8280 - val_loss: 0.2778 - val_accuracy: 0.8404\n",
      "Epoch 10/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.2523 - accuracy: 0.8375 - val_loss: 0.2795 - val_accuracy: 0.8830\n",
      "Epoch 11/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.2599 - accuracy: 0.8422 - val_loss: 0.2971 - val_accuracy: 0.8404\n",
      "Epoch 12/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.2234 - accuracy: 0.8575 - val_loss: 0.1641 - val_accuracy: 0.8936\n",
      "Epoch 13/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.2320 - accuracy: 0.8539 - val_loss: 0.5577 - val_accuracy: 0.7872\n",
      "Epoch 14/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.2504 - accuracy: 0.8445 - val_loss: 0.2071 - val_accuracy: 0.8723\n",
      "Epoch 15/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.2246 - accuracy: 0.8693 - val_loss: 0.1962 - val_accuracy: 0.8511\n",
      "Epoch 16/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.2260 - accuracy: 0.8575 - val_loss: 0.3439 - val_accuracy: 0.7872\n",
      "Epoch 17/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.1987 - accuracy: 0.8763 - val_loss: 0.1803 - val_accuracy: 0.8617\n",
      "Epoch 18/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.2195 - accuracy: 0.8657 - val_loss: 0.1363 - val_accuracy: 0.9043\n",
      "Epoch 19/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.2024 - accuracy: 0.8787 - val_loss: 0.4376 - val_accuracy: 0.7979\n",
      "Epoch 20/100\n",
      "213/213 [==============================] - 37s 171ms/step - loss: 0.2054 - accuracy: 0.8704 - val_loss: 0.2829 - val_accuracy: 0.8511\n",
      "Epoch 21/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.1702 - accuracy: 0.9105 - val_loss: 0.1193 - val_accuracy: 0.9468\n",
      "Epoch 22/100\n",
      "213/213 [==============================] - 37s 171ms/step - loss: 0.1887 - accuracy: 0.8869 - val_loss: 0.1272 - val_accuracy: 0.9149\n",
      "Epoch 23/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.1833 - accuracy: 0.8905 - val_loss: 0.0974 - val_accuracy: 0.9362\n",
      "Epoch 24/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.1628 - accuracy: 0.8987 - val_loss: 0.1311 - val_accuracy: 0.9468\n",
      "Epoch 25/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.1685 - accuracy: 0.9011 - val_loss: 0.1565 - val_accuracy: 0.9043\n",
      "Epoch 26/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.1568 - accuracy: 0.8975 - val_loss: 0.2375 - val_accuracy: 0.8723\n",
      "Epoch 27/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.1909 - accuracy: 0.8893 - val_loss: 0.1532 - val_accuracy: 0.8936\n",
      "Epoch 28/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.1677 - accuracy: 0.8975 - val_loss: 0.1260 - val_accuracy: 0.9255\n",
      "Epoch 29/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.1749 - accuracy: 0.8881 - val_loss: 0.1293 - val_accuracy: 0.8936\n",
      "Epoch 30/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.1592 - accuracy: 0.8940 - val_loss: 0.1407 - val_accuracy: 0.9468\n",
      "Epoch 31/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.1619 - accuracy: 0.8893 - val_loss: 0.2558 - val_accuracy: 0.8511\n",
      "Epoch 32/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.1745 - accuracy: 0.8999 - val_loss: 0.1215 - val_accuracy: 0.9468\n",
      "Epoch 33/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.1353 - accuracy: 0.9364 - val_loss: 0.1696 - val_accuracy: 0.8830\n",
      "Epoch 34/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.1333 - accuracy: 0.9305 - val_loss: 0.1398 - val_accuracy: 0.9468\n",
      "Epoch 35/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.1367 - accuracy: 0.9234 - val_loss: 0.1962 - val_accuracy: 0.9149\n",
      "Epoch 36/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.1095 - accuracy: 0.9458 - val_loss: 0.1424 - val_accuracy: 0.9362\n",
      "Epoch 37/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.1199 - accuracy: 0.9376 - val_loss: 0.2189 - val_accuracy: 0.8830\n",
      "Epoch 38/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.1032 - accuracy: 0.9494 - val_loss: 0.2164 - val_accuracy: 0.8511\n",
      "Epoch 39/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.1272 - accuracy: 0.9329 - val_loss: 0.1155 - val_accuracy: 0.9149\n",
      "Epoch 40/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.1116 - accuracy: 0.9376 - val_loss: 0.2322 - val_accuracy: 0.8936\n",
      "Epoch 41/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.1041 - accuracy: 0.9552 - val_loss: 0.1152 - val_accuracy: 0.9255\n",
      "Epoch 42/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0884 - accuracy: 0.9635 - val_loss: 0.1324 - val_accuracy: 0.9043\n",
      "Epoch 43/100\n",
      "213/213 [==============================] - 37s 174ms/step - loss: 0.1479 - accuracy: 0.9140 - val_loss: 0.1438 - val_accuracy: 0.8830\n",
      "Epoch 44/100\n",
      "213/213 [==============================] - 37s 175ms/step - loss: 0.1175 - accuracy: 0.9458 - val_loss: 0.2312 - val_accuracy: 0.8617\n",
      "Epoch 45/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.1190 - accuracy: 0.9435 - val_loss: 0.1574 - val_accuracy: 0.9255\n",
      "Epoch 46/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.1075 - accuracy: 0.9376 - val_loss: 0.1512 - val_accuracy: 0.9255\n",
      "Epoch 47/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.1163 - accuracy: 0.9458 - val_loss: 0.1247 - val_accuracy: 0.9362\n",
      "Epoch 48/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.1120 - accuracy: 0.9446 - val_loss: 0.1632 - val_accuracy: 0.9149\n",
      "Epoch 49/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0842 - accuracy: 0.9623 - val_loss: 0.1224 - val_accuracy: 0.9362\n",
      "Epoch 50/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.1032 - accuracy: 0.9541 - val_loss: 0.1323 - val_accuracy: 0.9043\n",
      "Epoch 51/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0719 - accuracy: 0.9658 - val_loss: 0.1524 - val_accuracy: 0.8936\n",
      "Epoch 52/100\n",
      "213/213 [==============================] - 37s 174ms/step - loss: 0.1155 - accuracy: 0.9470 - val_loss: 0.5504 - val_accuracy: 0.7340\n",
      "Epoch 53/100\n",
      "213/213 [==============================] - 37s 174ms/step - loss: 0.1024 - accuracy: 0.9494 - val_loss: 0.1229 - val_accuracy: 0.9362\n",
      "Epoch 54/100\n",
      "213/213 [==============================] - 37s 174ms/step - loss: 0.0755 - accuracy: 0.9717 - val_loss: 0.1238 - val_accuracy: 0.9255\n",
      "Epoch 55/100\n",
      "213/213 [==============================] - 37s 174ms/step - loss: 0.0793 - accuracy: 0.9670 - val_loss: 0.1311 - val_accuracy: 0.9149\n",
      "Epoch 56/100\n",
      "213/213 [==============================] - 37s 174ms/step - loss: 0.0816 - accuracy: 0.9623 - val_loss: 0.1456 - val_accuracy: 0.9043\n",
      "Epoch 57/100\n",
      "213/213 [==============================] - 37s 174ms/step - loss: 0.0854 - accuracy: 0.9588 - val_loss: 0.1380 - val_accuracy: 0.9149\n",
      "Epoch 58/100\n",
      "213/213 [==============================] - 37s 174ms/step - loss: 0.0843 - accuracy: 0.9682 - val_loss: 0.1612 - val_accuracy: 0.8936\n",
      "Epoch 59/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0959 - accuracy: 0.9600 - val_loss: 0.1222 - val_accuracy: 0.9149\n",
      "Epoch 60/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0881 - accuracy: 0.9647 - val_loss: 0.1249 - val_accuracy: 0.9043\n",
      "Epoch 61/100\n",
      "213/213 [==============================] - 37s 174ms/step - loss: 0.0869 - accuracy: 0.9647 - val_loss: 0.1439 - val_accuracy: 0.8936\n",
      "Epoch 62/100\n",
      "213/213 [==============================] - 37s 174ms/step - loss: 0.0662 - accuracy: 0.9741 - val_loss: 0.1276 - val_accuracy: 0.8936\n",
      "Epoch 63/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0926 - accuracy: 0.9564 - val_loss: 0.1269 - val_accuracy: 0.9149\n",
      "Epoch 64/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0610 - accuracy: 0.9729 - val_loss: 0.1412 - val_accuracy: 0.9043\n",
      "Epoch 65/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0714 - accuracy: 0.9717 - val_loss: 0.1263 - val_accuracy: 0.9255\n",
      "Epoch 66/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0719 - accuracy: 0.9717 - val_loss: 0.1238 - val_accuracy: 0.9255\n",
      "Epoch 67/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0878 - accuracy: 0.9600 - val_loss: 0.1417 - val_accuracy: 0.9043\n",
      "Epoch 68/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0680 - accuracy: 0.9764 - val_loss: 0.1151 - val_accuracy: 0.9362\n",
      "Epoch 69/100\n",
      "213/213 [==============================] - 37s 174ms/step - loss: 0.0672 - accuracy: 0.9741 - val_loss: 0.1131 - val_accuracy: 0.9149\n",
      "Epoch 70/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0780 - accuracy: 0.9647 - val_loss: 0.1124 - val_accuracy: 0.9149\n",
      "Epoch 71/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0689 - accuracy: 0.9647 - val_loss: 0.1689 - val_accuracy: 0.8723\n",
      "Epoch 72/100\n",
      "213/213 [==============================] - 37s 174ms/step - loss: 0.0591 - accuracy: 0.9741 - val_loss: 0.1455 - val_accuracy: 0.9043\n",
      "Epoch 73/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0873 - accuracy: 0.9623 - val_loss: 0.1361 - val_accuracy: 0.9043\n",
      "Epoch 74/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0511 - accuracy: 0.9800 - val_loss: 0.1179 - val_accuracy: 0.9149\n",
      "Epoch 75/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0565 - accuracy: 0.9717 - val_loss: 0.1080 - val_accuracy: 0.9149\n",
      "Epoch 76/100\n",
      "213/213 [==============================] - 37s 174ms/step - loss: 0.0562 - accuracy: 0.9741 - val_loss: 0.1286 - val_accuracy: 0.9149\n",
      "Epoch 77/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0546 - accuracy: 0.9764 - val_loss: 0.1118 - val_accuracy: 0.9255\n",
      "Epoch 78/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0543 - accuracy: 0.9741 - val_loss: 0.1107 - val_accuracy: 0.9149\n",
      "Epoch 79/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0843 - accuracy: 0.9623 - val_loss: 0.1314 - val_accuracy: 0.9255\n",
      "Epoch 80/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0570 - accuracy: 0.9706 - val_loss: 0.1294 - val_accuracy: 0.9043\n",
      "Epoch 81/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0844 - accuracy: 0.9611 - val_loss: 0.1220 - val_accuracy: 0.9255\n",
      "Epoch 82/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0820 - accuracy: 0.9611 - val_loss: 0.1288 - val_accuracy: 0.9043\n",
      "Epoch 83/100\n",
      "213/213 [==============================] - 37s 175ms/step - loss: 0.0895 - accuracy: 0.9552 - val_loss: 0.1156 - val_accuracy: 0.9149\n",
      "Epoch 84/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0730 - accuracy: 0.9623 - val_loss: 0.1231 - val_accuracy: 0.9255\n",
      "Epoch 85/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0777 - accuracy: 0.9635 - val_loss: 0.1220 - val_accuracy: 0.9362\n",
      "Epoch 86/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0499 - accuracy: 0.9800 - val_loss: 0.1255 - val_accuracy: 0.8936\n",
      "Epoch 87/100\n",
      "213/213 [==============================] - 37s 171ms/step - loss: 0.0551 - accuracy: 0.9753 - val_loss: 0.1169 - val_accuracy: 0.9255\n",
      "Epoch 88/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0611 - accuracy: 0.9741 - val_loss: 0.1191 - val_accuracy: 0.8936\n",
      "Epoch 89/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0602 - accuracy: 0.9706 - val_loss: 0.1128 - val_accuracy: 0.9255\n",
      "Epoch 90/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0573 - accuracy: 0.9741 - val_loss: 0.1131 - val_accuracy: 0.9255\n",
      "Epoch 91/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0397 - accuracy: 0.9835 - val_loss: 0.1128 - val_accuracy: 0.9255\n",
      "Epoch 92/100\n",
      "213/213 [==============================] - 37s 171ms/step - loss: 0.0912 - accuracy: 0.9529 - val_loss: 0.1138 - val_accuracy: 0.9255\n",
      "Epoch 93/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0526 - accuracy: 0.9823 - val_loss: 0.1172 - val_accuracy: 0.9149\n",
      "Epoch 94/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0516 - accuracy: 0.9800 - val_loss: 0.1275 - val_accuracy: 0.9043\n",
      "Epoch 95/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0624 - accuracy: 0.9729 - val_loss: 0.1134 - val_accuracy: 0.9255\n",
      "Epoch 96/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0494 - accuracy: 0.9812 - val_loss: 0.1177 - val_accuracy: 0.9149\n",
      "Epoch 97/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0582 - accuracy: 0.9753 - val_loss: 0.1273 - val_accuracy: 0.9255\n",
      "Epoch 98/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0477 - accuracy: 0.9835 - val_loss: 0.1099 - val_accuracy: 0.9255\n",
      "Epoch 99/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0696 - accuracy: 0.9670 - val_loss: 0.1148 - val_accuracy: 0.9043\n",
      "Epoch 100/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0370 - accuracy: 0.9859 - val_loss: 0.1263 - val_accuracy: 0.9255\n",
      "Score for fold 9: loss of 0.12632054090499878; accuracy of 92.55319237709045%\n",
      "(849,) (94,)\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 10 ...\n",
      "Epoch 1/100\n",
      "213/213 [==============================] - 45s 178ms/step - loss: 0.4970 - accuracy: 0.6196 - val_loss: 2.0799 - val_accuracy: 0.3404\n",
      "Epoch 2/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.3846 - accuracy: 0.7503 - val_loss: 0.9903 - val_accuracy: 0.6277\n",
      "Epoch 3/100\n",
      "213/213 [==============================] - 37s 174ms/step - loss: 0.3510 - accuracy: 0.7856 - val_loss: 4.7982 - val_accuracy: 0.5532\n",
      "Epoch 4/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.3238 - accuracy: 0.8080 - val_loss: 1.4821 - val_accuracy: 0.7979\n",
      "Epoch 5/100\n",
      "213/213 [==============================] - 37s 174ms/step - loss: 0.2838 - accuracy: 0.8245 - val_loss: 0.9369 - val_accuracy: 0.7340\n",
      "Epoch 6/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.2848 - accuracy: 0.8433 - val_loss: 0.3423 - val_accuracy: 0.8298\n",
      "Epoch 7/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.2919 - accuracy: 0.8280 - val_loss: 0.1746 - val_accuracy: 0.9149\n",
      "Epoch 8/100\n",
      "213/213 [==============================] - 37s 174ms/step - loss: 0.2609 - accuracy: 0.8410 - val_loss: 0.4650 - val_accuracy: 0.9043\n",
      "Epoch 9/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.2689 - accuracy: 0.8410 - val_loss: 0.1234 - val_accuracy: 0.9468\n",
      "Epoch 10/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.2531 - accuracy: 0.8433 - val_loss: 0.1877 - val_accuracy: 0.9043\n",
      "Epoch 11/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.2389 - accuracy: 0.8528 - val_loss: 0.2971 - val_accuracy: 0.8404\n",
      "Epoch 12/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.2687 - accuracy: 0.8292 - val_loss: 0.9943 - val_accuracy: 0.7340\n",
      "Epoch 13/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.2263 - accuracy: 0.8610 - val_loss: 0.1068 - val_accuracy: 0.9255\n",
      "Epoch 14/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.2229 - accuracy: 0.8645 - val_loss: 0.2601 - val_accuracy: 0.8617\n",
      "Epoch 15/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.2527 - accuracy: 0.8433 - val_loss: 0.2327 - val_accuracy: 0.8723\n",
      "Epoch 16/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.2015 - accuracy: 0.8775 - val_loss: 0.1745 - val_accuracy: 0.8830\n",
      "Epoch 17/100\n",
      "213/213 [==============================] - 37s 174ms/step - loss: 0.2164 - accuracy: 0.8657 - val_loss: 0.0995 - val_accuracy: 0.9255\n",
      "Epoch 18/100\n",
      "213/213 [==============================] - 40s 187ms/step - loss: 0.2097 - accuracy: 0.8704 - val_loss: 0.1007 - val_accuracy: 0.9468\n",
      "Epoch 19/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.2118 - accuracy: 0.8775 - val_loss: 0.9715 - val_accuracy: 0.7553\n",
      "Epoch 20/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.1732 - accuracy: 0.8952 - val_loss: 0.0967 - val_accuracy: 0.9362\n",
      "Epoch 21/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.2038 - accuracy: 0.8763 - val_loss: 0.2027 - val_accuracy: 0.8617\n",
      "Epoch 22/100\n",
      "213/213 [==============================] - 37s 174ms/step - loss: 0.1733 - accuracy: 0.8905 - val_loss: 0.4636 - val_accuracy: 0.8404\n",
      "Epoch 23/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.1695 - accuracy: 0.9034 - val_loss: 0.2398 - val_accuracy: 0.8936\n",
      "Epoch 24/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.1759 - accuracy: 0.8975 - val_loss: 0.0841 - val_accuracy: 0.9362\n",
      "Epoch 25/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.1801 - accuracy: 0.8916 - val_loss: 0.1143 - val_accuracy: 0.9468\n",
      "Epoch 26/100\n",
      "213/213 [==============================] - 37s 174ms/step - loss: 0.1445 - accuracy: 0.9211 - val_loss: 0.3461 - val_accuracy: 0.7872\n",
      "Epoch 27/100\n",
      "213/213 [==============================] - 37s 174ms/step - loss: 0.1503 - accuracy: 0.9140 - val_loss: 0.2561 - val_accuracy: 0.8404\n",
      "Epoch 28/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.1646 - accuracy: 0.9022 - val_loss: 0.1034 - val_accuracy: 0.9468\n",
      "Epoch 29/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.1368 - accuracy: 0.9223 - val_loss: 0.0877 - val_accuracy: 0.9362\n",
      "Epoch 30/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.1248 - accuracy: 0.9258 - val_loss: 0.2668 - val_accuracy: 0.8298\n",
      "Epoch 31/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.1403 - accuracy: 0.9199 - val_loss: 0.1874 - val_accuracy: 0.9043\n",
      "Epoch 32/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.1566 - accuracy: 0.9164 - val_loss: 0.1929 - val_accuracy: 0.8617\n",
      "Epoch 33/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.1450 - accuracy: 0.9305 - val_loss: 0.0921 - val_accuracy: 0.9362\n",
      "Epoch 34/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.1471 - accuracy: 0.9258 - val_loss: 0.2099 - val_accuracy: 0.8723\n",
      "Epoch 35/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.1129 - accuracy: 0.9388 - val_loss: 0.1573 - val_accuracy: 0.9043\n",
      "Epoch 36/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.1027 - accuracy: 0.9564 - val_loss: 0.1229 - val_accuracy: 0.9149\n",
      "Epoch 37/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0932 - accuracy: 0.9552 - val_loss: 0.1158 - val_accuracy: 0.9255\n",
      "Epoch 38/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.1296 - accuracy: 0.9317 - val_loss: 0.1194 - val_accuracy: 0.9362\n",
      "Epoch 39/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.1025 - accuracy: 0.9505 - val_loss: 0.0984 - val_accuracy: 0.8830\n",
      "Epoch 40/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.1128 - accuracy: 0.9458 - val_loss: 0.3839 - val_accuracy: 0.8085\n",
      "Epoch 41/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0988 - accuracy: 0.9505 - val_loss: 0.0805 - val_accuracy: 0.9468\n",
      "Epoch 42/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.1052 - accuracy: 0.9517 - val_loss: 0.1330 - val_accuracy: 0.9255\n",
      "Epoch 43/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.1240 - accuracy: 0.9305 - val_loss: 0.1416 - val_accuracy: 0.9149\n",
      "Epoch 44/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0952 - accuracy: 0.9588 - val_loss: 0.1558 - val_accuracy: 0.9043\n",
      "Epoch 45/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0801 - accuracy: 0.9729 - val_loss: 0.4228 - val_accuracy: 0.8298\n",
      "Epoch 46/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0920 - accuracy: 0.9541 - val_loss: 0.1878 - val_accuracy: 0.8723\n",
      "Epoch 47/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.1152 - accuracy: 0.9376 - val_loss: 0.3895 - val_accuracy: 0.8191\n",
      "Epoch 48/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.1006 - accuracy: 0.9576 - val_loss: 0.1879 - val_accuracy: 0.8936\n",
      "Epoch 49/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0815 - accuracy: 0.9647 - val_loss: 0.1688 - val_accuracy: 0.8936\n",
      "Epoch 50/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0651 - accuracy: 0.9717 - val_loss: 0.1816 - val_accuracy: 0.9149\n",
      "Epoch 51/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.1044 - accuracy: 0.9552 - val_loss: 0.1819 - val_accuracy: 0.9149\n",
      "Epoch 52/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0871 - accuracy: 0.9623 - val_loss: 0.2395 - val_accuracy: 0.8617\n",
      "Epoch 53/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0804 - accuracy: 0.9717 - val_loss: 0.0913 - val_accuracy: 0.9149\n",
      "Epoch 54/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.1027 - accuracy: 0.9505 - val_loss: 0.0943 - val_accuracy: 0.9468\n",
      "Epoch 55/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0697 - accuracy: 0.9741 - val_loss: 0.1564 - val_accuracy: 0.9149\n",
      "Epoch 56/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.1030 - accuracy: 0.9482 - val_loss: 0.1369 - val_accuracy: 0.9043\n",
      "Epoch 57/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0639 - accuracy: 0.9776 - val_loss: 0.1696 - val_accuracy: 0.8936\n",
      "Epoch 58/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.1098 - accuracy: 0.9470 - val_loss: 0.2920 - val_accuracy: 0.8404\n",
      "Epoch 59/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0779 - accuracy: 0.9682 - val_loss: 0.1721 - val_accuracy: 0.8830\n",
      "Epoch 60/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0716 - accuracy: 0.9682 - val_loss: 0.2142 - val_accuracy: 0.8723\n",
      "Epoch 61/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0735 - accuracy: 0.9694 - val_loss: 0.2081 - val_accuracy: 0.8936\n",
      "Epoch 62/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0594 - accuracy: 0.9753 - val_loss: 0.1755 - val_accuracy: 0.8936\n",
      "Epoch 63/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.1017 - accuracy: 0.9529 - val_loss: 0.1280 - val_accuracy: 0.9149\n",
      "Epoch 64/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0580 - accuracy: 0.9788 - val_loss: 0.2073 - val_accuracy: 0.8723\n",
      "Epoch 65/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.1049 - accuracy: 0.9505 - val_loss: 0.1304 - val_accuracy: 0.9362\n",
      "Epoch 66/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0678 - accuracy: 0.9694 - val_loss: 0.1247 - val_accuracy: 0.9362\n",
      "Epoch 67/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0829 - accuracy: 0.9564 - val_loss: 0.1222 - val_accuracy: 0.9255\n",
      "Epoch 68/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0803 - accuracy: 0.9611 - val_loss: 0.1132 - val_accuracy: 0.9043\n",
      "Epoch 69/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0940 - accuracy: 0.9588 - val_loss: 0.2524 - val_accuracy: 0.8723\n",
      "Epoch 70/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0974 - accuracy: 0.9505 - val_loss: 0.1040 - val_accuracy: 0.9043\n",
      "Epoch 71/100\n",
      "213/213 [==============================] - 37s 174ms/step - loss: 0.0609 - accuracy: 0.9776 - val_loss: 0.1374 - val_accuracy: 0.9149\n",
      "Epoch 72/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0774 - accuracy: 0.9600 - val_loss: 0.1137 - val_accuracy: 0.9043\n",
      "Epoch 73/100\n",
      "213/213 [==============================] - 37s 174ms/step - loss: 0.0691 - accuracy: 0.9741 - val_loss: 0.1086 - val_accuracy: 0.9043\n",
      "Epoch 74/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0742 - accuracy: 0.9611 - val_loss: 0.1620 - val_accuracy: 0.9043\n",
      "Epoch 75/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0567 - accuracy: 0.9753 - val_loss: 0.1455 - val_accuracy: 0.9043\n",
      "Epoch 76/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0490 - accuracy: 0.9812 - val_loss: 0.1967 - val_accuracy: 0.8830\n",
      "Epoch 77/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0853 - accuracy: 0.9623 - val_loss: 0.1614 - val_accuracy: 0.9149\n",
      "Epoch 78/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0618 - accuracy: 0.9741 - val_loss: 0.1589 - val_accuracy: 0.8936\n",
      "Epoch 79/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0672 - accuracy: 0.9694 - val_loss: 0.2614 - val_accuracy: 0.8511\n",
      "Epoch 80/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0378 - accuracy: 0.9870 - val_loss: 0.1796 - val_accuracy: 0.8936\n",
      "Epoch 81/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.1024 - accuracy: 0.9411 - val_loss: 0.1242 - val_accuracy: 0.9043\n",
      "Epoch 82/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0612 - accuracy: 0.9741 - val_loss: 0.1876 - val_accuracy: 0.8830\n",
      "Epoch 83/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0469 - accuracy: 0.9812 - val_loss: 0.1264 - val_accuracy: 0.9043\n",
      "Epoch 84/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0645 - accuracy: 0.9717 - val_loss: 0.1461 - val_accuracy: 0.9043\n",
      "Epoch 85/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0470 - accuracy: 0.9859 - val_loss: 0.1519 - val_accuracy: 0.9043\n",
      "Epoch 86/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0767 - accuracy: 0.9600 - val_loss: 0.1315 - val_accuracy: 0.9362\n",
      "Epoch 87/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0601 - accuracy: 0.9717 - val_loss: 0.1090 - val_accuracy: 0.9149\n",
      "Epoch 88/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0669 - accuracy: 0.9682 - val_loss: 0.1827 - val_accuracy: 0.8830\n",
      "Epoch 89/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0708 - accuracy: 0.9682 - val_loss: 0.1551 - val_accuracy: 0.8936\n",
      "Epoch 90/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0758 - accuracy: 0.9647 - val_loss: 0.1841 - val_accuracy: 0.8936\n",
      "Epoch 91/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0687 - accuracy: 0.9635 - val_loss: 0.1358 - val_accuracy: 0.9043\n",
      "Epoch 92/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0704 - accuracy: 0.9658 - val_loss: 0.1479 - val_accuracy: 0.9149\n",
      "Epoch 93/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0794 - accuracy: 0.9647 - val_loss: 0.1292 - val_accuracy: 0.9149\n",
      "Epoch 94/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0400 - accuracy: 0.9882 - val_loss: 0.1695 - val_accuracy: 0.9149\n",
      "Epoch 95/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0866 - accuracy: 0.9588 - val_loss: 0.1579 - val_accuracy: 0.8936\n",
      "Epoch 96/100\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.0527 - accuracy: 0.9800 - val_loss: 0.1481 - val_accuracy: 0.9043\n",
      "Epoch 97/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0549 - accuracy: 0.9800 - val_loss: 0.1684 - val_accuracy: 0.8936\n",
      "Epoch 98/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0753 - accuracy: 0.9682 - val_loss: 0.1536 - val_accuracy: 0.9149\n",
      "Epoch 99/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0774 - accuracy: 0.9670 - val_loss: 0.1602 - val_accuracy: 0.9043\n",
      "Epoch 100/100\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.0855 - accuracy: 0.9517 - val_loss: 0.1575 - val_accuracy: 0.9149\n",
      "Score for fold 10: loss of 0.1574546992778778; accuracy of 91.4893627166748%\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 0.10457567125558853 - Accuracy: 93.68420839309692%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 0.163054957985878 - Accuracy: 93.68420839309692%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 0.12388116121292114 - Accuracy: 93.68420839309692%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 0.10732637345790863 - Accuracy: 94.68085169792175%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 0.2894236743450165 - Accuracy: 84.04255509376526%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 6 - Loss: 0.17741024494171143 - Accuracy: 88.29787373542786%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 7 - Loss: 0.10538401454687119 - Accuracy: 93.6170220375061%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 8 - Loss: 0.12238933891057968 - Accuracy: 91.4893627166748%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 9 - Loss: 0.12632054090499878 - Accuracy: 92.55319237709045%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 10 - Loss: 0.1574546992778778 - Accuracy: 91.4893627166748%\n",
      "------------------------------------------------------------------------\n",
      "Average scores for all folds:\n",
      "> Accuracy: 91.72228455543518 (+- 3.088597289054967)\n",
      "> Loss: 0.14772206768393517\n",
      "------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "acc_per_fold = []\n",
    "loss_per_fold = []\n",
    "\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True)\n",
    "\n",
    "fold_no = 1\n",
    "\n",
    "with tf.device('/gpu:0'):\n",
    "    for train, test in kfold.split(X_train, y_train):\n",
    "        print(train.shape, test.shape)\n",
    "    \n",
    "        input = Input(shape=(300, 300, 2))\n",
    "        model = EfficientNetB3(input_tensor=input, include_top=False, weights=None, pooling='avg')\n",
    "        \n",
    "        x = model.output\n",
    "\n",
    "        x = Dense(3, activation='softmax', name='softmax', kernel_initializer='he_normal')(x)\n",
    "        model = Model(model.input, x)\n",
    "\n",
    "        \n",
    "        optimizer = tf.keras.optimizers.SGD(lr=0.01, decay=1e-3, momentum=0.9, nesterov=True)\n",
    "        #optimizer = optimizers.Adam(lr=0.001)\n",
    "        \n",
    "        #callbacks_list = [LearningRateSchedule([20,40])]\n",
    "        \n",
    "        model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "        \n",
    "        print('------------------------------------------------------------------------')\n",
    "        print(f'Training for fold {fold_no} ...')\n",
    "        \n",
    "        history = model.fit(inputs[train], targets[train], \n",
    "                            batch_size=4, \n",
    "                            epochs=100, \n",
    "                            verbose=1,\n",
    "                            validation_data=(inputs[test], targets[test]),\n",
    "                            callbacks = tensorboard_callback) #  Validation set  ?\n",
    "        \n",
    "        scores = model.evaluate(inputs[test], targets[test], verbose=0)\n",
    "        print(f'Score for fold {fold_no}: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}%')\n",
    "        acc_per_fold.append(scores[1] * 100)\n",
    "        loss_per_fold.append(scores[0])\n",
    "     \n",
    "\n",
    "        #np.savetxt('E:/Result/ver.3.25/EfficientNetB1/acc/'+ f'acc_{i}.csv', acc_per_fold, delimiter=\",\")\n",
    "        #np.savetxt('E:/Result/ver.3.25/EfficientNetB1/loss/'+ f'loss_{i}.csv', loss_per_fold, delimiter=\",\")\n",
    "        \n",
    "        model.save('E:/Result/ver.3.25/EfficientNetB3/' + f'Eff_{fold_no}.h5',fold_no)\n",
    "        \n",
    "        fold_no = fold_no + 1\n",
    "\n",
    "# == Provide average scores ==\n",
    "print('------------------------------------------------------------------------')\n",
    "print('Score per fold')\n",
    "for i in range(0, len(acc_per_fold)):\n",
    "  print('------------------------------------------------------------------------')\n",
    "  print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]}%')\n",
    "print('------------------------------------------------------------------------')\n",
    "print('Average scores for all folds:')\n",
    "print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n",
    "print(f'> Loss: {np.mean(loss_per_fold)}')\n",
    "print('------------------------------------------------------------------------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pylab as plt\n",
    "\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.legend(['training', 'validation'], loc= 'upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.legend(['training', 'validaion'], loc= 'upper right')\n",
    "plt.show()\\\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "351c676f2e382adcf1b705bc0333b8a2b296b5ec0e03cae74b6f4b5ae2fa5d28"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('tf2.0-gpu')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
